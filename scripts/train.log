[2024-04-11 20:25:16,177] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 20:25:16,658] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-11 20:25:16,658] [INFO] [runner.py:568:main] cmd = /home/cc/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --config_name my_model/config.json --tokenizer_name YeungNLP/firefly-llama2-7b-base --train_files data/训练数据/train_all_data_random.csv data/训练数据/train_news_chunk15.csv data/训练数据/train_news_chunk4.csv data/训练数据/train_my_baike_qa.csv data/训练数据/train_news_chunk16.csv data/训练数据/train_news_chunk5.csv data/训练数据/train_my_belll_3M_cn.csv data/训练数据/train_news_chunk17.csv data/训练数据/train_news_chunk6.csv data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv data/训练数据/train_news_chunk18.csv data/训练数据/train_news_chunk7.csv data/训练数据/train_my_web_text_zh.csv data/训练数据/train_news_chunk19.csv data/训练数据/train_news_chunk8.csv data/训练数据/train_news_chunk1.csv data/训练数据/train_news_chunk2.csv data/训练数据/train_news_chunk9.csv data/训练数据/train_news_chunk10.csv data/训练数据/train_news_chunk20.csv data/训练数据/train_sft_train.csv data/训练数据/train_news_chunk11.csv data/训练数据/train_news_chunk21.csv data/训练数据/train_web_poetry.csv data/训练数据/train_news_chunk12.csv data/训练数据/train_news_chunk22.csv data/训练数据/train_zhihu_kol.csv data/训练数据/train_news_chunk13.csv data/训练数据/train_news_chunk23.csv data/训练数据/train_news_chunk14.csv data/训练数据/train_news_chunk3.csv --validation_files data/dev_sft.csv --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --do_train --output_dir output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 100 --eval_steps 5000000 --save_total_limit 10 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2024-04-11 20:25:17,960] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 20:25:18,182] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2024-04-11 20:25:18,182] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-04-11 20:25:18,182] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-11 20:25:18,182] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-11 20:25:18,182] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-11 20:25:18,182] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-04-11 20:25:18,183] [INFO] [launch.py:253:main] process 178480 spawned with command: ['/home/cc/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', 'my_model/config.json', '--tokenizer_name', 'YeungNLP/firefly-llama2-7b-base', '--train_files', 'data/训练数据/train_all_data_random.csv', 'data/训练数据/train_news_chunk15.csv', 'data/训练数据/train_news_chunk4.csv', 'data/训练数据/train_my_baike_qa.csv', 'data/训练数据/train_news_chunk16.csv', 'data/训练数据/train_news_chunk5.csv', 'data/训练数据/train_my_belll_3M_cn.csv', 'data/训练数据/train_news_chunk17.csv', 'data/训练数据/train_news_chunk6.csv', 'data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv', 'data/训练数据/train_news_chunk18.csv', 'data/训练数据/train_news_chunk7.csv', 'data/训练数据/train_my_web_text_zh.csv', 'data/训练数据/train_news_chunk19.csv', 'data/训练数据/train_news_chunk8.csv', 'data/训练数据/train_news_chunk1.csv', 'data/训练数据/train_news_chunk2.csv', 'data/训练数据/train_news_chunk9.csv', 'data/训练数据/train_news_chunk10.csv', 'data/训练数据/train_news_chunk20.csv', 'data/训练数据/train_sft_train.csv', 'data/训练数据/train_news_chunk11.csv', 'data/训练数据/train_news_chunk21.csv', 'data/训练数据/train_web_poetry.csv', 'data/训练数据/train_news_chunk12.csv', 'data/训练数据/train_news_chunk22.csv', 'data/训练数据/train_zhihu_kol.csv', 'data/训练数据/train_news_chunk13.csv', 'data/训练数据/train_news_chunk23.csv', 'data/训练数据/train_news_chunk14.csv', 'data/训练数据/train_news_chunk3.csv', '--validation_files', 'data/dev_sft.csv', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '16', '--do_train', '--output_dir', 'output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', 'output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '100', '--eval_steps', '5000000', '--save_total_limit', '10', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', 'output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2024-04-11 20:25:19,887] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 20:25:20,706] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 20:25:20,706] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
04/11/2024 20:25:20 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
04/11/2024 20:25:20 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=18000000,
debug=[],
deepspeed=./ds_config_zero3.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=5000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=True,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_model/logs,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=output_model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=16,
per_device_train_batch_size=16,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=output_model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=10,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=5000,
weight_decay=0.0,
)
['data/训练数据/train_all_data_random.csv', 'data/训练数据/train_news_chunk15.csv', 'data/训练数据/train_news_chunk4.csv', 'data/训练数据/train_my_baike_qa.csv', 'data/训练数据/train_news_chunk16.csv', 'data/训练数据/train_news_chunk5.csv', 'data/训练数据/train_my_belll_3M_cn.csv', 'data/训练数据/train_news_chunk17.csv', 'data/训练数据/train_news_chunk6.csv', 'data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv', 'data/训练数据/train_news_chunk18.csv', 'data/训练数据/train_news_chunk7.csv', 'data/训练数据/train_my_web_text_zh.csv', 'data/训练数据/train_news_chunk19.csv', 'data/训练数据/train_news_chunk8.csv', 'data/训练数据/train_news_chunk1.csv', 'data/训练数据/train_news_chunk2.csv', 'data/训练数据/train_news_chunk9.csv', 'data/训练数据/train_news_chunk10.csv', 'data/训练数据/train_news_chunk20.csv', 'data/训练数据/train_sft_train.csv', 'data/训练数据/train_news_chunk11.csv', 'data/训练数据/train_news_chunk21.csv', 'data/训练数据/train_web_poetry.csv', 'data/训练数据/train_news_chunk12.csv', 'data/训练数据/train_news_chunk22.csv', 'data/训练数据/train_zhihu_kol.csv', 'data/训练数据/train_news_chunk13.csv', 'data/训练数据/train_news_chunk23.csv', 'data/训练数据/train_news_chunk14.csv', 'data/训练数据/train_news_chunk3.csv']
训练文件总个数 31
04/11/2024 20:25:21 - INFO - datasets.builder - Using custom data configuration default-1187581384cc2f62
04/11/2024 20:25:21 - INFO - datasets.info - Loading Dataset Infos from /home/cc/anaconda3/envs/llama-chinese/lib/python3.10/site-packages/datasets/packaged_modules/csv
04/11/2024 20:25:21 - INFO - datasets.builder - Generating dataset csv (/home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6)
04/11/2024 20:25:21 - INFO - datasets.builder - Downloading and preparing dataset csv/default to /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6...
04/11/2024 20:25:21 - INFO - datasets.download.download_manager - Downloading took 0.0 min
04/11/2024 20:25:21 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
04/11/2024 20:25:21 - INFO - datasets.builder - Generating train split
04/11/2024 20:26:58 - INFO - datasets.builder - Generating validation split
04/11/2024 20:26:58 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
04/11/2024 20:26:58 - INFO - datasets.builder - Dataset csv downloaded and prepared to /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6. Subsequent calls will reuse this data.
0 start load tokenizer
0 end load tokenizer
0 start load model
[2024-04-11 20:27:00,934] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 291, num_elems = 0.39B
04/11/2024 20:27:00 - INFO - __main__ - Training new model from scratch - Total size=0.00M params
0 end load model
['text']
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00000_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00001_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00002_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00003_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00004_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00005_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00006_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00007_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00008_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00009_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Spawning 10 processes
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00000_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00004_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00008_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00006_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00003_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00007_of_00010.arrow
04/11/2024 20:27:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00002_of_00010.arrow
04/11/2024 20:27:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00009_of_00010.arrow
04/11/2024 20:27:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00001_of_00010.arrow
04/11/2024 20:27:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00005_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00000_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00001_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00002_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00003_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00004_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00005_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00006_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00007_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00008_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00009_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Spawning 10 processes
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00000_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00001_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00002_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00004_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00003_of_00010.arrow
04/11/2024 20:46:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00005_of_00010.arrow
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00007_of_00010.arrow
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00008_of_00010.arrow
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00009_of_00010.arrow
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00006_of_00010.arrow
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00000_of_00010.arrow
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00001_of_00010.arrow
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00002_of_00010.arrow
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00003_of_00010.arrow
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00004_of_00010.arrow
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00005_of_00010.arrow
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00006_of_00010.arrow
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00007_of_00010.arrow
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00008_of_00010.arrow
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00009_of_00010.arrow
04/11/2024 20:46:50 - INFO - datasets.arrow_dataset - Spawning 10 processes
04/11/2024 20:46:52 - INFO - __main__ - group texts input examples length40000 after_group size3187
04/11/2024 20:46:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00000_of_00010.arrow
04/11/2024 20:46:52 - INFO - __main__ - group texts input examples length40000 after_group size3692
04/11/2024 20:46:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00008_of_00010.arrow
04/11/2024 20:46:53 - INFO - __main__ - group texts input examples length40000 after_group size4137
04/11/2024 20:46:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00004_of_00010.arrow
04/11/2024 20:46:53 - INFO - __main__ - group texts input examples length40000 after_group size4765
04/11/2024 20:46:53 - INFO - __main__ - group texts input examples length40000 after_group size4760
04/11/2024 20:46:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00006_of_00010.arrow
04/11/2024 20:46:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00007_of_00010.arrow
04/11/2024 20:46:55 - INFO - __main__ - group texts input examples length40000 after_group size3189
04/11/2024 20:46:55 - INFO - __main__ - group texts input examples length40000 after_group size3703
04/11/2024 20:46:55 - INFO - __main__ - group texts input examples length40000 after_group size8778
04/11/2024 20:46:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00003_of_00010.arrow
04/11/2024 20:46:56 - INFO - __main__ - group texts input examples length40000 after_group size4129
04/11/2024 20:46:56 - INFO - __main__ - group texts input examples length40000 after_group size10089
04/11/2024 20:46:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00002_of_00010.arrow
04/11/2024 20:46:57 - INFO - __main__ - group texts input examples length40000 after_group size4760
04/11/2024 20:46:57 - INFO - __main__ - group texts input examples length40000 after_group size4777
04/11/2024 20:46:58 - INFO - __main__ - group texts input examples length40000 after_group size3174
04/11/2024 20:46:58 - INFO - __main__ - group texts input examples length40000 after_group size3680
04/11/2024 20:46:59 - INFO - __main__ - group texts input examples length40000 after_group size4134
04/11/2024 20:47:00 - INFO - __main__ - group texts input examples length40000 after_group size4769
04/11/2024 20:47:01 - INFO - __main__ - group texts input examples length40000 after_group size3188
04/11/2024 20:47:01 - INFO - __main__ - group texts input examples length40000 after_group size4764
04/11/2024 20:47:02 - INFO - __main__ - group texts input examples length40000 after_group size3681
04/11/2024 20:47:03 - INFO - __main__ - group texts input examples length40000 after_group size21604
04/11/2024 20:47:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00005_of_00010.arrow
04/11/2024 20:47:03 - INFO - __main__ - group texts input examples length40000 after_group size8780
04/11/2024 20:47:03 - INFO - __main__ - group texts input examples length40000 after_group size4132
04/11/2024 20:47:03 - INFO - __main__ - group texts input examples length40000 after_group size20320
04/11/2024 20:47:03 - INFO - __main__ - group texts input examples length40000 after_group size3180
04/11/2024 20:47:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00009_of_00010.arrow
04/11/2024 20:47:04 - INFO - __main__ - group texts input examples length40000 after_group size4770
04/11/2024 20:47:05 - INFO - __main__ - group texts input examples length40000 after_group size4763
04/11/2024 20:47:05 - INFO - __main__ - group texts input examples length40000 after_group size3711
04/11/2024 20:47:05 - INFO - __main__ - group texts input examples length40000 after_group size9831
04/11/2024 20:47:06 - INFO - __main__ - group texts input examples length40000 after_group size3188
04/11/2024 20:47:06 - INFO - __main__ - group texts input examples length40000 after_group size4141
04/11/2024 20:47:08 - INFO - __main__ - group texts input examples length40000 after_group size3706
04/11/2024 20:47:08 - INFO - __main__ - group texts input examples length40000 after_group size4766
04/11/2024 20:47:09 - INFO - __main__ - group texts input examples length40000 after_group size4740
04/11/2024 20:47:09 - INFO - __main__ - group texts input examples length40000 after_group size3194
04/11/2024 20:47:09 - INFO - __main__ - group texts input examples length40000 after_group size31580
04/11/2024 20:47:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00001_of_00010.arrow
04/11/2024 20:47:10 - INFO - __main__ - group texts input examples length40000 after_group size8752
04/11/2024 20:47:10 - INFO - __main__ - group texts input examples length40000 after_group size4145
04/11/2024 20:47:10 - INFO - __main__ - group texts input examples length40000 after_group size4760
04/11/2024 20:47:11 - INFO - __main__ - group texts input examples length40000 after_group size3683
04/11/2024 20:47:12 - INFO - __main__ - group texts input examples length40000 after_group size3175
04/11/2024 20:47:12 - INFO - __main__ - group texts input examples length40000 after_group size4776
04/11/2024 20:47:13 - INFO - __main__ - group texts input examples length40000 after_group size4761
04/11/2024 20:47:13 - INFO - __main__ - group texts input examples length40000 after_group size9876
04/11/2024 20:47:14 - INFO - __main__ - group texts input examples length40000 after_group size4113
04/11/2024 20:47:14 - INFO - __main__ - group texts input examples length40000 after_group size4755
04/11/2024 20:47:14 - INFO - __main__ - group texts input examples length40000 after_group size3714
04/11/2024 20:47:15 - INFO - __main__ - group texts input examples length40000 after_group size3180
04/11/2024 20:47:16 - INFO - __main__ - group texts input examples length40000 after_group size4765
04/11/2024 20:47:17 - INFO - __main__ - group texts input examples length40000 after_group size4786
04/11/2024 20:47:17 - INFO - __main__ - group texts input examples length40000 after_group size4121
04/11/2024 20:47:17 - INFO - __main__ - group texts input examples length40000 after_group size8787
04/11/2024 20:47:17 - INFO - __main__ - group texts input examples length40000 after_group size3161
04/11/2024 20:47:18 - INFO - __main__ - group texts input examples length40000 after_group size4750
04/11/2024 20:47:19 - INFO - __main__ - group texts input examples length40000 after_group size6568
04/11/2024 20:47:20 - INFO - __main__ - group texts input examples length40000 after_group size19308
04/11/2024 20:47:20 - INFO - __main__ - group texts input examples length40000 after_group size3159
04/11/2024 20:47:21 - INFO - __main__ - group texts input examples length40000 after_group size4771
04/11/2024 20:47:21 - INFO - __main__ - group texts input examples length40000 after_group size4138
04/11/2024 20:47:22 - INFO - __main__ - group texts input examples length40000 after_group size9921
04/11/2024 20:47:22 - INFO - __main__ - group texts input examples length40000 after_group size4764
04/11/2024 20:47:23 - INFO - __main__ - group texts input examples length40000 after_group size8425
04/11/2024 20:47:23 - INFO - __main__ - group texts input examples length40000 after_group size3166
04/11/2024 20:47:24 - INFO - __main__ - group texts input examples length40000 after_group size4118
04/11/2024 20:47:25 - INFO - __main__ - group texts input examples length40000 after_group size4767
04/11/2024 20:47:25 - INFO - __main__ - group texts input examples length40000 after_group size8701
04/11/2024 20:47:26 - INFO - __main__ - group texts input examples length40000 after_group size8382
04/11/2024 20:47:26 - INFO - __main__ - group texts input examples length40000 after_group size4758
04/11/2024 20:47:26 - INFO - __main__ - group texts input examples length40000 after_group size3160
04/11/2024 20:47:28 - INFO - __main__ - group texts input examples length40000 after_group size4123
04/11/2024 20:47:29 - INFO - __main__ - group texts input examples length40000 after_group size4768
04/11/2024 20:47:29 - INFO - __main__ - group texts input examples length40000 after_group size3162
04/11/2024 20:47:30 - INFO - __main__ - group texts input examples length40000 after_group size9804
04/11/2024 20:47:30 - INFO - __main__ - group texts input examples length40000 after_group size4763
04/11/2024 20:47:31 - INFO - __main__ - group texts input examples length40000 after_group size4135
04/11/2024 20:47:32 - INFO - __main__ - group texts input examples length40000 after_group size3186
04/11/2024 20:47:33 - INFO - __main__ - group texts input examples length40000 after_group size8795
04/11/2024 20:47:33 - INFO - __main__ - group texts input examples length40000 after_group size8384
04/11/2024 20:47:33 - INFO - __main__ - group texts input examples length40000 after_group size4761
04/11/2024 20:47:34 - INFO - __main__ - group texts input examples length40000 after_group size30884
04/11/2024 20:47:34 - INFO - __main__ - group texts input examples length40000 after_group size4769
04/11/2024 20:47:35 - INFO - __main__ - group texts input examples length40000 after_group size3173
04/11/2024 20:47:35 - INFO - __main__ - group texts input examples length40000 after_group size4134
04/11/2024 20:47:35 - INFO - __main__ - group texts input examples length40000 after_group size18841
04/11/2024 20:47:37 - INFO - __main__ - group texts input examples length40000 after_group size4767
04/11/2024 20:47:38 - INFO - __main__ - group texts input examples length40000 after_group size3178
04/11/2024 20:47:38 - INFO - __main__ - group texts input examples length40000 after_group size10029
04/11/2024 20:47:38 - INFO - __main__ - group texts input examples length40000 after_group size4129
04/11/2024 20:47:38 - INFO - __main__ - group texts input examples length40000 after_group size4753
04/11/2024 20:47:40 - INFO - __main__ - group texts input examples length40000 after_group size8787
04/11/2024 20:47:41 - INFO - __main__ - group texts input examples length40000 after_group size3190
04/11/2024 20:47:41 - INFO - __main__ - group texts input examples length40000 after_group size4776
04/11/2024 20:47:42 - INFO - __main__ - group texts input examples length40000 after_group size4131
04/11/2024 20:47:42 - INFO - __main__ - group texts input examples length40000 after_group size4759
04/11/2024 20:47:44 - INFO - __main__ - group texts input examples length40000 after_group size3184
04/11/2024 20:47:44 - INFO - __main__ - group texts input examples length40000 after_group size31649
04/11/2024 20:47:45 - INFO - __main__ - group texts input examples length40000 after_group size4750
04/11/2024 20:47:45 - INFO - __main__ - group texts input examples length40000 after_group size4120
04/11/2024 20:47:46 - INFO - __main__ - group texts input examples length40000 after_group size18933
04/11/2024 20:47:47 - INFO - __main__ - group texts input examples length40000 after_group size9820
04/11/2024 20:47:47 - INFO - __main__ - group texts input examples length40000 after_group size3188
04/11/2024 20:47:47 - INFO - __main__ - group texts input examples length40000 after_group size4768
04/11/2024 20:47:48 - INFO - __main__ - group texts input examples length40000 after_group size8740
04/11/2024 20:47:49 - INFO - __main__ - group texts input examples length40000 after_group size4152
04/11/2024 20:47:49 - INFO - __main__ - group texts input examples length40000 after_group size3157
04/11/2024 20:47:49 - INFO - __main__ - group texts input examples length40000 after_group size4748
04/11/2024 20:47:50 - INFO - __main__ - group texts input examples length40000 after_group size4758
04/11/2024 20:47:51 - INFO - __main__ - group texts input examples length40000 after_group size19534
04/11/2024 20:47:52 - INFO - __main__ - group texts input examples length40000 after_group size4158
04/11/2024 20:47:52 - INFO - __main__ - group texts input examples length40000 after_group size3194
04/11/2024 20:47:53 - INFO - __main__ - group texts input examples length40000 after_group size4771
04/11/2024 20:47:54 - INFO - __main__ - group texts input examples length40000 after_group size4761
04/11/2024 20:47:55 - INFO - __main__ - group texts input examples length40000 after_group size3197
04/11/2024 20:47:56 - INFO - __main__ - group texts input examples length40000 after_group size8774
04/11/2024 20:47:57 - INFO - __main__ - group texts input examples length40000 after_group size4765
04/11/2024 20:47:58 - INFO - __main__ - group texts input examples length40000 after_group size3213
04/11/2024 20:47:58 - INFO - __main__ - group texts input examples length40000 after_group size31180
04/11/2024 20:47:58 - INFO - __main__ - group texts input examples length40000 after_group size8121
04/11/2024 20:47:59 - INFO - __main__ - group texts input examples length40000 after_group size4761
04/11/2024 20:48:01 - INFO - __main__ - group texts input examples length40000 after_group size3177
04/11/2024 20:48:01 - INFO - __main__ - group texts input examples length40000 after_group size21448
04/11/2024 20:48:02 - INFO - __main__ - group texts input examples length40000 after_group size4770
04/11/2024 20:48:03 - INFO - __main__ - group texts input examples length40000 after_group size4771
04/11/2024 20:48:03 - INFO - __main__ - group texts input examples length40000 after_group size8802
04/11/2024 20:48:04 - INFO - __main__ - group texts input examples length40000 after_group size3162
04/11/2024 20:48:06 - INFO - __main__ - group texts input examples length40000 after_group size4774
04/11/2024 20:48:06 - INFO - __main__ - group texts input examples length40000 after_group size18926
04/11/2024 20:48:07 - INFO - __main__ - group texts input examples length40000 after_group size3179
04/11/2024 20:48:07 - INFO - __main__ - group texts input examples length40000 after_group size4760
04/11/2024 20:48:08 - INFO - __main__ - group texts input examples length40000 after_group size31513
04/11/2024 20:48:10 - INFO - __main__ - group texts input examples length40000 after_group size31567
04/11/2024 20:48:10 - INFO - __main__ - group texts input examples length40000 after_group size4765
04/11/2024 20:48:11 - INFO - __main__ - group texts input examples length40000 after_group size4770
04/11/2024 20:48:11 - INFO - __main__ - group texts input examples length40000 after_group size8776
04/11/2024 20:48:14 - INFO - __main__ - group texts input examples length40000 after_group size4750
04/11/2024 20:48:15 - INFO - __main__ - group texts input examples length40000 after_group size4753
04/11/2024 20:48:18 - INFO - __main__ - group texts input examples length40000 after_group size4762
04/11/2024 20:48:18 - INFO - __main__ - group texts input examples length40000 after_group size22745
04/11/2024 20:48:19 - INFO - __main__ - group texts input examples length40000 after_group size31347
04/11/2024 20:48:19 - INFO - __main__ - group texts input examples length40000 after_group size8778
04/11/2024 20:48:19 - INFO - __main__ - group texts input examples length40000 after_group size4770
04/11/2024 20:48:20 - INFO - __main__ - group texts input examples length40000 after_group size20489
04/11/2024 20:48:22 - INFO - __main__ - group texts input examples length40000 after_group size19346
04/11/2024 20:48:22 - INFO - __main__ - group texts input examples length40000 after_group size4773
04/11/2024 20:48:23 - INFO - __main__ - group texts input examples length40000 after_group size4778
04/11/2024 20:48:26 - INFO - __main__ - group texts input examples length40000 after_group size4765
04/11/2024 20:48:27 - INFO - __main__ - group texts input examples length40000 after_group size8762
04/11/2024 20:48:27 - INFO - __main__ - group texts input examples length40000 after_group size31839
04/11/2024 20:48:27 - INFO - __main__ - group texts input examples length40000 after_group size4771
04/11/2024 20:48:29 - INFO - __main__ - group texts input examples length40000 after_group size9737
04/11/2024 20:48:30 - INFO - __main__ - group texts input examples length40000 after_group size4764
04/11/2024 20:48:31 - INFO - __main__ - group texts input examples length40000 after_group size4770
04/11/2024 20:48:34 - INFO - __main__ - group texts input examples length40000 after_group size31410
04/11/2024 20:48:34 - INFO - __main__ - group texts input examples length40000 after_group size8754
04/11/2024 20:48:34 - INFO - __main__ - group texts input examples length40000 after_group size4770
04/11/2024 20:48:36 - INFO - __main__ - group texts input examples length40000 after_group size4776
04/11/2024 20:48:36 - INFO - __main__ - group texts input examples length40000 after_group size31136
04/11/2024 20:48:38 - INFO - __main__ - group texts input examples length40000 after_group size9958
04/11/2024 20:48:38 - INFO - __main__ - group texts input examples length40000 after_group size20966
04/11/2024 20:48:39 - INFO - __main__ - group texts input examples length40000 after_group size4764
04/11/2024 20:48:40 - INFO - __main__ - group texts input examples length40000 after_group size4764
04/11/2024 20:48:42 - INFO - __main__ - group texts input examples length40000 after_group size8776
04/11/2024 20:48:43 - INFO - __main__ - group texts input examples length40000 after_group size4761
04/11/2024 20:48:43 - INFO - __main__ - group texts input examples length40000 after_group size31260
04/11/2024 20:48:44 - INFO - __main__ - group texts input examples length40000 after_group size31419
04/11/2024 20:48:44 - INFO - __main__ - group texts input examples length40000 after_group size4766
04/11/2024 20:48:46 - INFO - __main__ - group texts input examples length40000 after_group size9771
04/11/2024 20:48:47 - INFO - __main__ - group texts input examples length40000 after_group size4765
04/11/2024 20:48:48 - INFO - __main__ - group texts input examples length40000 after_group size4761
04/11/2024 20:48:48 - INFO - __main__ - group texts input examples length7792 after_group size915
04/11/2024 20:48:49 - INFO - __main__ - group texts input examples length40000 after_group size8758
04/11/2024 20:48:52 - INFO - __main__ - group texts input examples length40000 after_group size4758
04/11/2024 20:48:53 - INFO - __main__ - group texts input examples length40000 after_group size31596
04/11/2024 20:48:54 - INFO - __main__ - group texts input examples length40000 after_group size9947
04/11/2024 20:48:54 - INFO - __main__ - group texts input examples length40000 after_group size19172
04/11/2024 20:48:54 - INFO - __main__ - group texts input examples length7792 after_group size6032
04/11/2024 20:48:56 - INFO - __main__ - group texts input examples length40000 after_group size4757
04/11/2024 20:48:57 - INFO - __main__ - group texts input examples length40000 after_group size8752
04/11/2024 20:48:59 - INFO - __main__ - group texts input examples length40000 after_group size31184
04/11/2024 20:49:00 - INFO - __main__ - group texts input examples length40000 after_group size4762
04/11/2024 20:49:01 - INFO - __main__ - group texts input examples length40000 after_group size9767
04/11/2024 20:49:02 - INFO - __main__ - group texts input examples length40000 after_group size31524
04/11/2024 20:49:02 - INFO - __main__ - group texts input examples length7792 after_group size928
04/11/2024 20:49:04 - INFO - __main__ - group texts input examples length40000 after_group size8743
04/11/2024 20:49:07 - INFO - __main__ - group texts input examples length40000 after_group size16614
04/11/2024 20:49:07 - INFO - __main__ - group texts input examples length40000 after_group size31361
04/11/2024 20:49:09 - INFO - __main__ - group texts input examples length40000 after_group size9669
04/11/2024 20:49:10 - INFO - __main__ - group texts input examples length40000 after_group size8730
04/11/2024 20:49:16 - INFO - __main__ - group texts input examples length40000 after_group size10150
04/11/2024 20:49:17 - INFO - __main__ - group texts input examples length40000 after_group size31597
04/11/2024 20:49:17 - INFO - __main__ - group texts input examples length40000 after_group size8754
04/11/2024 20:49:20 - INFO - __main__ - group texts input examples length40000 after_group size18720
04/11/2024 20:49:21 - INFO - __main__ - group texts input examples length40000 after_group size31339
04/11/2024 20:49:23 - INFO - __main__ - group texts input examples length40000 after_group size9837
04/11/2024 20:49:24 - INFO - __main__ - group texts input examples length40000 after_group size8737
04/11/2024 20:49:25 - INFO - __main__ - group texts input examples length40000 after_group size31340
04/11/2024 20:49:29 - INFO - __main__ - group texts input examples length40000 after_group size31372
04/11/2024 20:49:30 - INFO - __main__ - group texts input examples length40000 after_group size9671
04/11/2024 20:49:31 - INFO - __main__ - group texts input examples length40000 after_group size8726
04/11/2024 20:49:33 - INFO - __main__ - group texts input examples length40000 after_group size18549
04/11/2024 20:49:37 - INFO - __main__ - group texts input examples length40000 after_group size9931
04/11/2024 20:49:38 - INFO - __main__ - group texts input examples length40000 after_group size8734
04/11/2024 20:49:39 - INFO - __main__ - group texts input examples length40000 after_group size30907
04/11/2024 20:49:43 - INFO - __main__ - group texts input examples length40000 after_group size30881
04/11/2024 20:49:44 - INFO - __main__ - group texts input examples length40000 after_group size8752
04/11/2024 20:49:45 - INFO - __main__ - group texts input examples length40000 after_group size9941
04/11/2024 20:49:47 - INFO - __main__ - group texts input examples length40000 after_group size29770
04/11/2024 20:49:48 - INFO - __main__ - group texts input examples length40000 after_group size19453
04/11/2024 20:49:50 - INFO - __main__ - group texts input examples length40000 after_group size8750
04/11/2024 20:49:51 - INFO - __main__ - group texts input examples length40000 after_group size31436
04/11/2024 20:49:52 - INFO - __main__ - group texts input examples length40000 after_group size9722
04/11/2024 20:49:55 - INFO - __main__ - group texts input examples length40000 after_group size4305
04/11/2024 20:49:57 - INFO - __main__ - group texts input examples length40000 after_group size8770
04/11/2024 20:49:59 - INFO - __main__ - group texts input examples length40000 after_group size9706
04/11/2024 20:50:00 - INFO - __main__ - group texts input examples length40000 after_group size16866
04/11/2024 20:50:01 - INFO - __main__ - group texts input examples length40000 after_group size28102
04/11/2024 20:50:03 - INFO - __main__ - group texts input examples length40000 after_group size8504
04/11/2024 20:50:04 - INFO - __main__ - group texts input examples length40000 after_group size14458
04/11/2024 20:50:06 - INFO - __main__ - group texts input examples length40000 after_group size30600
04/11/2024 20:50:06 - INFO - __main__ - group texts input examples length40000 after_group size9939
04/11/2024 20:50:07 - INFO - __main__ - group texts input examples length40000 after_group size4112
04/11/2024 20:50:10 - INFO - __main__ - group texts input examples length40000 after_group size4156
04/11/2024 20:50:11 - INFO - __main__ - group texts input examples length40000 after_group size8727
04/11/2024 20:50:11 - INFO - __main__ - group texts input examples length40000 after_group size27270
04/11/2024 20:50:12 - INFO - __main__ - group texts input examples length7792 after_group size793
04/11/2024 20:50:13 - INFO - __main__ - group texts input examples length40000 after_group size18789
04/11/2024 20:50:13 - INFO - __main__ - group texts input examples length40000 after_group size9858
04/11/2024 20:50:17 - INFO - __main__ - group texts input examples length40000 after_group size8752
04/11/2024 20:50:20 - INFO - __main__ - group texts input examples length40000 after_group size9851
04/11/2024 20:50:23 - INFO - __main__ - group texts input examples length40000 after_group size8766
04/11/2024 20:50:23 - INFO - __main__ - group texts input examples length40000 after_group size31274
04/11/2024 20:50:26 - INFO - __main__ - group texts input examples length40000 after_group size19072
04/11/2024 20:50:27 - INFO - __main__ - group texts input examples length40000 after_group size9971
04/11/2024 20:50:27 - INFO - __main__ - group texts input examples length40000 after_group size30924
04/11/2024 20:50:30 - INFO - __main__ - group texts input examples length40000 after_group size8786
04/11/2024 20:50:32 - INFO - __main__ - group texts input examples length40000 after_group size31697
04/11/2024 20:50:34 - INFO - __main__ - group texts input examples length40000 after_group size9829
04/11/2024 20:50:36 - INFO - __main__ - group texts input examples length40000 after_group size8727
04/11/2024 20:50:39 - INFO - __main__ - group texts input examples length40000 after_group size18756
04/11/2024 20:50:41 - INFO - __main__ - group texts input examples length40000 after_group size9938
04/11/2024 20:50:42 - INFO - __main__ - group texts input examples length40000 after_group size8759
04/11/2024 20:50:45 - INFO - __main__ - group texts input examples length40000 after_group size31454
04/11/2024 20:50:47 - INFO - __main__ - group texts input examples length40000 after_group size30339
04/11/2024 20:50:48 - INFO - __main__ - group texts input examples length40000 after_group size9844
04/11/2024 20:50:48 - INFO - __main__ - group texts input examples length40000 after_group size8735
04/11/2024 20:50:52 - INFO - __main__ - group texts input examples length40000 after_group size18756
04/11/2024 20:50:53 - INFO - __main__ - group texts input examples length40000 after_group size31613
04/11/2024 20:50:54 - INFO - __main__ - group texts input examples length40000 after_group size8763
04/11/2024 20:50:55 - INFO - __main__ - group texts input examples length40000 after_group size9736
04/11/2024 20:51:01 - INFO - __main__ - group texts input examples length40000 after_group size8760
04/11/2024 20:51:01 - INFO - __main__ - group texts input examples length40000 after_group size9995
04/11/2024 20:51:05 - INFO - __main__ - group texts input examples length40000 after_group size18101
04/11/2024 20:51:07 - INFO - __main__ - group texts input examples length40000 after_group size31271
04/11/2024 20:51:07 - INFO - __main__ - group texts input examples length40000 after_group size8741
04/11/2024 20:51:08 - INFO - __main__ - group texts input examples length40000 after_group size9935
04/11/2024 20:51:09 - INFO - __main__ - group texts input examples length40000 after_group size30671
04/11/2024 20:51:13 - INFO - __main__ - group texts input examples length40000 after_group size8722
04/11/2024 20:51:15 - INFO - __main__ - group texts input examples length40000 after_group size31264
04/11/2024 20:51:15 - INFO - __main__ - group texts input examples length40000 after_group size9974
04/11/2024 20:51:19 - INFO - __main__ - group texts input examples length40000 after_group size20599
04/11/2024 20:51:19 - INFO - __main__ - group texts input examples length40000 after_group size8744
04/11/2024 20:51:23 - INFO - __main__ - group texts input examples length40000 after_group size10096
04/11/2024 20:51:25 - INFO - __main__ - group texts input examples length7792 after_group size1827
04/11/2024 20:51:26 - INFO - __main__ - group texts input examples length40000 after_group size8737
04/11/2024 20:51:28 - INFO - __main__ - group texts input examples length40000 after_group size31743
04/11/2024 20:51:30 - INFO - __main__ - group texts input examples length40000 after_group size31104
04/11/2024 20:51:32 - INFO - __main__ - group texts input examples length40000 after_group size8725
04/11/2024 20:51:35 - INFO - __main__ - group texts input examples length40000 after_group size30927
04/11/2024 20:51:36 - INFO - __main__ - group texts input examples length40000 after_group size28134
04/11/2024 20:51:38 - INFO - __main__ - group texts input examples length40000 after_group size8770
04/11/2024 20:51:43 - INFO - __main__ - group texts input examples length7792 after_group size6160
04/11/2024 20:51:43 - INFO - __main__ - group texts input examples length40000 after_group size8822
04/11/2024 20:51:46 - INFO - __main__ - group texts input examples length7792 after_group size1736
04/11/2024 20:51:49 - INFO - __main__ - group texts input examples length40000 after_group size31298
04/11/2024 20:51:50 - INFO - __main__ - group texts input examples length40000 after_group size31846
04/11/2024 20:51:56 - INFO - __main__ - group texts input examples length40000 after_group size31537
04/11/2024 20:52:04 - INFO - __main__ - group texts input examples length40000 after_group size22037
04/11/2024 20:52:10 - INFO - __main__ - group texts input examples length40000 after_group size31198
04/11/2024 20:52:15 - INFO - __main__ - group texts input examples length40000 after_group size31640
04/11/2024 20:52:16 - INFO - __main__ - group texts input examples length40000 after_group size19202
04/11/2024 20:52:28 - INFO - __main__ - group texts input examples length40000 after_group size19364
04/11/2024 20:52:29 - INFO - __main__ - group texts input examples length40000 after_group size31109
04/11/2024 20:52:34 - INFO - __main__ - group texts input examples length40000 after_group size31658
04/11/2024 20:52:40 - INFO - __main__ - group texts input examples length40000 after_group size19700
04/11/2024 20:52:48 - INFO - __main__ - group texts input examples length40000 after_group size31315
04/11/2024 20:52:52 - INFO - __main__ - group texts input examples length40000 after_group size18876
04/11/2024 20:52:54 - INFO - __main__ - group texts input examples length40000 after_group size31429
04/11/2024 20:52:57 - INFO - __main__ - group texts input examples length7791 after_group size3319
04/11/2024 20:53:08 - INFO - __main__ - group texts input examples length40000 after_group size31671
04/11/2024 20:53:12 - INFO - __main__ - group texts input examples length40000 after_group size31385
04/11/2024 20:53:28 - INFO - __main__ - group texts input examples length40000 after_group size32051
04/11/2024 20:53:31 - INFO - __main__ - group texts input examples length40000 after_group size31846
04/11/2024 20:53:46 - INFO - __main__ - group texts input examples length40000 after_group size31225
04/11/2024 20:53:50 - INFO - __main__ - group texts input examples length40000 after_group size31549
04/11/2024 20:54:02 - INFO - __main__ - group texts input examples length40000 after_group size24745
04/11/2024 20:54:07 - INFO - __main__ - group texts input examples length40000 after_group size3714
04/11/2024 20:54:08 - INFO - __main__ - group texts input examples length40000 after_group size30530
04/11/2024 20:54:10 - INFO - __main__ - group texts input examples length40000 after_group size3682
04/11/2024 20:54:11 - INFO - __main__ - group texts input examples length7791 after_group size725
04/11/2024 20:54:26 - INFO - __main__ - group texts input examples length40000 after_group size31329
04/11/2024 20:54:34 - INFO - __main__ - group texts input examples length7791 after_group size6271
04/11/2024 20:54:35 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00000_of_00010.arrow
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00001_of_00010.arrow
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00002_of_00010.arrow
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00003_of_00010.arrow
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00004_of_00010.arrow
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00005_of_00010.arrow
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00006_of_00010.arrow
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00007_of_00010.arrow
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00008_of_00010.arrow
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00009_of_00010.arrow
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Spawning 10 processes
04/11/2024 20:54:36 - INFO - __main__ - group texts input examples length20 after_group size3
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00000_of_00010.arrow
04/11/2024 20:54:36 - INFO - __main__ - group texts input examples length20 after_group size3
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00001_of_00010.arrow
04/11/2024 20:54:36 - INFO - __main__ - group texts input examples length20 after_group size3
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00002_of_00010.arrow
04/11/2024 20:54:36 - INFO - __main__ - group texts input examples length20 after_group size4
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00003_of_00010.arrow
04/11/2024 20:54:36 - INFO - __main__ - group texts input examples length20 after_group size3
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00004_of_00010.arrow
04/11/2024 20:54:36 - INFO - __main__ - group texts input examples length20 after_group size3
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00005_of_00010.arrow
04/11/2024 20:54:36 - INFO - __main__ - group texts input examples length20 after_group size2
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00007_of_00010.arrow
04/11/2024 20:54:36 - INFO - __main__ - group texts input examples length20 after_group size3
04/11/2024 20:54:36 - INFO - __main__ - group texts input examples length20 after_group size2
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00008_of_00010.arrow
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00006_of_00010.arrow
04/11/2024 20:54:36 - INFO - __main__ - group texts input examples length20 after_group size3
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00009_of_00010.arrow
04/11/2024 20:54:36 - INFO - datasets.arrow_dataset - Concatenating 10 shards
0 start select train_dataset
0 end select train_dataset
0 start select eval_dataset
0 end select eval_dataset
0 start load metric
0 end load metric
0 Initialize our Trainer
0 start train
[2024-04-11 20:54:37,188] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-04-11 20:54:37,245] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
ninja: no work to do.
Time to load fused_adam op: 0.060300588607788086 seconds
[2024-04-11 20:54:37,605] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2024-04-11 20:54:37,605] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-04-11 20:54:37,613] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-04-11 20:54:37,613] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2024-04-11 20:54:37,613] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-04-11 20:54:37,613] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-04-11 20:54:37,684] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
[2024-04-11 20:54:37,685] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 1.07 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-11 20:54:37,685] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 1.69 GB, percent = 7.2%
[2024-04-11 20:54:37,687] [INFO] [stage3.py:130:__init__] Reduce bucket size 589824
[2024-04-11 20:54:37,687] [INFO] [stage3.py:131:__init__] Prefetch bucket size 530841
[2024-04-11 20:54:37,755] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-04-11 20:54:37,756] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.74 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-11 20:54:37,756] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 1.69 GB, percent = 7.2%
Parameter Offload: Total persistent parameters: 49920 in 65 params
[2024-04-11 20:54:37,865] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-04-11 20:54:37,866] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.82 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-11 20:54:37,866] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 1.69 GB, percent = 7.2%
[2024-04-11 20:54:37,963] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
[2024-04-11 20:54:37,964] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.74 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-11 20:54:37,964] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 1.69 GB, percent = 7.2%
[2024-04-11 20:54:38,426] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 1
[2024-04-11 20:54:38,427] [INFO] [utils.py:801:see_memory_usage] MA 0.72 GB         Max_MA 0.74 GB         CA 0.73 GB         Max_CA 1 GB 
[2024-04-11 20:54:38,427] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 1.69 GB, percent = 7.2%
[2024-04-11 20:54:38,496] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
[2024-04-11 20:54:38,497] [INFO] [utils.py:801:see_memory_usage] MA 0.72 GB         Max_MA 0.72 GB         CA 0.73 GB         Max_CA 1 GB 
[2024-04-11 20:54:38,497] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 1.69 GB, percent = 7.2%
[2024-04-11 20:54:38,579] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
[2024-04-11 20:54:38,579] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 2.89 GB         CA 2.9 GB         Max_CA 3 GB 
[2024-04-11 20:54:38,579] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 1.69 GB, percent = 7.2%
[2024-04-11 20:54:38,649] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-04-11 20:54:38,650] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 2.17 GB         CA 2.9 GB         Max_CA 3 GB 
[2024-04-11 20:54:38,650] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 1.69 GB, percent = 7.2%
[2024-04-11 20:54:38,726] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-04-11 20:54:38,726] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 3.61 GB         CA 4.34 GB         Max_CA 4 GB 
[2024-04-11 20:54:38,726] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 1.69 GB, percent = 7.2%
[2024-04-11 20:54:38,727] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-04-11 20:54:38,841] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-04-11 20:54:38,842] [INFO] [utils.py:801:see_memory_usage] MA 2.89 GB         Max_MA 3.05 GB         CA 4.34 GB         Max_CA 4 GB 
[2024-04-11 20:54:38,842] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 1.69 GB, percent = 7.2%
[2024-04-11 20:54:38,842] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2024-04-11 20:54:38,842] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-04-11 20:54:38,842] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fb930e58af0>
[2024-04-11 20:54:38,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2024-04-11 20:54:38,843] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-04-11 20:54:38,843] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-04-11 20:54:38,843] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-04-11 20:54:38,843] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-04-11 20:54:38,843] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-04-11 20:54:38,843] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb84174a320>
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 2
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupDecayLR
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   scheduler_params ............. {'last_batch_iteration': -1, 'total_num_steps': 342570, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 5000}
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  16
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-04-11 20:54:38,844] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-04-11 20:54:38,845] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=589824 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=530841 param_persistence_threshold=7680 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-04-11 20:54:38,845] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-04-11 20:54:38,845] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-04-11 20:54:38,845] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-04-11 20:54:38,845] [INFO] [config.py:986:print_user_config]   json = {
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "fp16_opt_level": "O2"
    }, 
    "bf16": {
        "enabled": true
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0001, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0.0
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "last_batch_iteration": -1, 
            "total_num_steps": 3.425700e+05, 
            "warmup_min_lr": 0, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 5.000000e+03
        }
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 5.898240e+05, 
        "stage3_prefetch_bucket_size": 5.308416e+05, 
        "stage3_param_persistence_threshold": 7.680000e+03, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 2, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 16, 
    "wall_clock_breakdown": false
}
{'loss': 10.6353, 'grad_norm': 4.655247930923011, 'learning_rate': 1.889634150903278e-05, 'epoch': 0.0}
{'loss': 9.8939, 'grad_norm': 1.9957714149411987, 'learning_rate': 2.7034552830322406e-05, 'epoch': 0.0}
{'loss': 9.6058, 'grad_norm': 1.8818135446054385, 'learning_rate': 3.1795101276221204e-05, 'epoch': 0.0}
{'loss': 9.4216, 'grad_norm': 1.7721421450684345, 'learning_rate': 3.5172764151612024e-05, 'epoch': 0.0}
{'loss': 9.2494, 'grad_norm': 1.6553965610466514, 'learning_rate': 3.779268301806556e-05, 'epoch': 0.0}
{'loss': 9.0517, 'grad_norm': 1.6581655304332106, 'learning_rate': 3.993331259751083e-05, 'epoch': 0.0}
{'loss': 8.841, 'grad_norm': 1.552362656513466, 'learning_rate': 4.1743189118600124e-05, 'epoch': 0.0}
{'loss': 8.6135, 'grad_norm': 1.4849955377233082, 'learning_rate': 4.331097547290165e-05, 'epoch': 0.0}
{'loss': 8.3813, 'grad_norm': 1.194422440539713, 'learning_rate': 4.4693861043409635e-05, 'epoch': 0.0}
{'loss': 8.1421, 'grad_norm': 1.4782651467038914, 'learning_rate': 4.5930894339355186e-05, 'epoch': 0.0}
{'loss': 7.8798, 'grad_norm': 2.0615423875380845, 'learning_rate': 4.7049927073054124e-05, 'epoch': 0.0}
{'loss': 7.6598, 'grad_norm': 1.955552598599647, 'learning_rate': 4.8071523918800455e-05, 'epoch': 0.0}
{'loss': 7.4967, 'grad_norm': 0.8256668110209954, 'learning_rate': 4.9011301916958396e-05, 'epoch': 0.0}
{'loss': 7.3076, 'grad_norm': 1.001901902868374, 'learning_rate': 4.9881400439889756e-05, 'epoch': 0.0}
{'loss': 7.2117, 'grad_norm': 0.8749889698717497, 'learning_rate': 5.0691442785253985e-05, 'epoch': 0.0}
{'loss': 7.0622, 'grad_norm': 0.6067366450525028, 'learning_rate': 5.1449186794191275e-05, 'epoch': 0.0}
{'loss': 6.966, 'grad_norm': 0.7646159690509339, 'learning_rate': 5.2160977879046945e-05, 'epoch': 0.0}
{'loss': 6.8487, 'grad_norm': 0.792022291435417, 'learning_rate': 5.283207236469926e-05, 'epoch': 0.0}
{'loss': 6.7465, 'grad_norm': 1.1708205192656944, 'learning_rate': 5.3466873290957045e-05, 'epoch': 0.0}
{'loss': 6.6423, 'grad_norm': 0.8789898042054528, 'learning_rate': 5.406910566064481e-05, 'epoch': 0.0}
[2024-04-11 20:59:15,518] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!
[2024-04-11 20:59:15,528] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 20:59:15,528] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 20:59:15,535] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 20:59:15,535] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 20:59:19,305] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 20:59:19,306] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 20:59:19,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
{'loss': 6.5764, 'grad_norm': 2.05619175675842, 'learning_rate': 5.4641948885788554e-05, 'epoch': 0.0}
{'loss': 6.4556, 'grad_norm': 1.4683146780562606, 'learning_rate': 5.518813839434375e-05, 'epoch': 0.0}
{'loss': 6.3619, 'grad_norm': 1.0485388321747044, 'learning_rate': 5.5710044632370994e-05, 'epoch': 0.0}
{'loss': 6.3131, 'grad_norm': 1.4928863262745773, 'learning_rate': 5.620973524009008e-05, 'epoch': 0.0}
{'loss': 6.2409, 'grad_norm': 1.0295340452036315, 'learning_rate': 5.668902452709834e-05, 'epoch': 0.0}
{'loss': 6.2412, 'grad_norm': 1.045863108480133, 'learning_rate': 5.714951323824802e-05, 'epoch': 0.0}
{'loss': 6.1199, 'grad_norm': 0.7886848112000857, 'learning_rate': 5.7592620810598065e-05, 'epoch': 0.0}
{'loss': 6.0206, 'grad_norm': 0.7912439217671491, 'learning_rate': 5.8019611761179374e-05, 'epoch': 0.0}
{'loss': 5.9928, 'grad_norm': 1.1026805273997036, 'learning_rate': 5.843161744218982e-05, 'epoch': 0.0}
{'loss': 5.9693, 'grad_norm': 0.650405771117456, 'learning_rate': 5.882965410654361e-05, 'epoch': 0.0}
{'loss': 5.9314, 'grad_norm': 1.2537073943544352, 'learning_rate': 5.9214638010114533e-05, 'epoch': 0.0}
{'loss': 5.8617, 'grad_norm': 0.8633536402417523, 'learning_rate': 5.95873981154809e-05, 'epoch': 0.0}
{'loss': 5.8461, 'grad_norm': 0.6963202553372674, 'learning_rate': 5.994868684024254e-05, 'epoch': 0.0}
{'loss': 5.8595, 'grad_norm': 1.1329626352898277, 'learning_rate': 6.029918920033657e-05, 'epoch': 0.0}
{'loss': 5.7823, 'grad_norm': 0.618248068639628, 'learning_rate': 6.063953062763291e-05, 'epoch': 0.0}
{'loss': 5.7696, 'grad_norm': 0.7119118011309672, 'learning_rate': 6.0970283685988885e-05, 'epoch': 0.0}
{'loss': 5.7236, 'grad_norm': 0.7846247896992686, 'learning_rate': 6.129197386692462e-05, 'epoch': 0.0}
{'loss': 5.6734, 'grad_norm': 0.8346090391104203, 'learning_rate': 6.160508461224668e-05, 'epoch': 0.0}
{'loss': 5.6552, 'grad_norm': 0.6559823882462459, 'learning_rate': 6.191006168414683e-05, 'epoch': 0.0}
{'loss': 5.6695, 'grad_norm': 1.0026053372964265, 'learning_rate': 6.220731698193443e-05, 'epoch': 0.0}
[2024-04-11 21:03:55,782] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2024-04-11 21:03:55,795] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 21:03:55,795] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 21:03:55,801] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 21:03:55,802] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 21:03:59,482] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 21:03:59,482] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 21:03:59,482] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
{'loss': 5.6436, 'grad_norm': 0.9390757873495791, 'learning_rate': 6.249723188741359e-05, 'epoch': 0.0}
{'loss': 5.6046, 'grad_norm': 0.8348342884007791, 'learning_rate': 6.278016020707817e-05, 'epoch': 0.0}
{'loss': 5.5611, 'grad_norm': 0.9052620168234082, 'learning_rate': 6.305643076806426e-05, 'epoch': 0.0}
{'loss': 5.5199, 'grad_norm': 1.0660483249378474, 'learning_rate': 6.332634971563337e-05, 'epoch': 0.0}
{'loss': 5.5209, 'grad_norm': 0.5759580774514904, 'learning_rate': 6.359020255244241e-05, 'epoch': 0.0}
{'loss': 5.4546, 'grad_norm': 0.755211538146084, 'learning_rate': 6.384825595366063e-05, 'epoch': 0.0}
{'loss': 5.4222, 'grad_norm': 0.7156300031759998, 'learning_rate': 6.410075938686486e-05, 'epoch': 0.0}
{'loss': 5.4211, 'grad_norm': 0.9046750925620086, 'learning_rate': 6.43479465613797e-05, 'epoch': 0.0}
{'loss': 5.4349, 'grad_norm': 0.7067158686295556, 'learning_rate': 6.459003672816747e-05, 'epoch': 0.0}
{'loss': 5.4252, 'grad_norm': 0.6290354035651256, 'learning_rate': 6.482723584838796e-05, 'epoch': 0.0}
{'loss': 5.4127, 'grad_norm': 0.7237298320333263, 'learning_rate': 6.505973764623537e-05, 'epoch': 0.0}
{'loss': 5.347, 'grad_norm': 0.6762728423120468, 'learning_rate': 6.528772455953764e-05, 'epoch': 0.0}
{'loss': 5.3063, 'grad_norm': 0.78507807011244, 'learning_rate': 6.551136859980541e-05, 'epoch': 0.0}
{'loss': 5.3264, 'grad_norm': 0.6975825567643948, 'learning_rate': 6.573083213188768e-05, 'epoch': 0.0}
{'loss': 5.3281, 'grad_norm': 1.1582188775608544, 'learning_rate': 6.59462685820869e-05, 'epoch': 0.0}
{'loss': 5.3079, 'grad_norm': 0.7530232807424014, 'learning_rate': 6.6157823082469e-05, 'epoch': 0.0}
{'loss': 5.3272, 'grad_norm': 0.7208713245467339, 'learning_rate': 6.636563305814549e-05, 'epoch': 0.0}
{'loss': 5.2213, 'grad_norm': 0.8121766737747939, 'learning_rate': 6.656982876347945e-05, 'epoch': 0.0}
{'loss': 5.2149, 'grad_norm': 0.8075061422831968, 'learning_rate': 6.677053377245504e-05, 'epoch': 0.0}
{'loss': 5.2731, 'grad_norm': 1.2333543784639087, 'learning_rate': 6.696786542783324e-05, 'epoch': 0.0}
[2024-04-11 21:08:36,522] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step300 is about to be saved!
[2024-04-11 21:08:36,528] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 21:08:36,528] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 21:08:36,535] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 21:08:36,535] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 21:08:40,133] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 21:08:40,133] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 21:08:40,133] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step300 is ready now!
{'loss': 5.2805, 'grad_norm': 1.0367873789898066, 'learning_rate': 6.716193525318215e-05, 'epoch': 0.0}
{'loss': 5.1872, 'grad_norm': 0.7407050765216165, 'learning_rate': 6.735284933140416e-05, 'epoch': 0.0}
{'loss': 5.2065, 'grad_norm': 0.8952977477982795, 'learning_rate': 6.754070865297698e-05, 'epoch': 0.0}
{'loss': 5.1679, 'grad_norm': 0.6563654948833497, 'learning_rate': 6.772560943677052e-05, 'epoch': 0.0}
{'loss': 5.1665, 'grad_norm': 1.0109314193752668, 'learning_rate': 6.790764342599117e-05, 'epoch': 0.0}
{'loss': 5.1382, 'grad_norm': 0.9745243454390573, 'learning_rate': 6.808689816153217e-05, 'epoch': 0.0}
{'loss': 5.09, 'grad_norm': 0.6479642848599455, 'learning_rate': 6.826345723476884e-05, 'epoch': 0.0}
{'loss': 5.1221, 'grad_norm': 0.6298823365695501, 'learning_rate': 6.84374005216262e-05, 'epoch': 0.0}
{'loss': 5.0678, 'grad_norm': 0.6831222508756801, 'learning_rate': 6.860880439955943e-05, 'epoch': 0.0}
{'loss': 5.1133, 'grad_norm': 0.9582860300962194, 'learning_rate': 6.877774194892253e-05, 'epoch': 0.0}
{'loss': 5.0538, 'grad_norm': 1.051581393698796, 'learning_rate': 6.894428314005404e-05, 'epoch': 0.0}
{'loss': 5.011, 'grad_norm': 0.7270717108298248, 'learning_rate': 6.910849500727851e-05, 'epoch': 0.0}
{'loss': 5.0664, 'grad_norm': 0.7750021433702424, 'learning_rate': 6.927044181090669e-05, 'epoch': 0.0}
{'loss': 5.0371, 'grad_norm': 0.8302354180280779, 'learning_rate': 6.943018518821426e-05, 'epoch': 0.0}
[2024-04-11 22:32:01,163] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 22:32:01,789] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-11 22:32:01,789] [INFO] [runner.py:568:main] cmd = /home/cc/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --config_name my_model/config.json --tokenizer_name YeungNLP/firefly-llama2-7b-base --train_files data/训练数据/train_all_data_random.csv data/训练数据/train_news_chunk15.csv data/训练数据/train_news_chunk4.csv data/训练数据/train_my_baike_qa.csv data/训练数据/train_news_chunk16.csv data/训练数据/train_news_chunk5.csv data/训练数据/train_my_belll_3M_cn.csv data/训练数据/train_news_chunk17.csv data/训练数据/train_news_chunk6.csv data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv data/训练数据/train_news_chunk18.csv data/训练数据/train_news_chunk7.csv data/训练数据/train_my_web_text_zh.csv data/训练数据/train_news_chunk19.csv data/训练数据/train_news_chunk8.csv data/训练数据/train_news_chunk1.csv data/训练数据/train_news_chunk2.csv data/训练数据/train_news_chunk9.csv data/训练数据/train_news_chunk10.csv data/训练数据/train_news_chunk20.csv data/训练数据/train_sft_train.csv data/训练数据/train_news_chunk11.csv data/训练数据/train_news_chunk21.csv data/训练数据/train_web_poetry.csv data/训练数据/train_news_chunk12.csv data/训练数据/train_news_chunk22.csv data/训练数据/train_zhihu_kol.csv data/训练数据/train_news_chunk13.csv data/训练数据/train_news_chunk23.csv data/训练数据/train_news_chunk14.csv data/训练数据/train_news_chunk3.csv --validation_files data/dev_sft.csv --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --do_train --output_dir output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 100 --eval_steps 5000000 --save_total_limit 10 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2024-04-11 22:32:03,112] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 22:32:03,336] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2024-04-11 22:32:03,336] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-04-11 22:32:03,336] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-11 22:32:03,336] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-11 22:32:03,336] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-11 22:32:03,336] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-04-11 22:32:03,337] [INFO] [launch.py:253:main] process 776 spawned with command: ['/home/cc/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', 'my_model/config.json', '--tokenizer_name', 'YeungNLP/firefly-llama2-7b-base', '--train_files', 'data/训练数据/train_all_data_random.csv', 'data/训练数据/train_news_chunk15.csv', 'data/训练数据/train_news_chunk4.csv', 'data/训练数据/train_my_baike_qa.csv', 'data/训练数据/train_news_chunk16.csv', 'data/训练数据/train_news_chunk5.csv', 'data/训练数据/train_my_belll_3M_cn.csv', 'data/训练数据/train_news_chunk17.csv', 'data/训练数据/train_news_chunk6.csv', 'data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv', 'data/训练数据/train_news_chunk18.csv', 'data/训练数据/train_news_chunk7.csv', 'data/训练数据/train_my_web_text_zh.csv', 'data/训练数据/train_news_chunk19.csv', 'data/训练数据/train_news_chunk8.csv', 'data/训练数据/train_news_chunk1.csv', 'data/训练数据/train_news_chunk2.csv', 'data/训练数据/train_news_chunk9.csv', 'data/训练数据/train_news_chunk10.csv', 'data/训练数据/train_news_chunk20.csv', 'data/训练数据/train_sft_train.csv', 'data/训练数据/train_news_chunk11.csv', 'data/训练数据/train_news_chunk21.csv', 'data/训练数据/train_web_poetry.csv', 'data/训练数据/train_news_chunk12.csv', 'data/训练数据/train_news_chunk22.csv', 'data/训练数据/train_zhihu_kol.csv', 'data/训练数据/train_news_chunk13.csv', 'data/训练数据/train_news_chunk23.csv', 'data/训练数据/train_news_chunk14.csv', 'data/训练数据/train_news_chunk3.csv', '--validation_files', 'data/dev_sft.csv', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '16', '--do_train', '--output_dir', 'output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', 'output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '100', '--eval_steps', '5000000', '--save_total_limit', '10', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', 'output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2024-04-11 22:32:05,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-11 22:32:05,985] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-11 22:32:05,985] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
04/11/2024 22:32:06 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
04/11/2024 22:32:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=18000000,
debug=[],
deepspeed=./ds_config_zero3.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=5000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=True,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_model/logs,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=output_model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=16,
per_device_train_batch_size=16,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=output_model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=10,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=5000,
weight_decay=0.0,
)
['data/训练数据/train_all_data_random.csv', 'data/训练数据/train_news_chunk15.csv', 'data/训练数据/train_news_chunk4.csv', 'data/训练数据/train_my_baike_qa.csv', 'data/训练数据/train_news_chunk16.csv', 'data/训练数据/train_news_chunk5.csv', 'data/训练数据/train_my_belll_3M_cn.csv', 'data/训练数据/train_news_chunk17.csv', 'data/训练数据/train_news_chunk6.csv', 'data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv', 'data/训练数据/train_news_chunk18.csv', 'data/训练数据/train_news_chunk7.csv', 'data/训练数据/train_my_web_text_zh.csv', 'data/训练数据/train_news_chunk19.csv', 'data/训练数据/train_news_chunk8.csv', 'data/训练数据/train_news_chunk1.csv', 'data/训练数据/train_news_chunk2.csv', 'data/训练数据/train_news_chunk9.csv', 'data/训练数据/train_news_chunk10.csv', 'data/训练数据/train_news_chunk20.csv', 'data/训练数据/train_sft_train.csv', 'data/训练数据/train_news_chunk11.csv', 'data/训练数据/train_news_chunk21.csv', 'data/训练数据/train_web_poetry.csv', 'data/训练数据/train_news_chunk12.csv', 'data/训练数据/train_news_chunk22.csv', 'data/训练数据/train_zhihu_kol.csv', 'data/训练数据/train_news_chunk13.csv', 'data/训练数据/train_news_chunk23.csv', 'data/训练数据/train_news_chunk14.csv', 'data/训练数据/train_news_chunk3.csv']
训练文件总个数 31
04/11/2024 22:32:08 - INFO - datasets.builder - Using custom data configuration default-1187581384cc2f62
04/11/2024 22:32:08 - INFO - datasets.info - Loading Dataset Infos from /home/cc/anaconda3/envs/llama-chinese/lib/python3.10/site-packages/datasets/packaged_modules/csv
04/11/2024 22:32:08 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/11/2024 22:32:08 - INFO - datasets.info - Loading Dataset info from output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
04/11/2024 22:32:08 - INFO - datasets.builder - Found cached dataset csv (/home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6)
04/11/2024 22:32:08 - INFO - datasets.info - Loading Dataset info from /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
0 start load tokenizer
0 end load tokenizer
0 start load model
[2024-04-11 22:32:12,714] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 291, num_elems = 0.39B
04/11/2024 22:32:12 - INFO - __main__ - Training new model from scratch - Total size=0.00M params
0 end load model
['text']
04/11/2024 22:32:12 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00000_of_00010.arrow
04/11/2024 22:32:12 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00001_of_00010.arrow
04/11/2024 22:32:12 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00002_of_00010.arrow
04/11/2024 22:32:12 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00003_of_00010.arrow
04/11/2024 22:32:12 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00004_of_00010.arrow
04/11/2024 22:32:12 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00005_of_00010.arrow
04/11/2024 22:32:12 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00006_of_00010.arrow
04/11/2024 22:32:12 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00007_of_00010.arrow
04/11/2024 22:32:12 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00008_of_00010.arrow
04/11/2024 22:32:12 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00009_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_*_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00000_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00001_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00002_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00003_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00004_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00005_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00006_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00007_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00008_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00009_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_*_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00000_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00001_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00002_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00003_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00004_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00005_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00006_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00007_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00008_of_00010.arrow
04/11/2024 22:32:18 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00009_of_00010.arrow
04/11/2024 22:32:20 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_*_of_00010.arrow
04/11/2024 22:32:20 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/11/2024 22:32:21 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00000_of_00010.arrow
04/11/2024 22:32:21 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00001_of_00010.arrow
04/11/2024 22:32:21 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00002_of_00010.arrow
04/11/2024 22:32:21 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00003_of_00010.arrow
04/11/2024 22:32:21 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00004_of_00010.arrow
04/11/2024 22:32:21 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00005_of_00010.arrow
04/11/2024 22:32:21 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00006_of_00010.arrow
04/11/2024 22:32:21 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00007_of_00010.arrow
04/11/2024 22:32:21 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00008_of_00010.arrow
04/11/2024 22:32:21 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00009_of_00010.arrow
04/11/2024 22:32:21 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_*_of_00010.arrow
04/11/2024 22:32:21 - INFO - datasets.arrow_dataset - Concatenating 10 shards
0 start select train_dataset
0 end select train_dataset
0 start select eval_dataset
0 end select eval_dataset
0 start load metric
0 end load metric
0 Initialize our Trainer
0 start train
[2024-04-11 22:32:21,155] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-04-11 22:32:21,164] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
ninja: no work to do.
Time to load fused_adam op: 0.04032158851623535 seconds
[2024-04-11 22:32:21,457] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2024-04-11 22:32:21,457] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-04-11 22:32:21,465] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-04-11 22:32:21,465] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2024-04-11 22:32:21,465] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-04-11 22:32:21,465] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-04-11 22:32:21,531] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
[2024-04-11 22:32:21,531] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 1.07 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-11 22:32:21,532] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.71 GB, percent = 11.6%
[2024-04-11 22:32:21,532] [INFO] [stage3.py:130:__init__] Reduce bucket size 589824
[2024-04-11 22:32:21,532] [INFO] [stage3.py:131:__init__] Prefetch bucket size 530841
[2024-04-11 22:32:21,596] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-04-11 22:32:21,597] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.74 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-11 22:32:21,597] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.71 GB, percent = 11.6%
Parameter Offload: Total persistent parameters: 49920 in 65 params
[2024-04-11 22:32:21,678] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-04-11 22:32:21,678] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.82 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-11 22:32:21,678] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.71 GB, percent = 11.6%
[2024-04-11 22:32:21,745] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
[2024-04-11 22:32:21,745] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.74 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-11 22:32:21,745] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.71 GB, percent = 11.6%
[2024-04-11 22:32:22,419] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 1
[2024-04-11 22:32:22,419] [INFO] [utils.py:801:see_memory_usage] MA 0.72 GB         Max_MA 0.74 GB         CA 0.73 GB         Max_CA 1 GB 
[2024-04-11 22:32:22,419] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.72 GB, percent = 11.6%
[2024-04-11 22:32:22,486] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
[2024-04-11 22:32:22,486] [INFO] [utils.py:801:see_memory_usage] MA 0.72 GB         Max_MA 0.72 GB         CA 0.73 GB         Max_CA 1 GB 
[2024-04-11 22:32:22,486] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.72 GB, percent = 11.6%
[2024-04-11 22:32:22,572] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
[2024-04-11 22:32:22,572] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 2.89 GB         CA 2.9 GB         Max_CA 3 GB 
[2024-04-11 22:32:22,572] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.72 GB, percent = 11.6%
[2024-04-11 22:32:22,706] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-04-11 22:32:22,707] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 2.17 GB         CA 2.9 GB         Max_CA 3 GB 
[2024-04-11 22:32:22,707] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.72 GB, percent = 11.6%
[2024-04-11 22:32:22,777] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-04-11 22:32:22,777] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 3.61 GB         CA 4.34 GB         Max_CA 4 GB 
[2024-04-11 22:32:22,777] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.72 GB, percent = 11.6%
[2024-04-11 22:32:22,778] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-04-11 22:32:22,888] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-04-11 22:32:22,889] [INFO] [utils.py:801:see_memory_usage] MA 2.89 GB         Max_MA 3.05 GB         CA 4.34 GB         Max_CA 4 GB 
[2024-04-11 22:32:22,889] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.72 GB, percent = 11.6%
[2024-04-11 22:32:22,889] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2024-04-11 22:32:22,889] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-04-11 22:32:22,889] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f4e9ee8fb20>
[2024-04-11 22:32:22,889] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2024-04-11 22:32:22,890] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-04-11 22:32:22,890] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-04-11 22:32:22,890] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-04-11 22:32:22,890] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-04-11 22:32:22,890] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-04-11 22:32:22,890] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-04-11 22:32:22,890] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-04-11 22:32:22,890] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-04-11 22:32:22,890] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-04-11 22:32:22,890] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-04-11 22:32:22,890] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-04-11 22:32:22,890] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4fa8073730>
[2024-04-11 22:32:22,890] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-04-11 22:32:22,890] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-04-11 22:32:22,890] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-04-11 22:32:22,891] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 2
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupDecayLR
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   scheduler_params ............. {'last_batch_iteration': -1, 'total_num_steps': 342570, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 5000}
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  16
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-04-11 22:32:22,892] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-04-11 22:32:22,893] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=589824 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=530841 param_persistence_threshold=7680 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-04-11 22:32:22,893] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-04-11 22:32:22,893] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-04-11 22:32:22,893] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-04-11 22:32:22,893] [INFO] [config.py:986:print_user_config]   json = {
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "fp16_opt_level": "O2"
    }, 
    "bf16": {
        "enabled": true
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0001, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0.0
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "last_batch_iteration": -1, 
            "total_num_steps": 3.425700e+05, 
            "warmup_min_lr": 0, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 5.000000e+03
        }
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 5.898240e+05, 
        "stage3_prefetch_bucket_size": 5.308416e+05, 
        "stage3_param_persistence_threshold": 7.680000e+03, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 2, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 16, 
    "wall_clock_breakdown": false
}
{'loss': 10.6353, 'grad_norm': 4.654900452374664, 'learning_rate': 1.889634150903278e-05, 'epoch': 0.0}
{'loss': 9.8939, 'grad_norm': 1.9957163148768968, 'learning_rate': 2.7034552830322406e-05, 'epoch': 0.0}
{'loss': 9.6058, 'grad_norm': 1.881680516525477, 'learning_rate': 3.1795101276221204e-05, 'epoch': 0.0}
{'loss': 9.4216, 'grad_norm': 1.7722045825845647, 'learning_rate': 3.5172764151612024e-05, 'epoch': 0.0}
{'loss': 9.2494, 'grad_norm': 1.6553826040217043, 'learning_rate': 3.779268301806556e-05, 'epoch': 0.0}
{'loss': 9.0517, 'grad_norm': 1.6582211797382365, 'learning_rate': 3.993331259751083e-05, 'epoch': 0.0}
{'loss': 8.841, 'grad_norm': 1.5523027288183664, 'learning_rate': 4.1743189118600124e-05, 'epoch': 0.0}
{'loss': 8.6135, 'grad_norm': 1.4849198251855438, 'learning_rate': 4.331097547290165e-05, 'epoch': 0.0}
{'loss': 8.3813, 'grad_norm': 1.1944549224923027, 'learning_rate': 4.4693861043409635e-05, 'epoch': 0.0}
{'loss': 8.1421, 'grad_norm': 1.4817094998838445, 'learning_rate': 4.5930894339355186e-05, 'epoch': 0.0}
{'loss': 7.8799, 'grad_norm': 2.05074427363375, 'learning_rate': 4.7049927073054124e-05, 'epoch': 0.0}
{'loss': 7.6599, 'grad_norm': 1.9632702804314717, 'learning_rate': 4.8071523918800455e-05, 'epoch': 0.0}
{'loss': 7.4967, 'grad_norm': 0.824572630114269, 'learning_rate': 4.9011301916958396e-05, 'epoch': 0.0}
{'loss': 7.3077, 'grad_norm': 1.0037891509768415, 'learning_rate': 4.9881400439889756e-05, 'epoch': 0.0}
{'loss': 7.2117, 'grad_norm': 0.8742487589448096, 'learning_rate': 5.0691442785253985e-05, 'epoch': 0.0}
{'loss': 7.0622, 'grad_norm': 0.6064481165616061, 'learning_rate': 5.1449186794191275e-05, 'epoch': 0.0}
{'loss': 6.9659, 'grad_norm': 0.7652762425141301, 'learning_rate': 5.2160977879046945e-05, 'epoch': 0.0}
{'loss': 6.8487, 'grad_norm': 0.7967551228469477, 'learning_rate': 5.283207236469926e-05, 'epoch': 0.0}
{'loss': 6.7466, 'grad_norm': 1.1523294748970085, 'learning_rate': 5.3466873290957045e-05, 'epoch': 0.0}
{'loss': 6.6434, 'grad_norm': 0.7578670484477498, 'learning_rate': 5.406910566064481e-05, 'epoch': 0.0}
[2024-04-11 22:37:00,851] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!
[2024-04-11 22:37:00,871] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 22:37:00,871] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 22:37:00,899] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 22:37:00,900] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 22:37:04,538] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 22:37:04,782] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 22:37:04,782] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
{'loss': 6.5701, 'grad_norm': 0.6219054267188246, 'learning_rate': 5.4641948885788554e-05, 'epoch': 0.0}
{'loss': 6.4507, 'grad_norm': 1.1486974427275103, 'learning_rate': 5.518813839434375e-05, 'epoch': 0.0}
{'loss': 6.3607, 'grad_norm': 1.761122896914329, 'learning_rate': 5.5710044632370994e-05, 'epoch': 0.0}
{'loss': 6.3147, 'grad_norm': 1.2352991744279143, 'learning_rate': 5.620973524009008e-05, 'epoch': 0.0}
{'loss': 6.2484, 'grad_norm': 1.4348800237222163, 'learning_rate': 5.668902452709834e-05, 'epoch': 0.0}
{'loss': 6.2434, 'grad_norm': 0.9172291771281249, 'learning_rate': 5.714951323824802e-05, 'epoch': 0.0}
{'loss': 6.1261, 'grad_norm': 1.2468894947404476, 'learning_rate': 5.7592620810598065e-05, 'epoch': 0.0}
{'loss': 6.0298, 'grad_norm': 0.9364148148463394, 'learning_rate': 5.8019611761179374e-05, 'epoch': 0.0}
{'loss': 6.008, 'grad_norm': 1.3142023651940704, 'learning_rate': 5.843161744218982e-05, 'epoch': 0.0}
{'loss': 5.9807, 'grad_norm': 0.735231775477695, 'learning_rate': 5.882965410654361e-05, 'epoch': 0.0}
{'loss': 5.9348, 'grad_norm': 0.6890808583997194, 'learning_rate': 5.9214638010114533e-05, 'epoch': 0.0}
{'loss': 5.8733, 'grad_norm': 0.8185004444236088, 'learning_rate': 5.95873981154809e-05, 'epoch': 0.0}
{'loss': 5.8597, 'grad_norm': 0.9079567777614289, 'learning_rate': 5.994868684024254e-05, 'epoch': 0.0}
{'loss': 5.8665, 'grad_norm': 0.7740132888527324, 'learning_rate': 6.029918920033657e-05, 'epoch': 0.0}
{'loss': 5.7885, 'grad_norm': 0.6078231614288963, 'learning_rate': 6.063953062763291e-05, 'epoch': 0.0}
{'loss': 5.7717, 'grad_norm': 0.554194439767338, 'learning_rate': 6.0970283685988885e-05, 'epoch': 0.0}
{'loss': 5.7279, 'grad_norm': 0.6908809716005342, 'learning_rate': 6.129197386692462e-05, 'epoch': 0.0}
{'loss': 5.6806, 'grad_norm': 0.884487032742257, 'learning_rate': 6.160508461224668e-05, 'epoch': 0.0}
{'loss': 5.6618, 'grad_norm': 0.8705841599363036, 'learning_rate': 6.191006168414683e-05, 'epoch': 0.0}
{'loss': 5.6733, 'grad_norm': 0.9244465044853009, 'learning_rate': 6.220731698193443e-05, 'epoch': 0.0}
[2024-04-11 22:41:42,908] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2024-04-11 22:41:42,914] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 22:41:42,914] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 22:41:42,921] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 22:41:42,921] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 22:41:46,560] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 22:41:46,560] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 22:41:46,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
{'loss': 5.6455, 'grad_norm': 0.7891928712163424, 'learning_rate': 6.249723188741359e-05, 'epoch': 0.0}
{'loss': 5.6083, 'grad_norm': 0.8203195669265111, 'learning_rate': 6.278016020707817e-05, 'epoch': 0.0}
{'loss': 5.5618, 'grad_norm': 0.6468392677729436, 'learning_rate': 6.305643076806426e-05, 'epoch': 0.0}
{'loss': 5.5185, 'grad_norm': 0.7903913037174176, 'learning_rate': 6.332634971563337e-05, 'epoch': 0.0}
{'loss': 5.5202, 'grad_norm': 0.980544761550691, 'learning_rate': 6.359020255244241e-05, 'epoch': 0.0}
{'loss': 5.4578, 'grad_norm': 0.7525098753459342, 'learning_rate': 6.384825595366063e-05, 'epoch': 0.0}
{'loss': 5.4255, 'grad_norm': 0.582942502943024, 'learning_rate': 6.410075938686486e-05, 'epoch': 0.0}
{'loss': 5.422, 'grad_norm': 0.9466425236848321, 'learning_rate': 6.43479465613797e-05, 'epoch': 0.0}
{'loss': 5.4387, 'grad_norm': 0.7586392296404745, 'learning_rate': 6.459003672816747e-05, 'epoch': 0.0}
{'loss': 5.4297, 'grad_norm': 0.7427559579641007, 'learning_rate': 6.482723584838796e-05, 'epoch': 0.0}
{'loss': 5.4157, 'grad_norm': 0.6404780431524475, 'learning_rate': 6.505973764623537e-05, 'epoch': 0.0}
{'loss': 5.3532, 'grad_norm': 0.6761896835532666, 'learning_rate': 6.528772455953764e-05, 'epoch': 0.0}
{'loss': 5.3104, 'grad_norm': 0.8165119876202119, 'learning_rate': 6.551136859980541e-05, 'epoch': 0.0}
{'loss': 5.3286, 'grad_norm': 0.9714342533947685, 'learning_rate': 6.573083213188768e-05, 'epoch': 0.0}
{'loss': 5.3241, 'grad_norm': 1.1674055135148493, 'learning_rate': 6.59462685820869e-05, 'epoch': 0.0}
{'loss': 5.304, 'grad_norm': 0.5131669243636423, 'learning_rate': 6.6157823082469e-05, 'epoch': 0.0}
{'loss': 5.3231, 'grad_norm': 0.7293547448615136, 'learning_rate': 6.636563305814549e-05, 'epoch': 0.0}
{'loss': 5.2203, 'grad_norm': 0.7687240952235296, 'learning_rate': 6.656982876347945e-05, 'epoch': 0.0}
{'loss': 5.2143, 'grad_norm': 0.8564054231545397, 'learning_rate': 6.677053377245504e-05, 'epoch': 0.0}
{'loss': 5.2734, 'grad_norm': 1.024885279954278, 'learning_rate': 6.696786542783324e-05, 'epoch': 0.0}
[2024-04-11 22:46:25,183] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step300 is about to be saved!
[2024-04-11 22:46:25,189] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 22:46:25,189] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 22:46:25,196] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 22:46:25,197] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 22:46:28,946] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 22:46:28,946] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 22:46:28,947] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step300 is ready now!
{'loss': 5.2763, 'grad_norm': 1.0605714988783108, 'learning_rate': 6.716193525318215e-05, 'epoch': 0.0}
{'loss': 5.1833, 'grad_norm': 0.7368881721277551, 'learning_rate': 6.735284933140416e-05, 'epoch': 0.0}
{'loss': 5.2016, 'grad_norm': 0.685967919261425, 'learning_rate': 6.754070865297698e-05, 'epoch': 0.0}
{'loss': 5.1653, 'grad_norm': 0.6018350685867723, 'learning_rate': 6.772560943677052e-05, 'epoch': 0.0}
{'loss': 5.1665, 'grad_norm': 1.010122305326539, 'learning_rate': 6.790764342599117e-05, 'epoch': 0.0}
{'loss': 5.1373, 'grad_norm': 0.89847456242636, 'learning_rate': 6.808689816153217e-05, 'epoch': 0.0}
{'loss': 5.0906, 'grad_norm': 0.8416473543304382, 'learning_rate': 6.826345723476884e-05, 'epoch': 0.0}
{'loss': 5.1252, 'grad_norm': 0.7568104886377389, 'learning_rate': 6.84374005216262e-05, 'epoch': 0.0}
{'loss': 5.0676, 'grad_norm': 0.6614712831362253, 'learning_rate': 6.860880439955943e-05, 'epoch': 0.0}
{'loss': 5.1103, 'grad_norm': 0.9242952151050198, 'learning_rate': 6.877774194892253e-05, 'epoch': 0.0}
{'loss': 5.0516, 'grad_norm': 1.084696095132853, 'learning_rate': 6.894428314005404e-05, 'epoch': 0.0}
{'loss': 5.0079, 'grad_norm': 0.703335902441084, 'learning_rate': 6.910849500727851e-05, 'epoch': 0.0}
{'loss': 5.0637, 'grad_norm': 0.6767951236412763, 'learning_rate': 6.927044181090669e-05, 'epoch': 0.0}
{'loss': 5.0348, 'grad_norm': 0.7675545015966233, 'learning_rate': 6.943018518821426e-05, 'epoch': 0.0}
{'loss': 5.0224, 'grad_norm': 1.007366561190731, 'learning_rate': 6.958778429428677e-05, 'epoch': 0.0}
{'loss': 4.9373, 'grad_norm': 0.7443062492593845, 'learning_rate': 6.97432959335363e-05, 'epoch': 0.0}
{'loss': 4.9917, 'grad_norm': 0.9632543238123149, 'learning_rate': 6.989677468262147e-05, 'epoch': 0.0}
{'loss': 4.9531, 'grad_norm': 0.6857063713207685, 'learning_rate': 7.004827300543644e-05, 'epoch': 0.0}
{'loss': 4.9663, 'grad_norm': 0.8757097016209758, 'learning_rate': 7.019784136077525e-05, 'epoch': 0.0}
{'loss': 4.9562, 'grad_norm': 0.7887887676968985, 'learning_rate': 7.034552830322405e-05, 'epoch': 0.0}
[2024-04-11 22:51:08,065] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2024-04-11 22:51:08,070] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 22:51:08,070] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 22:51:08,076] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 22:51:08,077] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 22:51:11,388] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 22:51:11,389] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 22:51:11,389] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
{'loss': 4.8999, 'grad_norm': 0.981918521347854, 'learning_rate': 7.049138057778648e-05, 'epoch': 0.0}
{'loss': 4.9557, 'grad_norm': 0.9290112215138409, 'learning_rate': 7.063544320870321e-05, 'epoch': 0.0}
{'loss': 4.888, 'grad_norm': 0.7072507924544031, 'learning_rate': 7.077775958288809e-05, 'epoch': 0.0}
{'loss': 4.904, 'grad_norm': 0.9055429529361584, 'learning_rate': 7.09183715283678e-05, 'epoch': 0.0}
{'loss': 4.8741, 'grad_norm': 0.8052186606605652, 'learning_rate': 7.105731938807974e-05, 'epoch': 0.0}
{'loss': 4.8608, 'grad_norm': 0.8562939560259502, 'learning_rate': 7.119464208935388e-05, 'epoch': 0.0}
{'loss': 4.8668, 'grad_norm': 0.7842280582088141, 'learning_rate': 7.133037720937826e-05, 'epoch': 0.0}
{'loss': 4.8652, 'grad_norm': 0.8014209745269052, 'learning_rate': 7.146456103692298e-05, 'epoch': 0.0}
{'loss': 4.8273, 'grad_norm': 0.7801178942574373, 'learning_rate': 7.159722863057722e-05, 'epoch': 0.0}
{'loss': 4.8034, 'grad_norm': 0.9650409304494695, 'learning_rate': 7.172841387373204e-05, 'epoch': 0.0}
{'loss': 4.8056, 'grad_norm': 0.8022310827095812, 'learning_rate': 7.185814952652574e-05, 'epoch': 0.0}
{'loss': 4.7749, 'grad_norm': 0.7428481914511341, 'learning_rate': 7.198646727495026e-05, 'epoch': 0.0}
{'loss': 4.79, 'grad_norm': 0.7582647482255129, 'learning_rate': 7.211339777730295e-05, 'epoch': 0.0}
{'loss': 4.7848, 'grad_norm': 0.878411906910265, 'learning_rate': 7.223897070815449e-05, 'epoch': 0.0}
{'loss': 4.754, 'grad_norm': 0.7753581491686691, 'learning_rate': 7.236321479998983e-05, 'epoch': 0.0}
{'loss': 4.7298, 'grad_norm': 0.7119067597879751, 'learning_rate': 7.248615788266932e-05, 'epoch': 0.0}
{'loss': 4.7686, 'grad_norm': 0.7134234661672678, 'learning_rate': 7.260782692084485e-05, 'epoch': 0.0}
{'loss': 4.7457, 'grad_norm': 1.0077157108341812, 'learning_rate': 7.272824804945709e-05, 'epoch': 0.0}
{'loss': 4.7291, 'grad_norm': 0.9130183381445272, 'learning_rate': 7.284744660743098e-05, 'epoch': 0.0}
{'loss': 4.7963, 'grad_norm': 1.094327415010676, 'learning_rate': 7.296544716967758e-05, 'epoch': 0.0}
[2024-04-11 22:55:50,427] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is about to be saved!
[2024-04-11 22:55:50,433] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 22:55:50,433] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 22:55:50,439] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 22:55:50,440] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 22:55:53,654] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 22:55:53,654] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 22:55:53,655] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
{'loss': 4.7346, 'grad_norm': 0.8717134560166404, 'learning_rate': 7.308227357750401e-05, 'epoch': 0.0}
{'loss': 4.6863, 'grad_norm': 0.7269721195302743, 'learning_rate': 7.319794896752499e-05, 'epoch': 0.0}
{'loss': 4.7331, 'grad_norm': 0.6906234103042544, 'learning_rate': 7.331249579916429e-05, 'epoch': 0.0}
{'loss': 4.6392, 'grad_norm': 0.7846892536835057, 'learning_rate': 7.342593588082727e-05, 'epoch': 0.0}
{'loss': 4.6771, 'grad_norm': 0.715664280481091, 'learning_rate': 7.353829039482133e-05, 'epoch': 0.0}
{'loss': 4.6908, 'grad_norm': 0.7611691612455502, 'learning_rate': 7.364957992109503e-05, 'epoch': 0.0}
{'loss': 4.655, 'grad_norm': 0.766584802926099, 'learning_rate': 7.375982445986283e-05, 'epoch': 0.0}
{'loss': 4.6745, 'grad_norm': 0.8384303358535258, 'learning_rate': 7.386904345317732e-05, 'epoch': 0.0}
{'loss': 4.6767, 'grad_norm': 0.665224046524264, 'learning_rate': 7.397725580550732e-05, 'epoch': 0.0}
{'loss': 4.6512, 'grad_norm': 0.8469032834980819, 'learning_rate': 7.408447990337652e-05, 'epoch': 0.0}
{'loss': 4.627, 'grad_norm': 0.8074875671002618, 'learning_rate': 7.419073363411306e-05, 'epoch': 0.0}
{'loss': 4.6074, 'grad_norm': 0.8246890886190812, 'learning_rate': 7.429603440375862e-05, 'epoch': 0.0}
{'loss': 4.627, 'grad_norm': 0.8192242490574757, 'learning_rate': 7.440039915418139e-05, 'epoch': 0.0}
{'loss': 4.5838, 'grad_norm': 1.0910241678484767, 'learning_rate': 7.450384437943511e-05, 'epoch': 0.0}
{'loss': 4.5767, 'grad_norm': 0.9824520648808976, 'learning_rate': 7.460638614140379e-05, 'epoch': 0.01}
{'loss': 4.6203, 'grad_norm': 0.8832312168749226, 'learning_rate': 7.470804008476907e-05, 'epoch': 0.01}
{'loss': 4.5698, 'grad_norm': 0.6982723815001274, 'learning_rate': 7.480882145133525e-05, 'epoch': 0.01}
{'loss': 4.5589, 'grad_norm': 0.6867498030737895, 'learning_rate': 7.490874509374465e-05, 'epoch': 0.01}
{'loss': 4.5779, 'grad_norm': 0.7688813415681596, 'learning_rate': 7.50078254886143e-05, 'epoch': 0.01}
{'loss': 4.5984, 'grad_norm': 0.8488623363630415, 'learning_rate': 7.510607674912285e-05, 'epoch': 0.01}
[2024-04-11 23:00:32,725] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!
[2024-04-11 23:00:32,731] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 23:00:32,731] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 23:00:32,737] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 23:00:32,738] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 23:00:35,870] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 23:00:35,870] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 23:00:35,870] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
{'loss': 4.576, 'grad_norm': 1.0320894459108472, 'learning_rate': 7.520351263707547e-05, 'epoch': 0.01}
{'loss': 4.5498, 'grad_norm': 0.7778995955556658, 'learning_rate': 7.530014657447177e-05, 'epoch': 0.01}
{'loss': 4.5848, 'grad_norm': 0.8273854973112077, 'learning_rate': 7.539599165460201e-05, 'epoch': 0.01}
{'loss': 4.4762, 'grad_norm': 0.7117339085181172, 'learning_rate': 7.549106065269378e-05, 'epoch': 0.01}
{'loss': 4.5175, 'grad_norm': 0.6720225084695255, 'learning_rate': 7.558536603613112e-05, 'epoch': 0.01}
{'loss': 4.5614, 'grad_norm': 0.9452582840331765, 'learning_rate': 7.567891997426661e-05, 'epoch': 0.01}
{'loss': 4.4574, 'grad_norm': 0.6883196799481713, 'learning_rate': 7.577173434784545e-05, 'epoch': 0.01}
{'loss': 4.4944, 'grad_norm': 0.751369945215647, 'learning_rate': 7.586382075806015e-05, 'epoch': 0.01}
{'loss': 4.4685, 'grad_norm': 0.8714979429075368, 'learning_rate': 7.595519053525268e-05, 'epoch': 0.01}
{'loss': 4.4795, 'grad_norm': 0.7811555110144155, 'learning_rate': 7.604585474728082e-05, 'epoch': 0.01}
{'loss': 4.4751, 'grad_norm': 0.7037241150672601, 'learning_rate': 7.61358242075637e-05, 'epoch': 0.01}
{'loss': 4.4757, 'grad_norm': 0.6763954642962857, 'learning_rate': 7.62251094828218e-05, 'epoch': 0.01}
{'loss': 4.4687, 'grad_norm': 0.7998430868133897, 'learning_rate': 7.631372090052439e-05, 'epoch': 0.01}
{'loss': 4.4352, 'grad_norm': 0.7681991736582373, 'learning_rate': 7.640166855605846e-05, 'epoch': 0.01}
{'loss': 4.448, 'grad_norm': 0.9341435130942763, 'learning_rate': 7.648896231963085e-05, 'epoch': 0.01}
{'loss': 4.4487, 'grad_norm': 0.7034272831002488, 'learning_rate': 7.65756118429158e-05, 'epoch': 0.01}
{'loss': 4.4446, 'grad_norm': 0.7125983354881399, 'learning_rate': 7.66616265654591e-05, 'epoch': 0.01}
{'loss': 4.4587, 'grad_norm': 0.7528627040135736, 'learning_rate': 7.674701572084905e-05, 'epoch': 0.01}
{'loss': 4.4213, 'grad_norm': 0.7137812320279241, 'learning_rate': 7.683178834266492e-05, 'epoch': 0.01}
{'loss': 4.4677, 'grad_norm': 0.7819349148547253, 'learning_rate': 7.691595327021215e-05, 'epoch': 0.01}
[2024-04-11 23:05:14,514] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step700 is about to be saved!
[2024-04-11 23:05:14,519] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-700/global_step700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 23:05:14,519] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-700/global_step700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 23:05:14,525] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-700/global_step700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 23:05:14,526] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-700/global_step700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 23:05:17,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-700/global_step700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 23:05:17,665] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-700/global_step700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 23:05:17,665] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step700 is ready now!
{'loss': 4.4376, 'grad_norm': 0.7918387936923201, 'learning_rate': 7.69995191540533e-05, 'epoch': 0.01}
{'loss': 4.4002, 'grad_norm': 0.7105466269778353, 'learning_rate': 7.708249446134367e-05, 'epoch': 0.01}
{'loss': 4.4347, 'grad_norm': 0.8395472448850676, 'learning_rate': 7.716488748097975e-05, 'epoch': 0.01}
{'loss': 4.412, 'grad_norm': 0.7496365054089013, 'learning_rate': 7.724670632856813e-05, 'epoch': 0.01}
{'loss': 4.4095, 'grad_norm': 0.6868152088808785, 'learning_rate': 7.73279589512226e-05, 'epoch': 0.01}
{'loss': 4.3996, 'grad_norm': 0.8921866654210213, 'learning_rate': 7.740865313219632e-05, 'epoch': 0.01}
{'loss': 4.362, 'grad_norm': 0.9666668845989868, 'learning_rate': 7.74887964953559e-05, 'epoch': 0.01}
{'loss': 4.4122, 'grad_norm': 0.9068735753143035, 'learning_rate': 7.756839650950389e-05, 'epoch': 0.01}
{'loss': 4.4067, 'grad_norm': 0.6928302190309685, 'learning_rate': 7.764746049255561e-05, 'epoch': 0.01}
{'loss': 4.3514, 'grad_norm': 0.6934179219403447, 'learning_rate': 7.772599561557638e-05, 'epoch': 0.01}
{'loss': 4.3477, 'grad_norm': 0.6864882561730205, 'learning_rate': 7.780400890668461e-05, 'epoch': 0.01}
{'loss': 4.3292, 'grad_norm': 0.7188916920874028, 'learning_rate': 7.788150725482592e-05, 'epoch': 0.01}
{'loss': 4.3645, 'grad_norm': 0.6653292092922193, 'learning_rate': 7.79584974134238e-05, 'epoch': 0.01}
{'loss': 4.3335, 'grad_norm': 0.7708317764563909, 'learning_rate': 7.803498600391108e-05, 'epoch': 0.01}
{'loss': 4.3553, 'grad_norm': 0.8165609356584844, 'learning_rate': 7.811097951914732e-05, 'epoch': 0.01}
{'loss': 4.3408, 'grad_norm': 0.7423496000610097, 'learning_rate': 7.818648432672608e-05, 'epoch': 0.01}
{'loss': 4.3482, 'grad_norm': 0.8222826384664949, 'learning_rate': 7.82615066721768e-05, 'epoch': 0.01}
{'loss': 4.2998, 'grad_norm': 0.6696422925837563, 'learning_rate': 7.833605268206489e-05, 'epoch': 0.01}
{'loss': 4.3187, 'grad_norm': 0.75885727668853, 'learning_rate': 7.841012836699384e-05, 'epoch': 0.01}
{'loss': 4.2999, 'grad_norm': 0.77195871306911, 'learning_rate': 7.848373962451368e-05, 'epoch': 0.01}
[2024-04-11 23:09:56,184] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step800 is about to be saved!
[2024-04-11 23:09:56,190] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-800/global_step800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 23:09:56,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-800/global_step800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 23:09:56,196] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-800/global_step800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 23:09:56,197] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 23:09:59,249] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 23:09:59,249] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 23:09:59,250] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!
{'loss': 4.3165, 'grad_norm': 0.7574652064651463, 'learning_rate': 7.855689224193834e-05, 'epoch': 0.01}
{'loss': 4.321, 'grad_norm': 0.8188550185723682, 'learning_rate': 7.862959189907611e-05, 'epoch': 0.01}
{'loss': 4.3112, 'grad_norm': 0.8311185288375448, 'learning_rate': 7.870184417087593e-05, 'epoch': 0.01}
{'loss': 4.3098, 'grad_norm': 0.7965415509227719, 'learning_rate': 7.877365452999284e-05, 'epoch': 0.01}
{'loss': 4.2842, 'grad_norm': 0.7683610975057279, 'learning_rate': 7.884502834927533e-05, 'epoch': 0.01}
{'loss': 4.3243, 'grad_norm': 0.8652817442151728, 'learning_rate': 7.89159709041777e-05, 'epoch': 0.01}
{'loss': 4.3589, 'grad_norm': 0.8919748533265348, 'learning_rate': 7.89864873750999e-05, 'epoch': 0.01}
{'loss': 4.3399, 'grad_norm': 0.7800115936465905, 'learning_rate': 7.905658284965742e-05, 'epoch': 0.01}
{'loss': 4.2842, 'grad_norm': 0.8242607226155707, 'learning_rate': 7.912626232488402e-05, 'epoch': 0.01}
{'loss': 4.2613, 'grad_norm': 0.6680246164377616, 'learning_rate': 7.919553070936936e-05, 'epoch': 0.01}
{'loss': 4.2592, 'grad_norm': 0.6580920858214795, 'learning_rate': 7.92643928253339e-05, 'epoch': 0.01}
{'loss': 4.287, 'grad_norm': 0.7847183276975042, 'learning_rate': 7.933285341064351e-05, 'epoch': 0.01}
{'loss': 4.2889, 'grad_norm': 0.7593475568701912, 'learning_rate': 7.940091712076538e-05, 'epoch': 0.01}
{'loss': 4.2361, 'grad_norm': 0.8443210075860064, 'learning_rate': 7.946858853066788e-05, 'epoch': 0.01}
{'loss': 4.2998, 'grad_norm': 0.7461432283495488, 'learning_rate': 7.953587213666568e-05, 'epoch': 0.01}
{'loss': 4.1968, 'grad_norm': 0.778128958894453, 'learning_rate': 7.960277235821263e-05, 'epoch': 0.01}
{'loss': 4.2194, 'grad_norm': 0.7055128768175752, 'learning_rate': 7.966929353964345e-05, 'epoch': 0.01}
{'loss': 4.2437, 'grad_norm': 0.7660041737389781, 'learning_rate': 7.973543995186684e-05, 'epoch': 0.01}
{'loss': 4.2671, 'grad_norm': 0.6651209315321337, 'learning_rate': 7.980121579401066e-05, 'epoch': 0.01}
{'loss': 4.272, 'grad_norm': 0.6778867009454198, 'learning_rate': 7.986662519502166e-05, 'epoch': 0.01}
[2024-04-11 23:14:38,130] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step900 is about to be saved!
[2024-04-11 23:14:38,136] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-900/global_step900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 23:14:38,136] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-900/global_step900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 23:14:38,143] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-900/global_step900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 23:14:38,144] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-900/global_step900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 23:14:41,304] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-900/global_step900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 23:14:41,304] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-900/global_step900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 23:14:41,305] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step900 is ready now!
{'loss': 4.2334, 'grad_norm': 0.7833853686035319, 'learning_rate': 7.993167221522075e-05, 'epoch': 0.01}
{'loss': 4.21, 'grad_norm': 0.6684710447283593, 'learning_rate': 7.999636084781537e-05, 'epoch': 0.01}
{'loss': 4.2073, 'grad_norm': 0.8754060153212412, 'learning_rate': 8.006069502037056e-05, 'epoch': 0.01}
{'loss': 4.2134, 'grad_norm': 0.8475062754037761, 'learning_rate': 8.012467859623988e-05, 'epoch': 0.01}
{'loss': 4.212, 'grad_norm': 0.6733188381402064, 'learning_rate': 8.018831537595742e-05, 'epoch': 0.01}
{'loss': 4.189, 'grad_norm': 0.6305973258427943, 'learning_rate': 8.025160909859258e-05, 'epoch': 0.01}
{'loss': 4.2076, 'grad_norm': 0.711985207800914, 'learning_rate': 8.031456344306829e-05, 'epoch': 0.01}
{'loss': 4.2182, 'grad_norm': 0.7687365686764366, 'learning_rate': 8.037718202944411e-05, 'epoch': 0.01}
{'loss': 4.1952, 'grad_norm': 0.7536238243167429, 'learning_rate': 8.043946842016542e-05, 'epoch': 0.01}
{'loss': 4.1928, 'grad_norm': 0.6918946411363758, 'learning_rate': 8.050142612127945e-05, 'epoch': 0.01}
{'loss': 4.1883, 'grad_norm': 0.6786101762791698, 'learning_rate': 8.056305858361968e-05, 'epoch': 0.01}
{'loss': 4.1466, 'grad_norm': 0.7013550849517842, 'learning_rate': 8.062436920395896e-05, 'epoch': 0.01}
{'loss': 4.1442, 'grad_norm': 0.7104716043121315, 'learning_rate': 8.068536132613294e-05, 'epoch': 0.01}
{'loss': 4.2108, 'grad_norm': 0.6797549338532983, 'learning_rate': 8.074603824213446e-05, 'epoch': 0.01}
{'loss': 4.1824, 'grad_norm': 0.6712599338042534, 'learning_rate': 8.080640319317962e-05, 'epoch': 0.01}
{'loss': 4.1758, 'grad_norm': 0.8019682430627421, 'learning_rate': 8.086645937074672e-05, 'epoch': 0.01}
{'loss': 4.162, 'grad_norm': 0.7005431842684853, 'learning_rate': 8.092620991758884e-05, 'epoch': 0.01}
{'loss': 4.1626, 'grad_norm': 0.7757769604903676, 'learning_rate': 8.09856579287206e-05, 'epoch': 0.01}
{'loss': 4.1057, 'grad_norm': 0.7434656149689125, 'learning_rate': 8.104480645238018e-05, 'epoch': 0.01}
{'loss': 4.1722, 'grad_norm': 0.6895285228940972, 'learning_rate': 8.110365849096721e-05, 'epoch': 0.01}
[2024-04-11 23:19:20,191] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
[2024-04-11 23:19:20,196] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 23:19:20,196] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 23:19:20,203] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 23:19:20,203] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 23:19:23,392] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 23:19:23,392] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 23:19:23,393] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
{'loss': 4.1586, 'grad_norm': 0.7195208431023151, 'learning_rate': 8.116221700195726e-05, 'epoch': 0.01}
{'loss': 4.1181, 'grad_norm': 0.6958514009733995, 'learning_rate': 8.122048489879363e-05, 'epoch': 0.01}
{'loss': 4.1347, 'grad_norm': 0.7473807523669795, 'learning_rate': 8.127846505175717e-05, 'epoch': 0.01}
{'loss': 4.1178, 'grad_norm': 0.725195454475398, 'learning_rate': 8.133616028881462e-05, 'epoch': 0.01}
{'loss': 4.1172, 'grad_norm': 0.8087350845654808, 'learning_rate': 8.139357339644637e-05, 'epoch': 0.01}
{'loss': 4.1764, 'grad_norm': 0.6439664751483083, 'learning_rate': 8.145070712045392e-05, 'epoch': 0.01}
{'loss': 4.1244, 'grad_norm': 0.7030633199912748, 'learning_rate': 8.150756416674785e-05, 'epoch': 0.01}
{'loss': 4.1159, 'grad_norm': 0.7549722650537842, 'learning_rate': 8.15641472021169e-05, 'epoch': 0.01}
{'loss': 4.1005, 'grad_norm': 0.7076874909768929, 'learning_rate': 8.162045885497838e-05, 'epoch': 0.01}
{'loss': 4.1315, 'grad_norm': 0.7384748460906864, 'learning_rate': 8.167650171611095e-05, 'epoch': 0.01}
{'loss': 4.0852, 'grad_norm': 0.6946875161213397, 'learning_rate': 8.173227833936972e-05, 'epoch': 0.01}
{'loss': 4.1122, 'grad_norm': 0.7451971168397145, 'learning_rate': 8.178779124238466e-05, 'epoch': 0.01}
{'loss': 4.0484, 'grad_norm': 0.7394806808467581, 'learning_rate': 8.184304290724247e-05, 'epoch': 0.01}
{'loss': 4.1128, 'grad_norm': 0.6670972413191111, 'learning_rate': 8.189803578115246e-05, 'epoch': 0.01}
{'loss': 4.0891, 'grad_norm': 0.6761848301855549, 'learning_rate': 8.195277227709704e-05, 'epoch': 0.01}
{'loss': 4.1051, 'grad_norm': 0.6838411573161457, 'learning_rate': 8.200725477446693e-05, 'epoch': 0.01}
{'loss': 4.0728, 'grad_norm': 0.7058185181158824, 'learning_rate': 8.206148561968188e-05, 'epoch': 0.01}
{'loss': 4.0877, 'grad_norm': 0.6919035052877439, 'learning_rate': 8.211546712679696e-05, 'epoch': 0.01}
{'loss': 4.0651, 'grad_norm': 0.7761506099369326, 'learning_rate': 8.216920157809512e-05, 'epoch': 0.01}
{'loss': 4.0584, 'grad_norm': 0.7114607462842633, 'learning_rate': 8.222269122466616e-05, 'epoch': 0.01}
[2024-04-11 23:24:02,111] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1100 is about to be saved!
[2024-04-11 23:24:02,118] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-1100/global_step1100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 23:24:02,118] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1100/global_step1100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 23:24:02,124] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1100/global_step1100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 23:24:02,125] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1100/global_step1100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 23:24:05,283] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1100/global_step1100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 23:24:05,283] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-1100/global_step1100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 23:24:05,283] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1100 is ready now!
{'loss': 4.0814, 'grad_norm': 0.650310672270823, 'learning_rate': 8.227593828697257e-05, 'epoch': 0.01}
{'loss': 4.047, 'grad_norm': 0.7033876612756617, 'learning_rate': 8.232894495540269e-05, 'epoch': 0.01}
{'loss': 4.0687, 'grad_norm': 0.6629076311044954, 'learning_rate': 8.23817133908113e-05, 'epoch': 0.01}
{'loss': 4.0947, 'grad_norm': 0.6963086312726353, 'learning_rate': 8.243424572504824e-05, 'epoch': 0.01}
{'loss': 4.0832, 'grad_norm': 0.7313692473558353, 'learning_rate': 8.24865440614752e-05, 'epoch': 0.01}
{'loss': 4.0306, 'grad_norm': 0.6466302143549784, 'learning_rate': 8.2538610475471e-05, 'epoch': 0.01}
{'loss': 4.0217, 'grad_norm': 0.7045150045994313, 'learning_rate': 8.259044701492588e-05, 'epoch': 0.01}
{'loss': 4.0458, 'grad_norm': 0.6896280200391662, 'learning_rate': 8.264205570072473e-05, 'epoch': 0.01}
{'loss': 4.0327, 'grad_norm': 0.693424084492809, 'learning_rate': 8.269343852721983e-05, 'epoch': 0.01}
{'loss': 4.0329, 'grad_norm': 0.7043123902722089, 'learning_rate': 8.27445974626934e-05, 'epoch': 0.01}
{'loss': 4.0452, 'grad_norm': 0.7496977765068572, 'learning_rate': 8.279553444980988e-05, 'epoch': 0.01}
{'loss': 4.0378, 'grad_norm': 0.7081408241976848, 'learning_rate': 8.284625140605869e-05, 'epoch': 0.01}
{'loss': 3.9695, 'grad_norm': 0.8916709379570008, 'learning_rate': 8.289675022418723e-05, 'epoch': 0.01}
{'loss': 4.0434, 'grad_norm': 0.6543025632505693, 'learning_rate': 8.294703277262488e-05, 'epoch': 0.01}
{'loss': 4.0214, 'grad_norm': 0.6982552268653827, 'learning_rate': 8.299710089589764e-05, 'epoch': 0.01}
{'loss': 4.0929, 'grad_norm': 0.6663353567747784, 'learning_rate': 8.304695641503428e-05, 'epoch': 0.01}
{'loss': 4.0056, 'grad_norm': 0.6291342680157235, 'learning_rate': 8.309660112796368e-05, 'epoch': 0.01}
{'loss': 4.0423, 'grad_norm': 0.7001167163682241, 'learning_rate': 8.31460368099039e-05, 'epoch': 0.01}
{'loss': 3.9797, 'grad_norm': 0.7003856866828987, 'learning_rate': 8.319526521374312e-05, 'epoch': 0.01}
{'loss': 4.0366, 'grad_norm': 0.7872236492565553, 'learning_rate': 8.324428807041249e-05, 'epoch': 0.01}
[2024-04-11 23:28:44,062] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1200 is about to be saved!
[2024-04-11 23:28:44,067] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-1200/global_step1200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 23:28:44,067] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1200/global_step1200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 23:28:44,073] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1200/global_step1200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 23:28:44,074] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 23:28:47,350] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 23:28:47,350] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 23:28:47,351] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1200 is ready now!
{'loss': 4.0406, 'grad_norm': 0.6809921655338582, 'learning_rate': 8.329310708925138e-05, 'epoch': 0.01}
{'loss': 4.0185, 'grad_norm': 0.6768931784164217, 'learning_rate': 8.334172395836509e-05, 'epoch': 0.01}
{'loss': 3.975, 'grad_norm': 0.6858703357051538, 'learning_rate': 8.339014034497491e-05, 'epoch': 0.01}
{'loss': 3.9958, 'grad_norm': 0.6723179146922362, 'learning_rate': 8.34383578957614e-05, 'epoch': 0.01}
{'loss': 4.023, 'grad_norm': 0.6722626962966515, 'learning_rate': 8.348637823720025e-05, 'epoch': 0.01}
{'loss': 3.9932, 'grad_norm': 0.7207492921391696, 'learning_rate': 8.353420297589165e-05, 'epoch': 0.01}
{'loss': 3.973, 'grad_norm': 0.6746183499388734, 'learning_rate': 8.358183369888267e-05, 'epoch': 0.01}
{'loss': 3.9843, 'grad_norm': 0.6414776781683948, 'learning_rate': 8.362927197398341e-05, 'epoch': 0.01}
{'loss': 3.9578, 'grad_norm': 0.6975957744744589, 'learning_rate': 8.367651935007651e-05, 'epoch': 0.01}
{'loss': 3.9497, 'grad_norm': 0.7387681851445321, 'learning_rate': 8.372357735742074e-05, 'epoch': 0.01}
{'loss': 4.032, 'grad_norm': 0.693842338067663, 'learning_rate': 8.377044750794827e-05, 'epoch': 0.01}
{'loss': 3.9376, 'grad_norm': 0.7181100329694149, 'learning_rate': 8.381713129555623e-05, 'epoch': 0.01}
{'loss': 3.9331, 'grad_norm': 0.7400446145489233, 'learning_rate': 8.386363019639234e-05, 'epoch': 0.01}
{'loss': 3.9942, 'grad_norm': 0.709881360708876, 'learning_rate': 8.390994566913507e-05, 'epoch': 0.01}
{'loss': 3.9483, 'grad_norm': 0.745628220516507, 'learning_rate': 8.395607915526815e-05, 'epoch': 0.01}
{'loss': 3.9839, 'grad_norm': 0.7366330247392574, 'learning_rate': 8.400203207934977e-05, 'epoch': 0.01}
{'loss': 3.9345, 'grad_norm': 0.828943161710299, 'learning_rate': 8.404780584927655e-05, 'epoch': 0.01}
{'loss': 3.9341, 'grad_norm': 0.7582881459444344, 'learning_rate': 8.409340185654231e-05, 'epoch': 0.01}
{'loss': 3.9704, 'grad_norm': 0.648903984121227, 'learning_rate': 8.413882147649197e-05, 'epoch': 0.01}
{'loss': 3.933, 'grad_norm': 0.6625730944345264, 'learning_rate': 8.418406606857043e-05, 'epoch': 0.01}
[2024-04-11 23:33:26,156] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1300 is about to be saved!
[2024-04-11 23:33:26,163] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-1300/global_step1300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 23:33:26,164] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1300/global_step1300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 23:33:26,170] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1300/global_step1300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 23:33:26,171] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1300/global_step1300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 23:33:29,539] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1300/global_step1300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 23:33:29,539] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-1300/global_step1300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 23:33:29,539] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1300 is ready now!
{'loss': 3.9246, 'grad_norm': 0.6917386113077963, 'learning_rate': 8.422913697656668e-05, 'epoch': 0.01}
{'loss': 3.9396, 'grad_norm': 0.7068085782251926, 'learning_rate': 8.427403552885332e-05, 'epoch': 0.01}
{'loss': 3.9198, 'grad_norm': 0.6599191860501514, 'learning_rate': 8.431876303862155e-05, 'epoch': 0.01}
{'loss': 3.9223, 'grad_norm': 0.7699471739271015, 'learning_rate': 8.436332080411142e-05, 'epoch': 0.01}
{'loss': 3.9299, 'grad_norm': 0.648801953796625, 'learning_rate': 8.44077101088382e-05, 'epoch': 0.01}
{'loss': 3.9358, 'grad_norm': 0.6433872708154461, 'learning_rate': 8.445193222181402e-05, 'epoch': 0.01}
{'loss': 3.8988, 'grad_norm': 0.6555890229312635, 'learning_rate': 8.449598839776565e-05, 'epoch': 0.01}
{'loss': 3.9064, 'grad_norm': 0.6811528594658424, 'learning_rate': 8.453987987734808e-05, 'epoch': 0.01}
{'loss': 3.9209, 'grad_norm': 0.67855247889489, 'learning_rate': 8.45836078873542e-05, 'epoch': 0.01}
{'loss': 3.8993, 'grad_norm': 0.6843544282751131, 'learning_rate': 8.462717364092046e-05, 'epoch': 0.01}
{'loss': 3.961, 'grad_norm': 0.7780407818492867, 'learning_rate': 8.467057833772894e-05, 'epoch': 0.01}
{'loss': 3.9421, 'grad_norm': 0.7331494462777409, 'learning_rate': 8.471382316420545e-05, 'epoch': 0.01}
{'loss': 3.8771, 'grad_norm': 0.6799033724049722, 'learning_rate': 8.475690929371418e-05, 'epoch': 0.01}
{'loss': 3.9147, 'grad_norm': 0.7684662421908477, 'learning_rate': 8.479983788674874e-05, 'epoch': 0.01}
{'loss': 3.9359, 'grad_norm': 0.7172096547097697, 'learning_rate': 8.484261009111968e-05, 'epoch': 0.01}
{'loss': 3.9029, 'grad_norm': 0.7142580940665961, 'learning_rate': 8.488522704213867e-05, 'epoch': 0.01}
{'loss': 3.8647, 'grad_norm': 0.6665691445476044, 'learning_rate': 8.492768986279929e-05, 'epoch': 0.01}
{'loss': 3.8585, 'grad_norm': 0.6449691761561119, 'learning_rate': 8.496999966395455e-05, 'epoch': 0.01}
{'loss': 3.8795, 'grad_norm': 0.6552471073506201, 'learning_rate': 8.501215754449139e-05, 'epoch': 0.01}
{'loss': 3.9001, 'grad_norm': 0.6598448292402226, 'learning_rate': 8.505416459150177e-05, 'epoch': 0.01}
[2024-04-11 23:38:08,505] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1400 is about to be saved!
[2024-04-11 23:38:08,511] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-1400/global_step1400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 23:38:08,511] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1400/global_step1400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 23:38:08,517] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1400/global_step1400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 23:38:08,517] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1400/global_step1400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 23:38:11,776] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1400/global_step1400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 23:38:11,776] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-1400/global_step1400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 23:38:11,777] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1400 is ready now!
{'loss': 3.8879, 'grad_norm': 0.666953564985436, 'learning_rate': 8.509602188045101e-05, 'epoch': 0.01}
{'loss': 3.8635, 'grad_norm': 0.6798693448879517, 'learning_rate': 8.513773047534291e-05, 'epoch': 0.01}
{'loss': 3.8966, 'grad_norm': 0.7389402931045508, 'learning_rate': 8.517929142888206e-05, 'epoch': 0.01}
{'loss': 3.8912, 'grad_norm': 0.6800323082838199, 'learning_rate': 8.522070578263329e-05, 'epoch': 0.01}
{'loss': 3.8706, 'grad_norm': 0.6385366087835623, 'learning_rate': 8.526197456717826e-05, 'epoch': 0.01}
{'loss': 3.8586, 'grad_norm': 0.6646372525825505, 'learning_rate': 8.530309880226936e-05, 'epoch': 0.01}
{'loss': 3.8422, 'grad_norm': 0.7001669111624726, 'learning_rate': 8.534407949698094e-05, 'epoch': 0.01}
{'loss': 3.8778, 'grad_norm': 0.689794125180531, 'learning_rate': 8.538491764985775e-05, 'epoch': 0.01}
{'loss': 3.8447, 'grad_norm': 0.6913222026177338, 'learning_rate': 8.542561424906111e-05, 'epoch': 0.01}
{'loss': 3.8737, 'grad_norm': 0.7127701390781926, 'learning_rate': 8.546617027251222e-05, 'epoch': 0.01}
{'loss': 3.9615, 'grad_norm': 0.7555694458693774, 'learning_rate': 8.550658668803327e-05, 'epoch': 0.01}
{'loss': 3.8351, 'grad_norm': 0.7783574531439762, 'learning_rate': 8.554686445348594e-05, 'epoch': 0.01}
{'loss': 3.8935, 'grad_norm': 0.6409885928934187, 'learning_rate': 8.558700451690766e-05, 'epoch': 0.01}
{'loss': 3.8204, 'grad_norm': 0.7133302869178093, 'learning_rate': 8.562700781664552e-05, 'epoch': 0.01}
{'loss': 3.8245, 'grad_norm': 0.6904487599686577, 'learning_rate': 8.566687528148781e-05, 'epoch': 0.01}
{'loss': 3.8127, 'grad_norm': 0.7138383924605757, 'learning_rate': 8.57066078307935e-05, 'epoch': 0.01}
{'loss': 3.8435, 'grad_norm': 0.7923668469031689, 'learning_rate': 8.574620637461941e-05, 'epoch': 0.01}
{'loss': 3.848, 'grad_norm': 0.6630148028543347, 'learning_rate': 8.578567181384524e-05, 'epoch': 0.01}
{'loss': 3.792, 'grad_norm': 0.6735844996674343, 'learning_rate': 8.582500504029662e-05, 'epoch': 0.01}
{'loss': 3.8266, 'grad_norm': 0.7155028453163822, 'learning_rate': 8.586420693686602e-05, 'epoch': 0.01}
[2024-04-11 23:42:50,794] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1500 is about to be saved!
[2024-04-11 23:42:50,800] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 23:42:50,800] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 23:42:50,806] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 23:42:50,807] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1500/global_step1500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 23:42:54,149] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1500/global_step1500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 23:42:54,149] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-1500/global_step1500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 23:42:54,150] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1500 is ready now!
{'loss': 3.8113, 'grad_norm': 0.740061166389729, 'learning_rate': 8.59032783776316e-05, 'epoch': 0.01}
{'loss': 3.8224, 'grad_norm': 0.6558095592935654, 'learning_rate': 8.594222022797423e-05, 'epoch': 0.01}
{'loss': 3.8313, 'grad_norm': 0.638882861835098, 'learning_rate': 8.598103334469243e-05, 'epoch': 0.01}
{'loss': 3.8283, 'grad_norm': 0.7006216208381058, 'learning_rate': 8.601971857611555e-05, 'epoch': 0.01}
{'loss': 3.8345, 'grad_norm': 0.6620465092101162, 'learning_rate': 8.605827676221493e-05, 'epoch': 0.01}
{'loss': 3.8376, 'grad_norm': 0.652605764957258, 'learning_rate': 8.609670873471342e-05, 'epoch': 0.01}
{'loss': 3.8336, 'grad_norm': 0.7647169593963304, 'learning_rate': 8.613501531719302e-05, 'epoch': 0.01}
{'loss': 3.8219, 'grad_norm': 0.6801071497581237, 'learning_rate': 8.617319732520071e-05, 'epoch': 0.01}
{'loss': 3.7989, 'grad_norm': 0.6783274949536816, 'learning_rate': 8.621125556635271e-05, 'epoch': 0.01}
{'loss': 3.7579, 'grad_norm': 0.676390260138666, 'learning_rate': 8.624919084043694e-05, 'epoch': 0.01}
{'loss': 3.7944, 'grad_norm': 0.6628625002029161, 'learning_rate': 8.628700393951384e-05, 'epoch': 0.01}
{'loss': 3.7935, 'grad_norm': 0.6723353666726702, 'learning_rate': 8.632469564801571e-05, 'epoch': 0.01}
{'loss': 3.8245, 'grad_norm': 0.667815885052215, 'learning_rate': 8.636226674284417e-05, 'epoch': 0.01}
{'loss': 3.823, 'grad_norm': 0.6997231623524478, 'learning_rate': 8.639971799346644e-05, 'epoch': 0.01}
{'loss': 3.7684, 'grad_norm': 0.6960125866843238, 'learning_rate': 8.643705016200976e-05, 'epoch': 0.01}
{'loss': 3.8547, 'grad_norm': 0.6966737028249512, 'learning_rate': 8.647426400335451e-05, 'epoch': 0.01}
{'loss': 3.7476, 'grad_norm': 0.6812047071532861, 'learning_rate': 8.651136026522576e-05, 'epoch': 0.01}
{'loss': 3.7706, 'grad_norm': 0.6704804759744027, 'learning_rate': 8.654833968828348e-05, 'epoch': 0.01}
{'loss': 3.8047, 'grad_norm': 0.611720208404299, 'learning_rate': 8.658520300621115e-05, 'epoch': 0.01}
{'loss': 3.7803, 'grad_norm': 0.6384538987398514, 'learning_rate': 8.66219509458033e-05, 'epoch': 0.01}
[2024-04-11 23:47:32,981] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1600 is about to be saved!
[2024-04-11 23:47:32,986] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-1600/global_step1600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 23:47:32,986] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1600/global_step1600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 23:47:32,992] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1600/global_step1600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 23:47:32,993] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 23:47:36,221] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 23:47:36,221] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 23:47:36,222] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1600 is ready now!
{'loss': 3.7724, 'grad_norm': 0.6788315750992987, 'learning_rate': 8.665858422705127e-05, 'epoch': 0.01}
{'loss': 3.7337, 'grad_norm': 0.6606106794895605, 'learning_rate': 8.669510356322798e-05, 'epoch': 0.01}
{'loss': 3.7967, 'grad_norm': 0.6455898345062238, 'learning_rate': 8.673150966097122e-05, 'epoch': 0.01}
{'loss': 3.7953, 'grad_norm': 0.7283366712231751, 'learning_rate': 8.676780322036573e-05, 'epoch': 0.01}
{'loss': 3.7594, 'grad_norm': 0.7247004672846645, 'learning_rate': 8.680398493502396e-05, 'epoch': 0.01}
{'loss': 3.7637, 'grad_norm': 0.6720483155739435, 'learning_rate': 8.684005549216557e-05, 'epoch': 0.01}
{'loss': 3.7408, 'grad_norm': 0.7107759840459011, 'learning_rate': 8.687601557269576e-05, 'epoch': 0.01}
{'loss': 3.7538, 'grad_norm': 0.7373809271639574, 'learning_rate': 8.691186585128246e-05, 'epoch': 0.01}
{'loss': 3.7526, 'grad_norm': 0.6594888276437807, 'learning_rate': 8.694760699643221e-05, 'epoch': 0.01}
{'loss': 3.7883, 'grad_norm': 0.7986173663460285, 'learning_rate': 8.698323967056495e-05, 'epoch': 0.01}
{'loss': 3.8012, 'grad_norm': 0.8394690306275568, 'learning_rate': 8.701876453008776e-05, 'epoch': 0.01}
{'loss': 3.7209, 'grad_norm': 0.81923795384767, 'learning_rate': 8.705418222546732e-05, 'epoch': 0.01}
{'loss': 3.7285, 'grad_norm': 0.6519879338551006, 'learning_rate': 8.70894934013015e-05, 'epoch': 0.01}
{'loss': 3.7259, 'grad_norm': 0.7056263570684156, 'learning_rate': 8.712469869638952e-05, 'epoch': 0.01}
{'loss': 3.7371, 'grad_norm': 0.6595326594640858, 'learning_rate': 8.71597987438016e-05, 'epoch': 0.01}
{'loss': 3.7454, 'grad_norm': 0.6831493588599153, 'learning_rate': 8.719479417094704e-05, 'epoch': 0.01}
{'loss': 3.7352, 'grad_norm': 0.641019154971486, 'learning_rate': 8.722968559964157e-05, 'epoch': 0.01}
{'loss': 3.7288, 'grad_norm': 0.6599112290002582, 'learning_rate': 8.726447364617366e-05, 'epoch': 0.01}
{'loss': 3.7075, 'grad_norm': 0.6600453670813058, 'learning_rate': 8.72991589213698e-05, 'epoch': 0.01}
{'loss': 3.7348, 'grad_norm': 0.673340849679298, 'learning_rate': 8.733374203065898e-05, 'epoch': 0.01}
[2024-04-11 23:52:15,055] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1700 is about to be saved!
[2024-04-11 23:52:15,060] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-1700/global_step1700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 23:52:15,060] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1700/global_step1700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 23:52:15,067] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1700/global_step1700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 23:52:15,067] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1700/global_step1700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 23:52:18,180] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1700/global_step1700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 23:52:18,181] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-1700/global_step1700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 23:52:18,181] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1700 is ready now!
{'loss': 3.7333, 'grad_norm': 0.6863711143568567, 'learning_rate': 8.736822357413588e-05, 'epoch': 0.01}
{'loss': 3.7856, 'grad_norm': 0.7048122932892785, 'learning_rate': 8.740260414662352e-05, 'epoch': 0.01}
{'loss': 3.7222, 'grad_norm': 0.6960575114448905, 'learning_rate': 8.74368843377348e-05, 'epoch': 0.02}
{'loss': 3.7364, 'grad_norm': 0.6993537137294754, 'learning_rate': 8.747106473193313e-05, 'epoch': 0.02}
{'loss': 3.7187, 'grad_norm': 0.6091297688202981, 'learning_rate': 8.75051459085922e-05, 'epoch': 0.02}
{'loss': 3.7599, 'grad_norm': 0.7332316813294721, 'learning_rate': 8.753912844205501e-05, 'epoch': 0.02}
{'loss': 3.7607, 'grad_norm': 0.7407555207477231, 'learning_rate': 8.757301290169182e-05, 'epoch': 0.02}
{'loss': 3.6934, 'grad_norm': 0.6837668344258152, 'learning_rate': 8.76067998519575e-05, 'epoch': 0.02}
{'loss': 3.706, 'grad_norm': 0.6946421771025896, 'learning_rate': 8.764048985244785e-05, 'epoch': 0.02}
{'loss': 3.6793, 'grad_norm': 0.6432177695791185, 'learning_rate': 8.76740834579553e-05, 'epoch': 0.02}
{'loss': 3.7272, 'grad_norm': 0.6538121586344632, 'learning_rate': 8.770758121852369e-05, 'epoch': 0.02}
{'loss': 3.7535, 'grad_norm': 0.6287054894444741, 'learning_rate': 8.774098367950224e-05, 'epoch': 0.02}
{'loss': 3.6767, 'grad_norm': 0.6445153437462767, 'learning_rate': 8.777429138159897e-05, 'epoch': 0.02}
{'loss': 3.7291, 'grad_norm': 0.728158082026414, 'learning_rate': 8.780750486093308e-05, 'epoch': 0.02}
{'loss': 3.6956, 'grad_norm': 0.6387706900397885, 'learning_rate': 8.784062464908682e-05, 'epoch': 0.02}
{'loss': 3.6978, 'grad_norm': 0.6755859097631521, 'learning_rate': 8.787365127315646e-05, 'epoch': 0.02}
{'loss': 3.7204, 'grad_norm': 0.7163963644893434, 'learning_rate': 8.79065852558027e-05, 'epoch': 0.02}
{'loss': 3.7024, 'grad_norm': 0.6820331746911626, 'learning_rate': 8.79394271153003e-05, 'epoch': 0.02}
{'loss': 3.7036, 'grad_norm': 0.6269302683419111, 'learning_rate': 8.797217736558683e-05, 'epoch': 0.02}
{'loss': 3.665, 'grad_norm': 0.6638772891963239, 'learning_rate': 8.800483651631128e-05, 'epoch': 0.02}
[2024-04-11 23:56:56,971] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1800 is about to be saved!
[2024-04-11 23:56:56,977] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-1800/global_step1800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-11 23:56:56,977] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1800/global_step1800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-11 23:56:56,984] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1800/global_step1800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-11 23:56:56,984] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-11 23:57:00,110] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-11 23:57:00,110] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-11 23:57:00,110] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1800 is ready now!
{'loss': 3.6728, 'grad_norm': 0.6824580772161064, 'learning_rate': 8.803740507288132e-05, 'epoch': 0.02}
{'loss': 3.7037, 'grad_norm': 0.71828323834831, 'learning_rate': 8.806988353651037e-05, 'epoch': 0.02}
{'loss': 3.6306, 'grad_norm': 0.6281048025838327, 'learning_rate': 8.810227240426389e-05, 'epoch': 0.02}
{'loss': 3.7059, 'grad_norm': 0.6186624245596589, 'learning_rate': 8.813457216910499e-05, 'epoch': 0.02}
{'loss': 3.666, 'grad_norm': 0.6060998424615194, 'learning_rate': 8.816678331993947e-05, 'epoch': 0.02}
{'loss': 3.7255, 'grad_norm': 0.6769571030946869, 'learning_rate': 8.81989063416602e-05, 'epoch': 0.02}
{'loss': 3.6544, 'grad_norm': 0.6432972227179974, 'learning_rate': 8.823094171519092e-05, 'epoch': 0.02}
{'loss': 3.6469, 'grad_norm': 0.6347172183529943, 'learning_rate': 8.82628899175295e-05, 'epoch': 0.02}
{'loss': 3.6669, 'grad_norm': 0.6318513916618571, 'learning_rate': 8.829475142179045e-05, 'epoch': 0.02}
{'loss': 3.6817, 'grad_norm': 0.6790594489442655, 'learning_rate': 8.832652669724704e-05, 'epoch': 0.02}
{'loss': 3.6188, 'grad_norm': 0.6584068420368208, 'learning_rate': 8.835821620937276e-05, 'epoch': 0.02}
{'loss': 3.6929, 'grad_norm': 0.7616186897697758, 'learning_rate': 8.838982041988221e-05, 'epoch': 0.02}
{'loss': 3.6573, 'grad_norm': 0.6408848666077006, 'learning_rate': 8.842133978677145e-05, 'epoch': 0.02}
{'loss': 3.6253, 'grad_norm': 0.6502956511979644, 'learning_rate': 8.845277476435792e-05, 'epoch': 0.02}
{'loss': 3.711, 'grad_norm': 0.739500651650439, 'learning_rate': 8.848412580331955e-05, 'epoch': 0.02}
{'loss': 3.6986, 'grad_norm': 0.6817446731047405, 'learning_rate': 8.851539335073373e-05, 'epoch': 0.02}
{'loss': 3.6374, 'grad_norm': 0.7174305990031663, 'learning_rate': 8.854657785011544e-05, 'epoch': 0.02}
{'loss': 3.6199, 'grad_norm': 0.6653860392535959, 'learning_rate': 8.857767974145503e-05, 'epoch': 0.02}
{'loss': 3.624, 'grad_norm': 0.6539824666395903, 'learning_rate': 8.860869946125547e-05, 'epoch': 0.02}
{'loss': 3.6499, 'grad_norm': 0.680718250018016, 'learning_rate': 8.863963744256908e-05, 'epoch': 0.02}
[2024-04-12 00:01:38,940] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1900 is about to be saved!
[2024-04-12 00:01:38,947] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-1900/global_step1900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 00:01:38,947] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1900/global_step1900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 00:01:38,953] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1900/global_step1900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 00:01:38,953] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-1900/global_step1900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 00:01:42,022] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-1900/global_step1900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 00:01:42,023] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-1900/global_step1900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 00:01:42,023] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1900 is ready now!
{'loss': 3.6864, 'grad_norm': 0.6469588257163109, 'learning_rate': 8.867049411503387e-05, 'epoch': 0.02}
{'loss': 3.6031, 'grad_norm': 0.6848250607733916, 'learning_rate': 8.87012699049093e-05, 'epoch': 0.02}
{'loss': 3.672, 'grad_norm': 0.6741998153747251, 'learning_rate': 8.873196523511157e-05, 'epoch': 0.02}
{'loss': 3.6261, 'grad_norm': 0.655546349910777, 'learning_rate': 8.876258052524857e-05, 'epoch': 0.02}
{'loss': 3.6148, 'grad_norm': 0.6928775218282521, 'learning_rate': 8.879311619165424e-05, 'epoch': 0.02}
{'loss': 3.5948, 'grad_norm': 0.648553056864981, 'learning_rate': 8.882357264742258e-05, 'epoch': 0.02}
{'loss': 3.6617, 'grad_norm': 0.6832919945202329, 'learning_rate': 8.885395030244112e-05, 'epoch': 0.02}
{'loss': 3.6428, 'grad_norm': 0.6295079789498058, 'learning_rate': 8.88842495634241e-05, 'epoch': 0.02}
{'loss': 3.6775, 'grad_norm': 0.6473210582951929, 'learning_rate': 8.891447083394507e-05, 'epoch': 0.02}
{'loss': 3.6428, 'grad_norm': 0.6548093750210141, 'learning_rate': 8.894461451446924e-05, 'epoch': 0.02}
{'loss': 3.6355, 'grad_norm': 0.6476185150942233, 'learning_rate': 8.897468100238516e-05, 'epoch': 0.02}
{'loss': 3.6216, 'grad_norm': 0.7001938520978809, 'learning_rate': 8.900467069203634e-05, 'epoch': 0.02}
{'loss': 3.6111, 'grad_norm': 0.7279617645058544, 'learning_rate': 8.903458397475213e-05, 'epoch': 0.02}
{'loss': 3.6233, 'grad_norm': 0.6387812369955208, 'learning_rate': 8.906442123887845e-05, 'epoch': 0.02}
{'loss': 3.6549, 'grad_norm': 0.690366172386434, 'learning_rate': 8.909418286980804e-05, 'epoch': 0.02}
{'loss': 3.6805, 'grad_norm': 0.7071178456663529, 'learning_rate': 8.912386925001022e-05, 'epoch': 0.02}
{'loss': 3.5955, 'grad_norm': 0.6542184083706529, 'learning_rate': 8.915348075906055e-05, 'epoch': 0.02}
{'loss': 3.5664, 'grad_norm': 0.6711869731154113, 'learning_rate': 8.918301777366981e-05, 'epoch': 0.02}
{'loss': 3.6291, 'grad_norm': 0.6383907972997782, 'learning_rate': 8.921248066771283e-05, 'epoch': 0.02}
{'loss': 3.6603, 'grad_norm': 0.6688628890509699, 'learning_rate': 8.924186981225684e-05, 'epoch': 0.02}
[2024-04-12 00:06:20,899] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!
[2024-04-12 00:06:20,904] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 00:06:20,904] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 00:06:20,911] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 00:06:20,911] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 00:06:23,927] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 00:06:23,927] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 00:06:23,927] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
{'loss': 3.5725, 'grad_norm': 0.6595681873744991, 'learning_rate': 8.927118557558958e-05, 'epoch': 0.02}
{'loss': 3.6101, 'grad_norm': 0.634288032445178, 'learning_rate': 8.930042832324688e-05, 'epoch': 0.02}
{'loss': 3.6234, 'grad_norm': 0.6870924031575848, 'learning_rate': 8.932959841804015e-05, 'epoch': 0.02}
{'loss': 3.5894, 'grad_norm': 0.6605084855506747, 'learning_rate': 8.935869622008325e-05, 'epoch': 0.02}
{'loss': 3.5903, 'grad_norm': 0.6581377283890357, 'learning_rate': 8.938772208681927e-05, 'epoch': 0.02}
{'loss': 3.567, 'grad_norm': 0.6553850406075845, 'learning_rate': 8.941667637304679e-05, 'epoch': 0.02}
{'loss': 3.5675, 'grad_norm': 0.6461233020678606, 'learning_rate': 8.944555943094597e-05, 'epoch': 0.02}
{'loss': 3.6058, 'grad_norm': 0.6673386451258705, 'learning_rate': 8.947437161010425e-05, 'epoch': 0.02}
{'loss': 3.5477, 'grad_norm': 0.6604877225675262, 'learning_rate': 8.950311325754165e-05, 'epoch': 0.02}
{'loss': 3.5907, 'grad_norm': 0.6755943056545427, 'learning_rate': 8.953178471773599e-05, 'epoch': 0.02}
{'loss': 3.585, 'grad_norm': 0.6886853379906603, 'learning_rate': 8.956038633264753e-05, 'epoch': 0.02}
{'loss': 3.5916, 'grad_norm': 0.6319036990068725, 'learning_rate': 8.958891844174354e-05, 'epoch': 0.02}
{'loss': 3.5962, 'grad_norm': 0.6805235052371145, 'learning_rate': 8.961738138202236e-05, 'epoch': 0.02}
{'loss': 3.5998, 'grad_norm': 0.6816975482313798, 'learning_rate': 8.964577548803747e-05, 'epoch': 0.02}
{'loss': 3.5424, 'grad_norm': 0.6736367023712, 'learning_rate': 8.967410109192087e-05, 'epoch': 0.02}
{'loss': 3.6026, 'grad_norm': 0.7035057483568308, 'learning_rate': 8.970235852340652e-05, 'epoch': 0.02}
{'loss': 3.6285, 'grad_norm': 0.5751020217775569, 'learning_rate': 8.973054810985336e-05, 'epoch': 0.02}
{'loss': 3.5373, 'grad_norm': 0.6176718858771095, 'learning_rate': 8.975867017626801e-05, 'epoch': 0.02}
{'loss': 3.569, 'grad_norm': 0.6541155911892582, 'learning_rate': 8.978672504532734e-05, 'epoch': 0.02}
{'loss': 3.5554, 'grad_norm': 0.6590529782959914, 'learning_rate': 8.981471303740057e-05, 'epoch': 0.02}
[2024-04-12 00:11:02,726] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2100 is about to be saved!
[2024-04-12 00:11:02,732] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-2100/global_step2100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 00:11:02,732] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2100/global_step2100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 00:11:02,738] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2100/global_step2100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 00:11:02,738] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2100/global_step2100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 00:11:05,732] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2100/global_step2100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 00:11:05,732] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-2100/global_step2100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 00:11:05,733] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2100 is ready now!
{'loss': 3.5799, 'grad_norm': 0.6592681880878933, 'learning_rate': 8.984263447057135e-05, 'epoch': 0.02}
{'loss': 3.5968, 'grad_norm': 0.6593195063940473, 'learning_rate': 8.987048966065936e-05, 'epoch': 0.02}
{'loss': 3.6168, 'grad_norm': 0.6627258394452934, 'learning_rate': 8.98982789212417e-05, 'epoch': 0.02}
{'loss': 3.6001, 'grad_norm': 0.7250847690014592, 'learning_rate': 8.99260025636743e-05, 'epoch': 0.02}
{'loss': 3.5407, 'grad_norm': 0.6774691506336737, 'learning_rate': 8.99536608971125e-05, 'epoch': 0.02}
{'loss': 3.5585, 'grad_norm': 0.6764019962828205, 'learning_rate': 8.998125422853208e-05, 'epoch': 0.02}
{'loss': 3.5329, 'grad_norm': 0.6849125633265495, 'learning_rate': 9.000878286274949e-05, 'epoch': 0.02}
{'loss': 3.5361, 'grad_norm': 0.6646838242507128, 'learning_rate': 9.003624710244208e-05, 'epoch': 0.02}
{'loss': 3.5945, 'grad_norm': 0.706641149875097, 'learning_rate': 9.006364724816817e-05, 'epoch': 0.02}
{'loss': 3.5055, 'grad_norm': 0.6209560311008723, 'learning_rate': 9.009098359838666e-05, 'epoch': 0.02}
{'loss': 3.6221, 'grad_norm': 0.7252377772247139, 'learning_rate': 9.011825644947662e-05, 'epoch': 0.02}
{'loss': 3.6139, 'grad_norm': 0.6988788766179591, 'learning_rate': 9.014546609575655e-05, 'epoch': 0.02}
{'loss': 3.5817, 'grad_norm': 0.6716830198586309, 'learning_rate': 9.017261282950343e-05, 'epoch': 0.02}
{'loss': 3.536, 'grad_norm': 0.626414411890231, 'learning_rate': 9.01996969409715e-05, 'epoch': 0.02}
{'loss': 3.5732, 'grad_norm': 0.6568757799568581, 'learning_rate': 9.022671871841103e-05, 'epoch': 0.02}
{'loss': 3.5588, 'grad_norm': 0.6307206408210759, 'learning_rate': 9.025367844808657e-05, 'epoch': 0.02}
{'loss': 3.5295, 'grad_norm': 0.6246844672414756, 'learning_rate': 9.028057641429526e-05, 'epoch': 0.02}
{'loss': 3.5262, 'grad_norm': 0.6501602891418912, 'learning_rate': 9.030741289938474e-05, 'epoch': 0.02}
{'loss': 3.5207, 'grad_norm': 0.635939527973606, 'learning_rate': 9.033418818377097e-05, 'epoch': 0.02}
{'loss': 3.5291, 'grad_norm': 0.6391276766992802, 'learning_rate': 9.036090254595577e-05, 'epoch': 0.02}
[2024-04-12 00:15:44,612] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2200 is about to be saved!
[2024-04-12 00:15:44,619] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-2200/global_step2200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 00:15:44,619] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2200/global_step2200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 00:15:44,625] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2200/global_step2200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 00:15:44,625] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2200/global_step2200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 00:15:47,646] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2200/global_step2200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 00:15:47,646] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-2200/global_step2200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 00:15:47,647] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2200 is ready now!
{'loss': 3.502, 'grad_norm': 0.6137913124477444, 'learning_rate': 9.038755626254433e-05, 'epoch': 0.02}
{'loss': 3.5745, 'grad_norm': 0.6561920011499752, 'learning_rate': 9.041414960826219e-05, 'epoch': 0.02}
{'loss': 3.55, 'grad_norm': 0.6636794741738701, 'learning_rate': 9.044068285597243e-05, 'epoch': 0.02}
{'loss': 3.5649, 'grad_norm': 0.682119928402143, 'learning_rate': 9.046715627669231e-05, 'epoch': 0.02}
{'loss': 3.5244, 'grad_norm': 0.6571379383406194, 'learning_rate': 9.049357013960999e-05, 'epoch': 0.02}
{'loss': 3.544, 'grad_norm': 0.7090761051308351, 'learning_rate': 9.051992471210092e-05, 'epoch': 0.02}
{'loss': 3.5489, 'grad_norm': 0.6327020952936446, 'learning_rate': 9.054622025974403e-05, 'epoch': 0.02}
{'loss': 3.5439, 'grad_norm': 0.684253642262814, 'learning_rate': 9.057245704633787e-05, 'epoch': 0.02}
{'loss': 3.5254, 'grad_norm': 0.6811989234689291, 'learning_rate': 9.059863533391642e-05, 'epoch': 0.02}
{'loss': 3.5962, 'grad_norm': 0.6670076205422847, 'learning_rate': 9.062475538276482e-05, 'epoch': 0.02}
{'loss': 3.552, 'grad_norm': 0.6430941570834603, 'learning_rate': 9.065081745143493e-05, 'epoch': 0.02}
{'loss': 3.4955, 'grad_norm': 0.7072731375678246, 'learning_rate': 9.067682179676063e-05, 'epoch': 0.02}
{'loss': 3.5363, 'grad_norm': 0.6363507023000933, 'learning_rate': 9.070276867387303e-05, 'epoch': 0.02}
{'loss': 3.518, 'grad_norm': 0.6663047603245722, 'learning_rate': 9.07286583362155e-05, 'epoch': 0.02}
{'loss': 3.4924, 'grad_norm': 0.6536780803252044, 'learning_rate': 9.075449103555852e-05, 'epoch': 0.02}
{'loss': 3.5251, 'grad_norm': 0.6782314906185352, 'learning_rate': 9.078026702201434e-05, 'epoch': 0.02}
{'loss': 3.5242, 'grad_norm': 0.6975752033149561, 'learning_rate': 9.080598654405155e-05, 'epoch': 0.02}
{'loss': 3.5393, 'grad_norm': 0.6758456946675524, 'learning_rate': 9.083164984850946e-05, 'epoch': 0.02}
{'loss': 3.4984, 'grad_norm': 0.6547274121457516, 'learning_rate': 9.085725718061223e-05, 'epoch': 0.02}
{'loss': 3.5429, 'grad_norm': 0.7347502256922541, 'learning_rate': 9.088280878398303e-05, 'epoch': 0.02}
[2024-04-12 00:20:26,564] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2300 is about to be saved!
[2024-04-12 00:20:26,570] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-2300/global_step2300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 00:20:26,570] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2300/global_step2300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 00:20:26,576] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2300/global_step2300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 00:20:26,576] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2300/global_step2300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 00:20:29,646] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2300/global_step2300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 00:20:29,647] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-2300/global_step2300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 00:20:29,647] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2300 is ready now!
{'loss': 3.4851, 'grad_norm': 0.6429581164919478, 'learning_rate': 9.090830490065791e-05, 'epoch': 0.02}
{'loss': 3.5217, 'grad_norm': 0.6649893844416571, 'learning_rate': 9.093374577109952e-05, 'epoch': 0.02}
{'loss': 3.4255, 'grad_norm': 0.6584250825311283, 'learning_rate': 9.095913163421078e-05, 'epoch': 0.02}
{'loss': 3.4947, 'grad_norm': 0.6362816100158293, 'learning_rate': 9.098446272734832e-05, 'epoch': 0.02}
{'loss': 3.487, 'grad_norm': 0.6421968706837545, 'learning_rate': 9.100973928633574e-05, 'epoch': 0.02}
{'loss': 3.5295, 'grad_norm': 0.6512404626454021, 'learning_rate': 9.103496154547686e-05, 'epoch': 0.02}
{'loss': 3.4706, 'grad_norm': 0.6273095261785393, 'learning_rate': 9.106012973756875e-05, 'epoch': 0.02}
{'loss': 3.552, 'grad_norm': 0.6552032153402988, 'learning_rate': 9.10852440939145e-05, 'epoch': 0.02}
{'loss': 3.5302, 'grad_norm': 0.6231749643335538, 'learning_rate': 9.111030484433618e-05, 'epoch': 0.02}
{'loss': 3.4909, 'grad_norm': 0.6680976194913031, 'learning_rate': 9.113531221718726e-05, 'epoch': 0.02}
{'loss': 3.545, 'grad_norm': 0.660848735518759, 'learning_rate': 9.116026643936524e-05, 'epoch': 0.02}
{'loss': 3.5274, 'grad_norm': 0.6352668063787448, 'learning_rate': 9.118516773632391e-05, 'epoch': 0.02}
{'loss': 3.5361, 'grad_norm': 0.6358227007568128, 'learning_rate': 9.12100163320856e-05, 'epoch': 0.02}
{'loss': 3.5869, 'grad_norm': 0.6604712472491105, 'learning_rate': 9.12348124492533e-05, 'epoch': 0.02}
{'loss': 3.505, 'grad_norm': 0.6110290887021408, 'learning_rate': 9.125955630902261e-05, 'epoch': 0.02}
{'loss': 3.4762, 'grad_norm': 0.6418901121990862, 'learning_rate': 9.128424813119353e-05, 'epoch': 0.02}
{'loss': 3.455, 'grad_norm': 0.6667614595773427, 'learning_rate': 9.130888813418227e-05, 'epoch': 0.02}
{'loss': 3.4969, 'grad_norm': 0.6277145666390254, 'learning_rate': 9.133347653503274e-05, 'epoch': 0.02}
{'loss': 3.4635, 'grad_norm': 0.6457053295967161, 'learning_rate': 9.135801354942812e-05, 'epoch': 0.02}
{'loss': 3.4846, 'grad_norm': 0.6406520382267172, 'learning_rate': 9.13824993917021e-05, 'epoch': 0.02}
[2024-04-12 00:25:08,689] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2400 is about to be saved!
[2024-04-12 00:25:08,695] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-2400/global_step2400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 00:25:08,695] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2400/global_step2400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 00:25:08,701] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2400/global_step2400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 00:25:08,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2400/global_step2400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 00:25:11,773] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2400/global_step2400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 00:25:11,773] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-2400/global_step2400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 00:25:11,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2400 is ready now!
{'loss': 3.4907, 'grad_norm': 0.6157045331052221, 'learning_rate': 9.140693427485026e-05, 'epoch': 0.02}
{'loss': 3.4819, 'grad_norm': 0.6249678682736223, 'learning_rate': 9.143131841054102e-05, 'epoch': 0.02}
{'loss': 3.496, 'grad_norm': 0.6258695284841923, 'learning_rate': 9.145565200912677e-05, 'epoch': 0.02}
{'loss': 3.4536, 'grad_norm': 0.6288458677597741, 'learning_rate': 9.14799352796547e-05, 'epoch': 0.02}
{'loss': 3.4983, 'grad_norm': 0.6798263853681348, 'learning_rate': 9.150416842987763e-05, 'epoch': 0.02}
{'loss': 3.4887, 'grad_norm': 0.6826483992372949, 'learning_rate': 9.152835166626453e-05, 'epoch': 0.02}
{'loss': 3.4893, 'grad_norm': 0.6489674968714974, 'learning_rate': 9.15524851940113e-05, 'epoch': 0.02}
{'loss': 3.4276, 'grad_norm': 0.613762072006206, 'learning_rate': 9.157656921705101e-05, 'epoch': 0.02}
{'loss': 3.4584, 'grad_norm': 0.6351859229423154, 'learning_rate': 9.160060393806437e-05, 'epoch': 0.02}
{'loss': 3.5122, 'grad_norm': 0.6584763747389167, 'learning_rate': 9.162458955848987e-05, 'epoch': 0.02}
{'loss': 3.434, 'grad_norm': 0.6559707169760275, 'learning_rate': 9.164852627853402e-05, 'epoch': 0.02}
{'loss': 3.464, 'grad_norm': 0.7071972394626086, 'learning_rate': 9.167241429718126e-05, 'epoch': 0.02}
{'loss': 3.46, 'grad_norm': 0.6710085307411472, 'learning_rate': 9.169625381220399e-05, 'epoch': 0.02}
{'loss': 3.4287, 'grad_norm': 0.6168046509353835, 'learning_rate': 9.172004502017228e-05, 'epoch': 0.02}
{'loss': 3.456, 'grad_norm': 0.6313955273987252, 'learning_rate': 9.174378811646375e-05, 'epoch': 0.02}
{'loss': 3.4203, 'grad_norm': 0.6045143334751625, 'learning_rate': 9.176748329527303e-05, 'epoch': 0.02}
{'loss': 3.4683, 'grad_norm': 0.6177394065166439, 'learning_rate': 9.179113074962138e-05, 'epoch': 0.02}
{'loss': 3.5059, 'grad_norm': 0.6920431313386289, 'learning_rate': 9.181473067136614e-05, 'epoch': 0.02}
{'loss': 3.4376, 'grad_norm': 0.6240964930764163, 'learning_rate': 9.183828325121003e-05, 'epoch': 0.02}
{'loss': 3.4174, 'grad_norm': 0.6250772586828851, 'learning_rate': 9.186178867871037e-05, 'epoch': 0.02}
[2024-04-12 00:29:50,724] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2500 is about to be saved!
[2024-04-12 00:29:50,729] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-2500/global_step2500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 00:29:50,730] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2500/global_step2500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 00:29:50,735] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2500/global_step2500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 00:29:50,736] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2500/global_step2500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 00:29:53,776] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2500/global_step2500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 00:29:53,776] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-2500/global_step2500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 00:29:53,776] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2500 is ready now!
{'loss': 3.5309, 'grad_norm': 0.6413073433032297, 'learning_rate': 9.188524714228833e-05, 'epoch': 0.02}
{'loss': 3.4725, 'grad_norm': 0.6655517272700632, 'learning_rate': 9.19086588292379e-05, 'epoch': 0.02}
{'loss': 3.4384, 'grad_norm': 0.6416602443566296, 'learning_rate': 9.19320239257349e-05, 'epoch': 0.02}
{'loss': 3.4707, 'grad_norm': 0.6423445358855321, 'learning_rate': 9.195534261684585e-05, 'epoch': 0.02}
{'loss': 3.4555, 'grad_norm': 0.6927815398558578, 'learning_rate': 9.197861508653679e-05, 'epoch': 0.02}
{'loss': 3.4431, 'grad_norm': 0.6698470678967955, 'learning_rate': 9.200184151768197e-05, 'epoch': 0.02}
{'loss': 3.4542, 'grad_norm': 0.6586710731929158, 'learning_rate': 9.202502209207245e-05, 'epoch': 0.02}
{'loss': 3.384, 'grad_norm': 0.6612304860369888, 'learning_rate': 9.20481569904247e-05, 'epoch': 0.02}
{'loss': 3.4156, 'grad_norm': 0.6280457745893108, 'learning_rate': 9.207124639238897e-05, 'epoch': 0.02}
{'loss': 3.5038, 'grad_norm': 0.6557325768780644, 'learning_rate': 9.209429047655778e-05, 'epoch': 0.02}
{'loss': 3.435, 'grad_norm': 0.6115477660161072, 'learning_rate': 9.211728942047402e-05, 'epoch': 0.02}
{'loss': 3.4106, 'grad_norm': 0.6493808345217179, 'learning_rate': 9.214024340063939e-05, 'epoch': 0.02}
{'loss': 3.4261, 'grad_norm': 0.6647123608733991, 'learning_rate': 9.216315259252232e-05, 'epoch': 0.02}
{'loss': 3.409, 'grad_norm': 0.5900113070903905, 'learning_rate': 9.218601717056616e-05, 'epoch': 0.02}
{'loss': 3.3988, 'grad_norm': 0.6194322249280824, 'learning_rate': 9.220883730819707e-05, 'epoch': 0.02}
{'loss': 3.4189, 'grad_norm': 0.6405118271304302, 'learning_rate': 9.223161317783193e-05, 'epoch': 0.02}
{'loss': 3.4457, 'grad_norm': 0.6245481660368614, 'learning_rate': 9.22543449508862e-05, 'epoch': 0.02}
{'loss': 3.4545, 'grad_norm': 0.6313589765549392, 'learning_rate': 9.22770327977816e-05, 'epoch': 0.02}
{'loss': 3.4186, 'grad_norm': 0.6762651140641961, 'learning_rate': 9.229967688795381e-05, 'epoch': 0.02}
{'loss': 3.4313, 'grad_norm': 0.6205802508753138, 'learning_rate': 9.232227738986005e-05, 'epoch': 0.02}
[2024-04-12 00:34:32,812] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2600 is about to be saved!
[2024-04-12 00:34:32,817] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-2600/global_step2600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 00:34:32,817] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2600/global_step2600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 00:34:32,823] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2600/global_step2600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 00:34:32,824] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2600/global_step2600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 00:34:35,934] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2600/global_step2600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 00:34:35,934] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-2600/global_step2600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 00:34:35,935] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2600 is ready now!
{'loss': 3.4579, 'grad_norm': 0.6423702674790375, 'learning_rate': 9.234483447098662e-05, 'epoch': 0.02}
{'loss': 3.4116, 'grad_norm': 0.6447061359138346, 'learning_rate': 9.236734829785629e-05, 'epoch': 0.02}
{'loss': 3.4354, 'grad_norm': 0.6231368768052767, 'learning_rate': 9.238981903603578e-05, 'epoch': 0.02}
{'loss': 3.4333, 'grad_norm': 0.6431739367383575, 'learning_rate': 9.241224685014296e-05, 'epoch': 0.02}
{'loss': 3.435, 'grad_norm': 0.6230489766818588, 'learning_rate': 9.24346319038541e-05, 'epoch': 0.02}
{'loss': 3.4044, 'grad_norm': 0.6132839140510757, 'learning_rate': 9.245697435991117e-05, 'epoch': 0.02}
{'loss': 3.4656, 'grad_norm': 0.7285145014531272, 'learning_rate': 9.24792743801287e-05, 'epoch': 0.02}
{'loss': 3.4385, 'grad_norm': 0.6546817876424413, 'learning_rate': 9.250153212540105e-05, 'epoch': 0.02}
{'loss': 3.4307, 'grad_norm': 0.6673490257155639, 'learning_rate': 9.252374775570922e-05, 'epoch': 0.02}
{'loss': 3.3826, 'grad_norm': 0.6364351190829179, 'learning_rate': 9.254592143012783e-05, 'epoch': 0.02}
{'loss': 3.3856, 'grad_norm': 0.6309104916220672, 'learning_rate': 9.256805330683189e-05, 'epoch': 0.02}
{'loss': 3.3767, 'grad_norm': 0.6014606808830721, 'learning_rate': 9.259014354310365e-05, 'epoch': 0.02}
{'loss': 3.3929, 'grad_norm': 0.6205419966765886, 'learning_rate': 9.261219229533921e-05, 'epoch': 0.02}
{'loss': 3.3738, 'grad_norm': 0.6427782153907997, 'learning_rate': 9.263419971905527e-05, 'epoch': 0.02}
{'loss': 3.403, 'grad_norm': 0.6237202415319111, 'learning_rate': 9.265616596889561e-05, 'epoch': 0.02}
{'loss': 3.4796, 'grad_norm': 0.7337197430920305, 'learning_rate': 9.267809119863771e-05, 'epoch': 0.02}
{'loss': 3.4089, 'grad_norm': 0.6594290815852265, 'learning_rate': 9.26999755611991e-05, 'epoch': 0.02}
{'loss': 3.4255, 'grad_norm': 0.610622288479588, 'learning_rate': 9.272181920864383e-05, 'epoch': 0.02}
{'loss': 3.4257, 'grad_norm': 0.6054516317661403, 'learning_rate': 9.27436222921888e-05, 'epoch': 0.02}
{'loss': 3.3805, 'grad_norm': 0.6134937728495224, 'learning_rate': 9.276538496221008e-05, 'epoch': 0.02}
[2024-04-12 00:39:14,899] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2700 is about to be saved!
[2024-04-12 00:39:14,905] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-2700/global_step2700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 00:39:14,905] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2700/global_step2700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 00:39:14,911] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2700/global_step2700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 00:39:14,911] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2700/global_step2700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 00:39:17,965] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2700/global_step2700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 00:39:17,965] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-2700/global_step2700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 00:39:17,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2700 is ready now!
{'loss': 3.3842, 'grad_norm': 0.6334455162795919, 'learning_rate': 9.278710736824903e-05, 'epoch': 0.02}
{'loss': 3.4034, 'grad_norm': 0.611458388507962, 'learning_rate': 9.280878965901856e-05, 'epoch': 0.02}
{'loss': 3.386, 'grad_norm': 0.6020419774688085, 'learning_rate': 9.283043198240917e-05, 'epoch': 0.02}
{'loss': 3.3793, 'grad_norm': 0.6306848548144512, 'learning_rate': 9.285203448549506e-05, 'epoch': 0.02}
{'loss': 5.9916, 'grad_norm': 2.488771719881303, 'learning_rate': 9.28735973145401e-05, 'epoch': 0.02}
{'loss': 7.0066, 'grad_norm': 3.154839546095694, 'learning_rate': 9.28951206150038e-05, 'epoch': 0.02}
{'loss': 6.8723, 'grad_norm': 2.8955785744610796, 'learning_rate': 9.29166045315471e-05, 'epoch': 0.02}
{'loss': 6.7621, 'grad_norm': 3.1390072374089173, 'learning_rate': 9.293804920803834e-05, 'epoch': 0.02}
{'loss': 6.7131, 'grad_norm': 1.8954584024409304, 'learning_rate': 9.2959454787559e-05, 'epoch': 0.02}
{'loss': 6.6576, 'grad_norm': 1.630585299706707, 'learning_rate': 9.29808214124093e-05, 'epoch': 0.02}
{'loss': 6.4818, 'grad_norm': 2.4859803709539783, 'learning_rate': 9.300214922411409e-05, 'epoch': 0.02}
{'loss': 6.5646, 'grad_norm': 1.2872980926191921, 'learning_rate': 9.30234383634283e-05, 'epoch': 0.02}
{'loss': 6.8389, 'grad_norm': 3.139470684337509, 'learning_rate': 9.30446889703426e-05, 'epoch': 0.02}
{'loss': 6.4549, 'grad_norm': 1.7667252761601826, 'learning_rate': 9.30659011840889e-05, 'epoch': 0.02}
{'loss': 6.3416, 'grad_norm': 1.4868151802518363, 'learning_rate': 9.308707514314584e-05, 'epoch': 0.02}
{'loss': 6.4545, 'grad_norm': 2.3488263795880577, 'learning_rate': 9.310821098524417e-05, 'epoch': 0.02}
{'loss': 6.383, 'grad_norm': 2.1925034089725672, 'learning_rate': 9.312930884737219e-05, 'epoch': 0.02}
{'loss': 6.3242, 'grad_norm': 1.9651130068022435, 'learning_rate': 9.3150368865781e-05, 'epoch': 0.02}
{'loss': 6.2187, 'grad_norm': 1.2548969166712776, 'learning_rate': 9.317139117598988e-05, 'epoch': 0.02}
{'loss': 6.2424, 'grad_norm': 1.6229095111260847, 'learning_rate': 9.31923759127914e-05, 'epoch': 0.02}
[2024-04-12 00:43:56,942] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2800 is about to be saved!
[2024-04-12 00:43:56,948] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-2800/global_step2800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 00:43:56,948] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2800/global_step2800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 00:43:56,955] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2800/global_step2800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 00:43:56,955] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2800/global_step2800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 00:44:00,080] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2800/global_step2800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 00:44:00,080] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-2800/global_step2800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 00:44:00,080] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2800 is ready now!
{'loss': 6.2211, 'grad_norm': 1.2908613801373918, 'learning_rate': 9.321332321025671e-05, 'epoch': 0.02}
{'loss': 6.1683, 'grad_norm': 1.1112982179935456, 'learning_rate': 9.323423320174064e-05, 'epoch': 0.02}
{'loss': 6.1369, 'grad_norm': 7.620936842720426, 'learning_rate': 9.325510601988679e-05, 'epoch': 0.02}
{'loss': 6.0093, 'grad_norm': 1.2223068504395869, 'learning_rate': 9.327594179663253e-05, 'epoch': 0.02}
{'loss': 6.0474, 'grad_norm': 1.3807462826567751, 'learning_rate': 9.329674066321416e-05, 'epoch': 0.02}
{'loss': 5.869, 'grad_norm': 3.268731211713522, 'learning_rate': 9.33175027501717e-05, 'epoch': 0.02}
{'loss': 5.8799, 'grad_norm': 1.1853877856794115, 'learning_rate': 9.333822818735383e-05, 'epoch': 0.02}
{'loss': 6.0039, 'grad_norm': 1.2121879927477857, 'learning_rate': 9.335891710392291e-05, 'epoch': 0.02}
{'loss': 5.904, 'grad_norm': 1.58265830621202, 'learning_rate': 9.337956962835963e-05, 'epoch': 0.02}
{'loss': 5.989, 'grad_norm': 13.290046201681855, 'learning_rate': 9.340018588846787e-05, 'epoch': 0.02}
{'loss': 5.8607, 'grad_norm': 1.2626120900625106, 'learning_rate': 9.342076601137951e-05, 'epoch': 0.03}
{'loss': 5.9863, 'grad_norm': 1.1445454596137137, 'learning_rate': 9.344131012355898e-05, 'epoch': 0.03}
{'loss': 5.9349, 'grad_norm': 0.98452480958519, 'learning_rate': 9.346181835080809e-05, 'epoch': 0.03}
{'loss': 5.8092, 'grad_norm': 1.192717596394986, 'learning_rate': 9.348229081827054e-05, 'epoch': 0.03}
{'loss': 5.9821, 'grad_norm': 1.053561680268361, 'learning_rate': 9.350272765043656e-05, 'epoch': 0.03}
{'loss': 5.8509, 'grad_norm': 1.2394675754212003, 'learning_rate': 9.352312897114738e-05, 'epoch': 0.03}
{'loss': 5.8832, 'grad_norm': 1.1333011025064332, 'learning_rate': 9.354349490359983e-05, 'epoch': 0.03}
{'loss': 5.8706, 'grad_norm': 1.1358303950511812, 'learning_rate': 9.356382557035074e-05, 'epoch': 0.03}
{'loss': 5.8968, 'grad_norm': 1.382164700647034, 'learning_rate': 9.358412109332137e-05, 'epoch': 0.03}
{'loss': 5.7498, 'grad_norm': 1.3208288614486967, 'learning_rate': 9.360438159380185e-05, 'epoch': 0.03}
[2024-04-12 00:48:39,135] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2900 is about to be saved!
[2024-04-12 00:48:39,141] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-2900/global_step2900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 00:48:39,141] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2900/global_step2900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 00:48:39,147] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2900/global_step2900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 00:48:39,147] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2900/global_step2900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 00:48:42,229] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2900/global_step2900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 00:48:42,229] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-2900/global_step2900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 00:48:42,230] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2900 is ready now!
{'loss': 5.8498, 'grad_norm': 1.0048894606579326, 'learning_rate': 9.362460719245542e-05, 'epoch': 0.03}
{'loss': 5.6086, 'grad_norm': 2.4113681538849705, 'learning_rate': 9.364479800932289e-05, 'epoch': 0.03}
{'loss': 5.6659, 'grad_norm': 1.2254596859743652, 'learning_rate': 9.366495416382676e-05, 'epoch': 0.03}
{'loss': 5.7678, 'grad_norm': 1.2575997447016243, 'learning_rate': 9.368507577477557e-05, 'epoch': 0.03}
{'loss': 5.7319, 'grad_norm': 1.3045511732867197, 'learning_rate': 9.370516296036804e-05, 'epoch': 0.03}
{'loss': 5.6842, 'grad_norm': 1.2664547815870464, 'learning_rate': 9.372521583819729e-05, 'epoch': 0.03}
{'loss': 5.4023, 'grad_norm': 1.3008552299722091, 'learning_rate': 9.374523452525492e-05, 'epoch': 0.03}
{'loss': 5.8268, 'grad_norm': 1.3024191001324943, 'learning_rate': 9.376521913793514e-05, 'epoch': 0.03}
{'loss': 5.7606, 'grad_norm': 1.028143491485932, 'learning_rate': 9.37851697920388e-05, 'epoch': 0.03}
{'loss': 5.4885, 'grad_norm': 2.102963440902037, 'learning_rate': 9.380508660277744e-05, 'epoch': 0.03}
{'loss': 5.4486, 'grad_norm': 1.2080808969240444, 'learning_rate': 9.382496968477727e-05, 'epoch': 0.03}
{'loss': 5.7203, 'grad_norm': 1.078154652479756, 'learning_rate': 9.384481915208312e-05, 'epoch': 0.03}
{'loss': 5.6658, 'grad_norm': 4.678957384953905, 'learning_rate': 9.386463511816243e-05, 'epoch': 0.03}
{'loss': 5.4816, 'grad_norm': 1.1430591897224043, 'learning_rate': 9.388441769590903e-05, 'epoch': 0.03}
{'loss': 5.5984, 'grad_norm': 0.9517646011466057, 'learning_rate': 9.390416699764708e-05, 'epoch': 0.03}
{'loss': 5.4625, 'grad_norm': 1.2679266109696303, 'learning_rate': 9.392388313513486e-05, 'epoch': 0.03}
{'loss': 5.6605, 'grad_norm': 0.981942983542754, 'learning_rate': 9.394356621956862e-05, 'epoch': 0.03}
{'loss': 5.709, 'grad_norm': 0.8889362901787844, 'learning_rate': 9.396321636158625e-05, 'epoch': 0.03}
{'loss': 5.7189, 'grad_norm': 1.4436416582197173, 'learning_rate': 9.398283367127109e-05, 'epoch': 0.03}
{'loss': 5.7185, 'grad_norm': 1.174359862037274, 'learning_rate': 9.400241825815563e-05, 'epoch': 0.03}
[2024-04-12 00:53:21,291] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3000 is about to be saved!
[2024-04-12 00:53:21,297] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-3000/global_step3000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 00:53:21,297] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3000/global_step3000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 00:53:21,303] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3000/global_step3000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 00:53:21,303] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3000/global_step3000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 00:53:24,297] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3000/global_step3000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 00:53:24,297] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-3000/global_step3000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 00:53:24,298] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
{'loss': 5.3854, 'grad_norm': 1.3669481674420099, 'learning_rate': 9.402197023122511e-05, 'epoch': 0.03}
{'loss': 5.5677, 'grad_norm': 1.1947443635986454, 'learning_rate': 9.404148969892124e-05, 'epoch': 0.03}
{'loss': 5.6002, 'grad_norm': 1.2321333742525935, 'learning_rate': 9.406097676914569e-05, 'epoch': 0.03}
{'loss': 5.683, 'grad_norm': 0.939622246300157, 'learning_rate': 9.408043154926386e-05, 'epoch': 0.03}
{'loss': 5.406, 'grad_norm': 0.8795371519415511, 'learning_rate': 9.409985414610825e-05, 'epoch': 0.03}
{'loss': 5.4732, 'grad_norm': 1.1899227416576998, 'learning_rate': 9.411924466598205e-05, 'epoch': 0.03}
{'loss': 5.456, 'grad_norm': 0.85488139675911, 'learning_rate': 9.41386032146627e-05, 'epoch': 0.03}
{'loss': 5.4529, 'grad_norm': 1.1246121676100866, 'learning_rate': 9.415792989740516e-05, 'epoch': 0.03}
{'loss': 5.6049, 'grad_norm': 1.0334209933252378, 'learning_rate': 9.417722481894558e-05, 'epoch': 0.03}
{'loss': 5.5809, 'grad_norm': 0.9082139640334183, 'learning_rate': 9.419648808350454e-05, 'epoch': 0.03}
{'loss': 5.4562, 'grad_norm': 1.2045512706403452, 'learning_rate': 9.421571979479048e-05, 'epoch': 0.03}
{'loss': 5.5108, 'grad_norm': 0.8995790542039123, 'learning_rate': 9.423492005600304e-05, 'epoch': 0.03}
{'loss': 5.5258, 'grad_norm': 0.8674969579776652, 'learning_rate': 9.425408896983643e-05, 'epoch': 0.03}
{'loss': 5.4493, 'grad_norm': 0.9046638988397248, 'learning_rate': 9.427322663848265e-05, 'epoch': 0.03}
{'loss': 5.4889, 'grad_norm': 0.9070879347760638, 'learning_rate': 9.429233316363481e-05, 'epoch': 0.03}
{'loss': 5.3162, 'grad_norm': 0.8668885492115305, 'learning_rate': 9.431140864649033e-05, 'epoch': 0.03}
{'loss': 5.4615, 'grad_norm': 0.8820325127613258, 'learning_rate': 9.433045318775428e-05, 'epoch': 0.03}
{'loss': 5.3818, 'grad_norm': 1.1519741642038053, 'learning_rate': 9.434946688764234e-05, 'epoch': 0.03}
{'loss': 5.3907, 'grad_norm': 1.019832318171528, 'learning_rate': 9.43684498458842e-05, 'epoch': 0.03}
{'loss': 5.4933, 'grad_norm': 0.9840253936296388, 'learning_rate': 9.438740216172656e-05, 'epoch': 0.03}
[2024-04-12 00:58:03,330] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3100 is about to be saved!
[2024-04-12 00:58:03,336] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-3100/global_step3100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 00:58:03,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3100/global_step3100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 00:58:03,342] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3100/global_step3100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 00:58:03,343] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3100/global_step3100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 00:58:06,435] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3100/global_step3100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 00:58:06,436] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-3100/global_step3100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 00:58:06,436] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3100 is ready now!
{'loss': 5.4934, 'grad_norm': 1.0321806146773362, 'learning_rate': 9.440632393393627e-05, 'epoch': 0.03}
{'loss': 5.5066, 'grad_norm': 0.897426292016863, 'learning_rate': 9.442521526080346e-05, 'epoch': 0.03}
{'loss': 4.8352, 'grad_norm': 1.3518154968531915, 'learning_rate': 9.444407624014455e-05, 'epoch': 0.03}
{'loss': 5.3821, 'grad_norm': 6.76629922837004, 'learning_rate': 9.446290696930533e-05, 'epoch': 0.03}
{'loss': 4.144, 'grad_norm': 1.4183578694389205, 'learning_rate': 9.44817075451639e-05, 'epoch': 0.03}
{'loss': 5.5079, 'grad_norm': 0.9844837174478079, 'learning_rate': 9.450047806413379e-05, 'epoch': 0.03}
{'loss': 5.2836, 'grad_norm': 1.1180161565895028, 'learning_rate': 9.451921862216682e-05, 'epoch': 0.03}
{'loss': 5.3451, 'grad_norm': 0.9533688759847826, 'learning_rate': 9.453792931475607e-05, 'epoch': 0.03}
{'loss': 5.3325, 'grad_norm': 0.9735485100082772, 'learning_rate': 9.455661023693879e-05, 'epoch': 0.03}
{'loss': 5.3689, 'grad_norm': 1.4195613035435708, 'learning_rate': 9.457526148329938e-05, 'epoch': 0.03}
{'loss': 5.4579, 'grad_norm': 0.9111460849925742, 'learning_rate': 9.459388314797212e-05, 'epoch': 0.03}
{'loss': 5.3449, 'grad_norm': 1.0516412763649836, 'learning_rate': 9.461247532464413e-05, 'epoch': 0.03}
{'loss': 5.3475, 'grad_norm': 1.0325940001479101, 'learning_rate': 9.463103810655815e-05, 'epoch': 0.03}
{'loss': 5.2336, 'grad_norm': 1.7715261360313073, 'learning_rate': 9.464957158651539e-05, 'epoch': 0.03}
{'loss': 5.2827, 'grad_norm': 0.9720662002154709, 'learning_rate': 9.466807585687823e-05, 'epoch': 0.03}
{'loss': 5.2923, 'grad_norm': 0.909729978786143, 'learning_rate': 9.468655100957309e-05, 'epoch': 0.03}
{'loss': 5.1866, 'grad_norm': 0.8869397765728207, 'learning_rate': 9.47049971360931e-05, 'epoch': 0.03}
{'loss': 5.3004, 'grad_norm': 0.8847394950583246, 'learning_rate': 9.472341432750078e-05, 'epoch': 0.03}
{'loss': 5.1575, 'grad_norm': 0.9777643218388191, 'learning_rate': 9.47418026744309e-05, 'epoch': 0.03}
{'loss': 5.3853, 'grad_norm': 1.1078873205731772, 'learning_rate': 9.476016226709293e-05, 'epoch': 0.03}
[2024-04-12 01:02:45,497] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3200 is about to be saved!
[2024-04-12 01:02:45,504] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-3200/global_step3200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 01:02:45,504] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3200/global_step3200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 01:02:45,510] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3200/global_step3200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 01:02:45,511] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3200/global_step3200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 01:02:48,571] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3200/global_step3200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 01:02:48,571] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-3200/global_step3200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 01:02:48,572] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3200 is ready now!
{'loss': 5.2575, 'grad_norm': 0.9425822630366125, 'learning_rate': 9.477849319527389e-05, 'epoch': 0.03}
{'loss': 5.3026, 'grad_norm': 1.5668210376992935, 'learning_rate': 9.479679554834087e-05, 'epoch': 0.03}
{'loss': 5.2592, 'grad_norm': 0.9002075369549315, 'learning_rate': 9.481506941524376e-05, 'epoch': 0.03}
{'loss': 5.1805, 'grad_norm': 0.860611147118279, 'learning_rate': 9.483331488451758e-05, 'epoch': 0.03}
{'loss': 5.2269, 'grad_norm': 0.9009822934192772, 'learning_rate': 9.485153204428545e-05, 'epoch': 0.03}
{'loss': 5.3255, 'grad_norm': 0.9944597046350414, 'learning_rate': 9.486972098226083e-05, 'epoch': 0.03}
{'loss': 5.2521, 'grad_norm': 1.3617583689305683, 'learning_rate': 9.488788178575016e-05, 'epoch': 0.03}
{'loss': 4.9353, 'grad_norm': 0.9294895637026183, 'learning_rate': 9.490601454165536e-05, 'epoch': 0.03}
{'loss': 5.3812, 'grad_norm': 1.0406160347933369, 'learning_rate': 9.492411933647638e-05, 'epoch': 0.03}
{'loss': 5.2584, 'grad_norm': 0.8612063372780451, 'learning_rate': 9.494219625631358e-05, 'epoch': 0.03}
{'loss': 5.1771, 'grad_norm': 0.9712325423406065, 'learning_rate': 9.49602453868703e-05, 'epoch': 0.03}
{'loss': 5.3558, 'grad_norm': 0.9330754018492815, 'learning_rate': 9.497826681345518e-05, 'epoch': 0.03}
{'loss': 5.0319, 'grad_norm': 0.9157093604352241, 'learning_rate': 9.499626062098467e-05, 'epoch': 0.03}
{'loss': 5.2099, 'grad_norm': 1.0421219350320865, 'learning_rate': 9.501422689398538e-05, 'epoch': 0.03}
{'loss': 5.1881, 'grad_norm': 0.8517322648462106, 'learning_rate': 9.503216571659649e-05, 'epoch': 0.03}
{'loss': 5.2126, 'grad_norm': 0.8662769461548323, 'learning_rate': 9.505007717257209e-05, 'epoch': 0.03}
{'loss': 4.8671, 'grad_norm': 1.083086883501122, 'learning_rate': 9.506796134528355e-05, 'epoch': 0.03}
{'loss': 5.1757, 'grad_norm': 0.87082880351678, 'learning_rate': 9.508581831772183e-05, 'epoch': 0.03}
{'loss': 5.1133, 'grad_norm': 0.9349525273314125, 'learning_rate': 9.51036481724998e-05, 'epoch': 0.03}
{'loss': 5.1231, 'grad_norm': 0.7605071101381866, 'learning_rate': 9.512145099185458e-05, 'epoch': 0.03}
[2024-04-12 01:07:27,310] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3300 is about to be saved!
[2024-04-12 01:07:27,316] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-3300/global_step3300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 01:07:27,316] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3300/global_step3300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 01:07:27,323] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3300/global_step3300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 01:07:27,323] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3300/global_step3300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 01:07:30,390] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3300/global_step3300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 01:07:30,390] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-3300/global_step3300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 01:07:30,391] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3300 is ready now!
{'loss': 5.1359, 'grad_norm': 1.0774216010788047, 'learning_rate': 9.513922685764966e-05, 'epoch': 0.03}
{'loss': 5.2157, 'grad_norm': 0.8525766593492758, 'learning_rate': 9.515697585137739e-05, 'epoch': 0.03}
{'loss': 5.1042, 'grad_norm': 0.9887985890228663, 'learning_rate': 9.517469805416098e-05, 'epoch': 0.03}
{'loss': 5.276, 'grad_norm': 0.8609444329844971, 'learning_rate': 9.519239354675697e-05, 'epoch': 0.03}
{'loss': 5.2505, 'grad_norm': 0.9438350536932267, 'learning_rate': 9.521006240955718e-05, 'epoch': 0.03}
{'loss': 5.1885, 'grad_norm': 0.9124776142830996, 'learning_rate': 9.52277047225911e-05, 'epoch': 0.03}
{'loss': 4.9779, 'grad_norm': 0.9077648091744916, 'learning_rate': 9.524532056552803e-05, 'epoch': 0.03}
{'loss': 4.7903, 'grad_norm': 1.335047824224196, 'learning_rate': 9.526291001767917e-05, 'epoch': 0.03}
{'loss': 5.2436, 'grad_norm': 0.8835529909464159, 'learning_rate': 9.528047315799973e-05, 'epoch': 0.03}
{'loss': 5.1229, 'grad_norm': 2.8761479228000604, 'learning_rate': 9.529801006509122e-05, 'epoch': 0.03}
{'loss': 4.9708, 'grad_norm': 0.956440038560933, 'learning_rate': 9.531552081720348e-05, 'epoch': 0.03}
{'loss': 5.1251, 'grad_norm': 0.8505615636923329, 'learning_rate': 9.533300549223666e-05, 'epoch': 0.03}
{'loss': 5.0475, 'grad_norm': 0.8472448030261196, 'learning_rate': 9.53504641677435e-05, 'epoch': 0.03}
{'loss': 5.1538, 'grad_norm': 0.8081138424078868, 'learning_rate': 9.536789692093117e-05, 'epoch': 0.03}
{'loss': 5.174, 'grad_norm': 0.8193812949968245, 'learning_rate': 9.538530382866361e-05, 'epoch': 0.03}
{'loss': 5.1161, 'grad_norm': 0.9576886045252977, 'learning_rate': 9.540268496746328e-05, 'epoch': 0.03}
{'loss': 4.9225, 'grad_norm': 0.8572427569471242, 'learning_rate': 9.542004041351328e-05, 'epoch': 0.03}
{'loss': 5.0145, 'grad_norm': 0.9086480627831092, 'learning_rate': 9.543737024265944e-05, 'epoch': 0.03}
{'loss': 4.8131, 'grad_norm': 0.8829671490611556, 'learning_rate': 9.545467453041218e-05, 'epoch': 0.03}
{'loss': 5.102, 'grad_norm': 0.8321730425929026, 'learning_rate': 9.54719533519486e-05, 'epoch': 0.03}
[2024-04-12 01:12:09,177] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3400 is about to be saved!
[2024-04-12 01:12:09,183] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-3400/global_step3400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 01:12:09,183] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3400/global_step3400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 01:12:09,189] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3400/global_step3400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 01:12:09,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3400/global_step3400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 01:12:12,305] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3400/global_step3400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 01:12:12,305] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-3400/global_step3400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 01:12:12,306] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3400 is ready now!
{'loss': 5.0404, 'grad_norm': 1.0745764385954961, 'learning_rate': 9.54892067821143e-05, 'epoch': 0.03}
{'loss': 5.0291, 'grad_norm': 1.022635206345417, 'learning_rate': 9.55064348954255e-05, 'epoch': 0.03}
{'loss': 5.0528, 'grad_norm': 0.8350774826257006, 'learning_rate': 9.552363776607079e-05, 'epoch': 0.03}
{'loss': 5.0211, 'grad_norm': 0.8215517020425516, 'learning_rate': 9.554081546791315e-05, 'epoch': 0.03}
{'loss': 4.7623, 'grad_norm': 1.4083365456312233, 'learning_rate': 9.555796807449189e-05, 'epoch': 0.03}
{'loss': 5.1104, 'grad_norm': 0.8318367075530959, 'learning_rate': 9.557509565902442e-05, 'epoch': 0.03}
{'loss': 4.9137, 'grad_norm': 0.9628840336262926, 'learning_rate': 9.559219829440827e-05, 'epoch': 0.03}
{'loss': 5.167, 'grad_norm': 0.8208015465511633, 'learning_rate': 9.560927605322276e-05, 'epoch': 0.03}
{'loss': 4.9144, 'grad_norm': 0.9448185382321203, 'learning_rate': 9.562632900773103e-05, 'epoch': 0.03}
{'loss': 5.1577, 'grad_norm': 0.8012904449297724, 'learning_rate': 9.564335722988183e-05, 'epoch': 0.03}
{'loss': 4.8565, 'grad_norm': 0.8068772721273488, 'learning_rate': 9.566036079131124e-05, 'epoch': 0.03}
{'loss': 5.0831, 'grad_norm': 0.8906322319575243, 'learning_rate': 9.567733976334464e-05, 'epoch': 0.03}
{'loss': 5.1966, 'grad_norm': 0.8851251864991642, 'learning_rate': 9.569429421699832e-05, 'epoch': 0.03}
{'loss': 4.9794, 'grad_norm': 0.9899590812433151, 'learning_rate': 9.571122422298146e-05, 'epoch': 0.03}
{'loss': 5.07, 'grad_norm': 0.8205716307648313, 'learning_rate': 9.572812985169772e-05, 'epoch': 0.03}
{'loss': 4.9732, 'grad_norm': 0.8520767110218468, 'learning_rate': 9.574501117324711e-05, 'epoch': 0.03}
{'loss': 4.9176, 'grad_norm': 0.8815792763577511, 'learning_rate': 9.576186825742774e-05, 'epoch': 0.03}
{'loss': 5.0341, 'grad_norm': 1.01277627335456, 'learning_rate': 9.577870117373746e-05, 'epoch': 0.03}
{'loss': 4.942, 'grad_norm': 0.8641801802895274, 'learning_rate': 9.579550999137567e-05, 'epoch': 0.03}
{'loss': 4.8973, 'grad_norm': 0.8894926090930484, 'learning_rate': 9.581229477924493e-05, 'epoch': 0.03}
[2024-04-12 01:16:51,018] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3500 is about to be saved!
[2024-04-12 01:16:51,024] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-3500/global_step3500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 01:16:51,024] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3500/global_step3500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 01:16:51,030] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3500/global_step3500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 01:16:51,031] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3500/global_step3500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 01:16:54,132] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3500/global_step3500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 01:16:54,132] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-3500/global_step3500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 01:16:54,132] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3500 is ready now!
{'loss': 5.0676, 'grad_norm': 0.848547179507126, 'learning_rate': 9.582905560595276e-05, 'epoch': 0.03}
{'loss': 4.9383, 'grad_norm': 1.1731979453841692, 'learning_rate': 9.584579253981332e-05, 'epoch': 0.03}
{'loss': 5.0791, 'grad_norm': 0.810331233828947, 'learning_rate': 9.586250564884891e-05, 'epoch': 0.03}
{'loss': 4.8796, 'grad_norm': 1.0741850893328015, 'learning_rate': 9.587919500079186e-05, 'epoch': 0.03}
{'loss': 4.8755, 'grad_norm': 5.024473650159127, 'learning_rate': 9.589586066308606e-05, 'epoch': 0.03}
{'loss': 4.8391, 'grad_norm': 1.2243109339134204, 'learning_rate': 9.59125027028886e-05, 'epoch': 0.03}
{'loss': 4.7899, 'grad_norm': 0.8316751485059576, 'learning_rate': 9.592912118707136e-05, 'epoch': 0.03}
{'loss': 5.0554, 'grad_norm': 1.2581991164542872, 'learning_rate': 9.594571618222272e-05, 'epoch': 0.03}
{'loss': 4.843, 'grad_norm': 2.63569423633515, 'learning_rate': 9.596228775464906e-05, 'epoch': 0.03}
{'loss': 4.803, 'grad_norm': 0.8720377942820109, 'learning_rate': 9.597883597037644e-05, 'epoch': 0.03}
{'loss': 4.8981, 'grad_norm': 0.8995268006337754, 'learning_rate': 9.599536089515212e-05, 'epoch': 0.03}
{'loss': 4.9808, 'grad_norm': 0.7836970440302754, 'learning_rate': 9.601186259444608e-05, 'epoch': 0.03}
{'loss': 4.6859, 'grad_norm': 0.8966986619625079, 'learning_rate': 9.602834113345275e-05, 'epoch': 0.03}
{'loss': 5.0455, 'grad_norm': 0.9142840612288695, 'learning_rate': 9.604479657709234e-05, 'epoch': 0.03}
{'loss': 5.0064, 'grad_norm': 0.7749544048768621, 'learning_rate': 9.606122899001251e-05, 'epoch': 0.03}
{'loss': 4.8633, 'grad_norm': 1.0779288567202059, 'learning_rate': 9.60776384365899e-05, 'epoch': 0.03}
{'loss': 4.9005, 'grad_norm': 0.862918042450401, 'learning_rate': 9.609402498093155e-05, 'epoch': 0.03}
{'loss': 4.9458, 'grad_norm': 0.7918800033086905, 'learning_rate': 9.611038868687647e-05, 'epoch': 0.03}
{'loss': 4.8546, 'grad_norm': 0.9730899142065305, 'learning_rate': 9.612672961799713e-05, 'epoch': 0.03}
{'loss': 4.8206, 'grad_norm': 0.793566665846062, 'learning_rate': 9.614304783760091e-05, 'epoch': 0.03}
[2024-04-12 01:21:32,732] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3600 is about to be saved!
[2024-04-12 01:21:32,738] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-3600/global_step3600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 01:21:32,738] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3600/global_step3600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 01:21:32,744] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3600/global_step3600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 01:21:32,744] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3600/global_step3600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 01:21:35,861] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3600/global_step3600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 01:21:35,861] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-3600/global_step3600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 01:21:35,862] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3600 is ready now!
{'loss': 4.8851, 'grad_norm': 0.8248472637208324, 'learning_rate': 9.615934340873163e-05, 'epoch': 0.03}
{'loss': 4.9254, 'grad_norm': 0.8278368946094788, 'learning_rate': 9.617561639417095e-05, 'epoch': 0.03}
{'loss': 5.0353, 'grad_norm': 0.8778215373751915, 'learning_rate': 9.619186685643982e-05, 'epoch': 0.03}
{'loss': 4.8204, 'grad_norm': 0.9969042492914971, 'learning_rate': 9.62080948578e-05, 'epoch': 0.03}
{'loss': 5.0404, 'grad_norm': 0.7927952340460519, 'learning_rate': 9.622430046025538e-05, 'epoch': 0.03}
{'loss': 4.908, 'grad_norm': 1.0490806521195957, 'learning_rate': 9.624048372555352e-05, 'epoch': 0.03}
{'loss': 4.9964, 'grad_norm': 0.7698426701683183, 'learning_rate': 9.625664471518695e-05, 'epoch': 0.03}
{'loss': 4.9593, 'grad_norm': 0.7802889091250024, 'learning_rate': 9.627278349039461e-05, 'epoch': 0.03}
{'loss': 5.0154, 'grad_norm': 0.8833286280396268, 'learning_rate': 9.628890011216334e-05, 'epoch': 0.03}
{'loss': 4.9629, 'grad_norm': 0.7592272189389468, 'learning_rate': 9.63049946412291e-05, 'epoch': 0.03}
{'loss': 4.8769, 'grad_norm': 0.9865657201184691, 'learning_rate': 9.632106713807842e-05, 'epoch': 0.03}
{'loss': 4.7742, 'grad_norm': 0.8127314600136998, 'learning_rate': 9.633711766294981e-05, 'epoch': 0.03}
{'loss': 4.9798, 'grad_norm': 0.758601274566568, 'learning_rate': 9.635314627583506e-05, 'epoch': 0.03}
{'loss': 4.8803, 'grad_norm': 0.7667440196963243, 'learning_rate': 9.636915303648055e-05, 'epoch': 0.03}
{'loss': 4.7228, 'grad_norm': 0.7962900943123172, 'learning_rate': 9.638513800438867e-05, 'epoch': 0.03}
{'loss': 4.8639, 'grad_norm': 0.8500575372103129, 'learning_rate': 9.640110123881911e-05, 'epoch': 0.03}
{'loss': 4.7019, 'grad_norm': 0.8630202898028281, 'learning_rate': 9.641704279879017e-05, 'epoch': 0.03}
{'loss': 5.0201, 'grad_norm': 0.8542609590977612, 'learning_rate': 9.643296274308006e-05, 'epoch': 0.03}
{'loss': 4.8533, 'grad_norm': 0.8531312468057495, 'learning_rate': 9.644886113022825e-05, 'epoch': 0.03}
{'loss': 4.3389, 'grad_norm': 1.8120074116001343, 'learning_rate': 9.646473801853665e-05, 'epoch': 0.03}
[2024-04-12 01:26:14,672] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3700 is about to be saved!
[2024-04-12 01:26:14,678] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-3700/global_step3700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 01:26:14,679] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3700/global_step3700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 01:26:14,685] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3700/global_step3700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 01:26:14,685] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3700/global_step3700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 01:26:17,761] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3700/global_step3700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 01:26:17,762] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-3700/global_step3700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 01:26:17,762] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3700 is ready now!
{'loss': 4.8448, 'grad_norm': 0.9298465629987295, 'learning_rate': 9.648059346607108e-05, 'epoch': 0.03}
{'loss': 4.9575, 'grad_norm': 0.9410597627705478, 'learning_rate': 9.649642753066238e-05, 'epoch': 0.03}
{'loss': 5.0263, 'grad_norm': 0.825440030085022, 'learning_rate': 9.651224026990769e-05, 'epoch': 0.03}
{'loss': 4.7729, 'grad_norm': 0.915513761490106, 'learning_rate': 9.652803174117183e-05, 'epoch': 0.03}
{'loss': 4.5196, 'grad_norm': 0.867926437889912, 'learning_rate': 9.654380200158838e-05, 'epoch': 0.03}
{'loss': 4.8644, 'grad_norm': 0.8515115229064114, 'learning_rate': 9.655955110806109e-05, 'epoch': 0.03}
{'loss': 4.7723, 'grad_norm': 0.7952732199127432, 'learning_rate': 9.657527911726495e-05, 'epoch': 0.03}
{'loss': 4.8801, 'grad_norm': 0.8508879982432208, 'learning_rate': 9.659098608564754e-05, 'epoch': 0.03}
{'loss': 4.8676, 'grad_norm': 0.8108819249098892, 'learning_rate': 9.660667206943018e-05, 'epoch': 0.03}
{'loss': 4.9467, 'grad_norm': 5.344593648450456, 'learning_rate': 9.662233712460918e-05, 'epoch': 0.03}
{'loss': 4.7933, 'grad_norm': 0.8462021803582818, 'learning_rate': 9.663798130695696e-05, 'epoch': 0.03}
{'loss': 4.9088, 'grad_norm': 0.7845512890747801, 'learning_rate': 9.665360467202336e-05, 'epoch': 0.03}
{'loss': 4.7564, 'grad_norm': 0.8994860608102823, 'learning_rate': 9.66692072751367e-05, 'epoch': 0.03}
{'loss': 4.8895, 'grad_norm': 0.752691031703534, 'learning_rate': 9.668478917140505e-05, 'epoch': 0.03}
{'loss': 4.734, 'grad_norm': 0.8978171378848848, 'learning_rate': 9.670035041571738e-05, 'epoch': 0.03}
{'loss': 4.7476, 'grad_norm': 0.740583111428602, 'learning_rate': 9.671589106274465e-05, 'epoch': 0.03}
{'loss': 4.9086, 'grad_norm': 1.0241279265850778, 'learning_rate': 9.673141116694105e-05, 'epoch': 0.03}
{'loss': 4.6468, 'grad_norm': 0.8364532834282107, 'learning_rate': 9.674691078254508e-05, 'epoch': 0.03}
{'loss': 4.8671, 'grad_norm': 0.7706425989287439, 'learning_rate': 9.676238996358077e-05, 'epoch': 0.03}
{'loss': 4.7548, 'grad_norm': 0.7320312200254333, 'learning_rate': 9.67778487638587e-05, 'epoch': 0.03}
[2024-04-12 01:30:56,578] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3800 is about to be saved!
[2024-04-12 01:30:56,584] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-3800/global_step3800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 01:30:56,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3800/global_step3800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 01:30:56,590] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3800/global_step3800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 01:30:56,590] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3800/global_step3800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 01:30:59,707] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3800/global_step3800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 01:30:59,707] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-3800/global_step3800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 01:30:59,708] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3800 is ready now!
{'loss': 4.5976, 'grad_norm': 1.2528584175766238, 'learning_rate': 9.679328723697721e-05, 'epoch': 0.03}
{'loss': 4.7625, 'grad_norm': 0.9702113657040538, 'learning_rate': 9.68087054363235e-05, 'epoch': 0.03}
{'loss': 4.7775, 'grad_norm': 0.8632264983847272, 'learning_rate': 9.682410341507466e-05, 'epoch': 0.03}
{'loss': 4.7063, 'grad_norm': 0.7954923163589098, 'learning_rate': 9.683948122619892e-05, 'epoch': 0.03}
{'loss': 4.6994, 'grad_norm': 0.8156729690871066, 'learning_rate': 9.685483892245659e-05, 'epoch': 0.03}
{'loss': 4.7397, 'grad_norm': 0.7859046292282944, 'learning_rate': 9.68701765564012e-05, 'epoch': 0.03}
{'loss': 4.8818, 'grad_norm': 0.8537969434668309, 'learning_rate': 9.688549418038065e-05, 'epoch': 0.03}
{'loss': 4.7915, 'grad_norm': 0.9199292973702761, 'learning_rate': 9.690079184653818e-05, 'epoch': 0.03}
{'loss': 4.8255, 'grad_norm': 0.8827687140136102, 'learning_rate': 9.691606960681354e-05, 'epoch': 0.03}
{'loss': 4.8498, 'grad_norm': 0.8057011133980188, 'learning_rate': 9.693132751294389e-05, 'epoch': 0.03}
{'loss': 4.7138, 'grad_norm': 0.8543855650699763, 'learning_rate': 9.694656561646496e-05, 'epoch': 0.03}
{'loss': 4.6462, 'grad_norm': 0.7673639588297698, 'learning_rate': 9.69617839687122e-05, 'epoch': 0.03}
{'loss': 4.6476, 'grad_norm': 0.8061158243324369, 'learning_rate': 9.697698262082157e-05, 'epoch': 0.03}
{'loss': 4.8828, 'grad_norm': 0.8241676573543868, 'learning_rate': 9.699216162373072e-05, 'epoch': 0.03}
{'loss': 4.7241, 'grad_norm': 0.7423313707651028, 'learning_rate': 9.70073210281801e-05, 'epoch': 0.03}
{'loss': 4.8195, 'grad_norm': 0.7793849069901967, 'learning_rate': 9.70224608847137e-05, 'epoch': 0.03}
{'loss': 4.9197, 'grad_norm': 0.7494803410687044, 'learning_rate': 9.703758124368041e-05, 'epoch': 0.03}
{'loss': 4.6274, 'grad_norm': 0.885464523301009, 'learning_rate': 9.70526821552347e-05, 'epoch': 0.03}
{'loss': 4.714, 'grad_norm': 0.9431551393629545, 'learning_rate': 9.706776366933785e-05, 'epoch': 0.03}
{'loss': 4.8391, 'grad_norm': 0.7911500982569937, 'learning_rate': 9.708282583575884e-05, 'epoch': 0.03}
[2024-04-12 01:35:38,403] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3900 is about to be saved!
[2024-04-12 01:35:38,408] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-3900/global_step3900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 01:35:38,408] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3900/global_step3900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 01:35:38,415] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3900/global_step3900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 01:35:38,415] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-3900/global_step3900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 01:35:41,507] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-3900/global_step3900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 01:35:41,507] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-3900/global_step3900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 01:35:41,508] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3900 is ready now!
{'loss': 4.4358, 'grad_norm': 0.784412922189052, 'learning_rate': 9.709786870407539e-05, 'epoch': 0.03}
{'loss': 4.5271, 'grad_norm': 0.7769934553438362, 'learning_rate': 9.711289232367479e-05, 'epoch': 0.03}
{'loss': 4.7704, 'grad_norm': 0.8454397376186218, 'learning_rate': 9.712789674375509e-05, 'epoch': 0.03}
{'loss': 4.59, 'grad_norm': 0.86624132722727, 'learning_rate': 9.714288201332597e-05, 'epoch': 0.03}
{'loss': 4.7021, 'grad_norm': 1.009106035772307, 'learning_rate': 9.715784818120958e-05, 'epoch': 0.03}
{'loss': 4.3544, 'grad_norm': 0.8498520196993943, 'learning_rate': 9.717279529604176e-05, 'epoch': 0.03}
{'loss': 4.7438, 'grad_norm': 0.7808810344029508, 'learning_rate': 9.718772340627269e-05, 'epoch': 0.03}
{'loss': 4.7094, 'grad_norm': 0.857802021261866, 'learning_rate': 9.72026325601681e-05, 'epoch': 0.03}
{'loss': 4.7467, 'grad_norm': 0.844246873919703, 'learning_rate': 9.721752280580996e-05, 'epoch': 0.03}
{'loss': 4.6592, 'grad_norm': 0.7868609734662919, 'learning_rate': 9.723239419109766e-05, 'epoch': 0.03}
{'loss': 4.1946, 'grad_norm': 0.9484839119648415, 'learning_rate': 9.724724676374872e-05, 'epoch': 0.03}
{'loss': 4.7246, 'grad_norm': 0.7979625565269242, 'learning_rate': 9.726208057129985e-05, 'epoch': 0.03}
{'loss': 4.6801, 'grad_norm': 0.9462530034054141, 'learning_rate': 9.727689566110776e-05, 'epoch': 0.03}
{'loss': 4.796, 'grad_norm': 1.0318940612684941, 'learning_rate': 9.729169208035017e-05, 'epoch': 0.03}
{'loss': 4.8902, 'grad_norm': 0.7862294267388689, 'learning_rate': 9.730646987602663e-05, 'epoch': 0.03}
{'loss': 4.6057, 'grad_norm': 0.7847297241310903, 'learning_rate': 9.732122909495942e-05, 'epoch': 0.03}
{'loss': 4.1597, 'grad_norm': 1.2323271327302363, 'learning_rate': 9.733596978379453e-05, 'epoch': 0.03}
{'loss': 4.5401, 'grad_norm': 0.877481958048239, 'learning_rate': 9.735069198900246e-05, 'epoch': 0.03}
{'loss': 4.6419, 'grad_norm': 2.4771523645672002, 'learning_rate': 9.736539575687902e-05, 'epoch': 0.03}
{'loss': 4.6744, 'grad_norm': 0.873877182081984, 'learning_rate': 9.738008113354646e-05, 'epoch': 0.04}
[2024-04-12 01:40:20,254] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4000 is about to be saved!
[2024-04-12 01:40:20,260] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-4000/global_step4000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 01:40:20,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4000/global_step4000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 01:40:20,266] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4000/global_step4000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 01:40:20,266] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4000/global_step4000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 01:40:23,429] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4000/global_step4000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 01:40:23,429] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-4000/global_step4000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 01:40:23,429] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
{'loss': 4.735, 'grad_norm': 0.7963090393189605, 'learning_rate': 9.739474816495406e-05, 'epoch': 0.04}
{'loss': 4.4076, 'grad_norm': 0.7750552312691208, 'learning_rate': 9.740939689687919e-05, 'epoch': 0.04}
{'loss': 4.1191, 'grad_norm': 0.8557438087131888, 'learning_rate': 9.742402737492802e-05, 'epoch': 0.04}
{'loss': 4.6563, 'grad_norm': 1.0827064471572023, 'learning_rate': 9.74386396445365e-05, 'epoch': 0.04}
{'loss': 4.7502, 'grad_norm': 1.052447927214719, 'learning_rate': 9.745323375097112e-05, 'epoch': 0.04}
{'loss': 4.633, 'grad_norm': 0.8329851733736261, 'learning_rate': 9.746780973932978e-05, 'epoch': 0.04}
{'loss': 4.7843, 'grad_norm': 1.0265912472266068, 'learning_rate': 9.748236765454262e-05, 'epoch': 0.04}
{'loss': 4.6656, 'grad_norm': 0.818319653890661, 'learning_rate': 9.749690754137288e-05, 'epoch': 0.04}
{'loss': 4.5967, 'grad_norm': 0.7830114027326879, 'learning_rate': 9.751142944441768e-05, 'epoch': 0.04}
{'loss': 4.746, 'grad_norm': 1.0454994776229167, 'learning_rate': 9.752593340810889e-05, 'epoch': 0.04}
{'loss': 4.52, 'grad_norm': 0.9100491220210768, 'learning_rate': 9.754041947671388e-05, 'epoch': 0.04}
{'loss': 4.6051, 'grad_norm': 0.7746655283984437, 'learning_rate': 9.755488769433642e-05, 'epoch': 0.04}
{'loss': 4.6382, 'grad_norm': 0.7512051407276408, 'learning_rate': 9.756933810491737e-05, 'epoch': 0.04}
{'loss': 4.7978, 'grad_norm': 0.7864620900115866, 'learning_rate': 9.75837707522356e-05, 'epoch': 0.04}
{'loss': 4.7064, 'grad_norm': 0.7423594888361092, 'learning_rate': 9.75981856799087e-05, 'epoch': 0.04}
{'loss': 4.7031, 'grad_norm': 0.7891884771191056, 'learning_rate': 9.761258293139387e-05, 'epoch': 0.04}
{'loss': 4.7095, 'grad_norm': 0.7759121282417094, 'learning_rate': 9.762696254998853e-05, 'epoch': 0.04}
{'loss': 4.5859, 'grad_norm': 0.8725589531006994, 'learning_rate': 9.764132457883128e-05, 'epoch': 0.04}
{'loss': 4.6235, 'grad_norm': 0.8138150849576471, 'learning_rate': 9.765566906090259e-05, 'epoch': 0.04}
{'loss': 4.7392, 'grad_norm': 0.7824846604289941, 'learning_rate': 9.766999603902561e-05, 'epoch': 0.04}
[2024-04-12 01:45:02,183] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4100 is about to be saved!
[2024-04-12 01:45:02,188] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-4100/global_step4100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 01:45:02,188] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4100/global_step4100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 01:45:02,195] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4100/global_step4100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 01:45:02,195] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4100/global_step4100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 01:45:05,339] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4100/global_step4100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 01:45:05,339] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-4100/global_step4100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 01:45:05,339] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4100 is ready now!
{'loss': 4.4999, 'grad_norm': 0.8006797785745365, 'learning_rate': 9.76843055558669e-05, 'epoch': 0.04}
{'loss': 4.5464, 'grad_norm': 0.9062692809954304, 'learning_rate': 9.769859765393716e-05, 'epoch': 0.04}
{'loss': 4.6246, 'grad_norm': 0.7713171052027059, 'learning_rate': 9.771287237559211e-05, 'epoch': 0.04}
{'loss': 4.6412, 'grad_norm': 0.776215823128374, 'learning_rate': 9.772712976303316e-05, 'epoch': 0.04}
{'loss': 4.6041, 'grad_norm': 0.8336518294204108, 'learning_rate': 9.774136985830812e-05, 'epoch': 0.04}
{'loss': 4.7343, 'grad_norm': 0.7436303551226957, 'learning_rate': 9.775559270331201e-05, 'epoch': 0.04}
{'loss': 4.6437, 'grad_norm': 0.8528314583341792, 'learning_rate': 9.77697983397878e-05, 'epoch': 0.04}
{'loss': 4.4526, 'grad_norm': 0.8105133629639394, 'learning_rate': 9.77839868093271e-05, 'epoch': 0.04}
{'loss': 4.6502, 'grad_norm': 0.8037438150530934, 'learning_rate': 9.779815815337096e-05, 'epoch': 0.04}
{'loss': 4.6615, 'grad_norm': 0.8385795193232757, 'learning_rate': 9.78123124132105e-05, 'epoch': 0.04}
{'loss': 4.4636, 'grad_norm': 0.8784212912665763, 'learning_rate': 9.782644962998771e-05, 'epoch': 0.04}
{'loss': 4.5526, 'grad_norm': 0.8472701964844306, 'learning_rate': 9.784056984469614e-05, 'epoch': 0.04}
{'loss': 4.6955, 'grad_norm': 0.7783688825840982, 'learning_rate': 9.785467309818163e-05, 'epoch': 0.04}
{'loss': 4.467, 'grad_norm': 0.7837926771774095, 'learning_rate': 9.786875943114299e-05, 'epoch': 0.04}
{'loss': 4.5444, 'grad_norm': 0.9012256829242219, 'learning_rate': 9.788282888413269e-05, 'epoch': 0.04}
{'loss': 4.6007, 'grad_norm': 0.8468901828910598, 'learning_rate': 9.789688149755763e-05, 'epoch': 0.04}
{'loss': 4.6993, 'grad_norm': 0.7958300069408709, 'learning_rate': 9.791091731167981e-05, 'epoch': 0.04}
{'loss': 4.4717, 'grad_norm': 0.9310505671884639, 'learning_rate': 9.792493636661695e-05, 'epoch': 0.04}
{'loss': 4.4736, 'grad_norm': 0.9186699509683366, 'learning_rate': 9.79389387023433e-05, 'epoch': 0.04}
{'loss': 4.681, 'grad_norm': 0.7997836986448491, 'learning_rate': 9.79529243586902e-05, 'epoch': 0.04}
[2024-04-12 01:49:44,123] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4200 is about to be saved!
[2024-04-12 01:49:44,129] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-4200/global_step4200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 01:49:44,129] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4200/global_step4200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 01:49:44,135] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4200/global_step4200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 01:49:44,135] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4200/global_step4200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 01:49:47,225] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4200/global_step4200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 01:49:47,225] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-4200/global_step4200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 01:49:47,226] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4200 is ready now!
{'loss': 4.7597, 'grad_norm': 0.8000167903930138, 'learning_rate': 9.796689337534688e-05, 'epoch': 0.04}
{'loss': 4.6973, 'grad_norm': 0.8835310424217132, 'learning_rate': 9.798084579186098e-05, 'epoch': 0.04}
{'loss': 4.5198, 'grad_norm': 0.7732826741753832, 'learning_rate': 9.799478164763945e-05, 'epoch': 0.04}
{'loss': 4.6171, 'grad_norm': 0.7421219249059903, 'learning_rate': 9.800870098194897e-05, 'epoch': 0.04}
{'loss': 4.6531, 'grad_norm': 0.7609512248535971, 'learning_rate': 9.802260383391679e-05, 'epoch': 0.04}
{'loss': 4.5145, 'grad_norm': 0.7825827602638853, 'learning_rate': 9.803649024253134e-05, 'epoch': 0.04}
{'loss': 4.6383, 'grad_norm': 0.8649634969474228, 'learning_rate': 9.80503602466428e-05, 'epoch': 0.04}
{'loss': 4.514, 'grad_norm': 0.7902251985577754, 'learning_rate': 9.806421388496391e-05, 'epoch': 0.04}
{'loss': 4.6312, 'grad_norm': 0.692917178630341, 'learning_rate': 9.80780511960705e-05, 'epoch': 0.04}
{'loss': 4.5368, 'grad_norm': 0.794387405756178, 'learning_rate': 9.809187221840214e-05, 'epoch': 0.04}
{'loss': 4.5434, 'grad_norm': 0.853851957496337, 'learning_rate': 9.810567699026286e-05, 'epoch': 0.04}
{'loss': 4.6947, 'grad_norm': 1.0038194279766521, 'learning_rate': 9.811946554982172e-05, 'epoch': 0.04}
{'loss': 4.6792, 'grad_norm': 0.7914340381482768, 'learning_rate': 9.813323793511346e-05, 'epoch': 0.04}
{'loss': 4.6348, 'grad_norm': 0.7989640013727379, 'learning_rate': 9.814699418403912e-05, 'epoch': 0.04}
{'loss': 4.5239, 'grad_norm': 0.7636020775299708, 'learning_rate': 9.816073433436669e-05, 'epoch': 0.04}
{'loss': 4.5874, 'grad_norm': 1.0124705608254307, 'learning_rate': 9.817445842373171e-05, 'epoch': 0.04}
{'loss': 4.6735, 'grad_norm': 0.7232167864931995, 'learning_rate': 9.818816648963792e-05, 'epoch': 0.04}
{'loss': 4.7956, 'grad_norm': 0.8351879987720896, 'learning_rate': 9.820185856945779e-05, 'epoch': 0.04}
{'loss': 4.5617, 'grad_norm': 0.8985508469781158, 'learning_rate': 9.821553470043327e-05, 'epoch': 0.04}
{'loss': 4.6778, 'grad_norm': 0.7999459521576107, 'learning_rate': 9.822919491967628e-05, 'epoch': 0.04}
[2024-04-12 01:54:26,019] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4300 is about to be saved!
[2024-04-12 01:54:26,025] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-4300/global_step4300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 01:54:26,025] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4300/global_step4300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 01:54:26,031] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4300/global_step4300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 01:54:26,032] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4300/global_step4300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 01:54:29,124] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4300/global_step4300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 01:54:29,125] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-4300/global_step4300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 01:54:29,125] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4300 is ready now!
{'loss': 4.4625, 'grad_norm': 0.7694429345556066, 'learning_rate': 9.824283926416936e-05, 'epoch': 0.04}
{'loss': 4.5896, 'grad_norm': 0.7826817507311989, 'learning_rate': 9.825646777076624e-05, 'epoch': 0.04}
{'loss': 4.6614, 'grad_norm': 0.7164697208574764, 'learning_rate': 9.827008047619254e-05, 'epoch': 0.04}
{'loss': 4.5632, 'grad_norm': 0.7800586579013937, 'learning_rate': 9.828367741704618e-05, 'epoch': 0.04}
{'loss': 4.5795, 'grad_norm': 0.7631820833976076, 'learning_rate': 9.829725862979816e-05, 'epoch': 0.04}
{'loss': 4.4197, 'grad_norm': 0.9555235208806842, 'learning_rate': 9.831082415079303e-05, 'epoch': 0.04}
{'loss': 4.3114, 'grad_norm': 0.7484584636263012, 'learning_rate': 9.832437401624955e-05, 'epoch': 0.04}
{'loss': 4.4234, 'grad_norm': 0.7963727050517417, 'learning_rate': 9.833790826226113e-05, 'epoch': 0.04}
{'loss': 4.6535, 'grad_norm': 0.7759576998208367, 'learning_rate': 9.83514269247966e-05, 'epoch': 0.04}
{'loss': 4.5344, 'grad_norm': 0.8308737195813685, 'learning_rate': 9.836493003970066e-05, 'epoch': 0.04}
{'loss': 4.2969, 'grad_norm': 0.853102331768477, 'learning_rate': 9.837841764269445e-05, 'epoch': 0.04}
{'loss': 4.6014, 'grad_norm': 0.7984904251189215, 'learning_rate': 9.839188976937619e-05, 'epoch': 0.04}
{'loss': 4.4209, 'grad_norm': 0.8087059728482492, 'learning_rate': 9.840534645522169e-05, 'epoch': 0.04}
{'loss': 4.3833, 'grad_norm': 0.8196894773000873, 'learning_rate': 9.841878773558488e-05, 'epoch': 0.04}
{'loss': 4.6522, 'grad_norm': 1.0641184634484477, 'learning_rate': 9.843221364569847e-05, 'epoch': 0.04}
{'loss': 4.4951, 'grad_norm': 0.7404091357284136, 'learning_rate': 9.844562422067437e-05, 'epoch': 0.04}
{'loss': 4.2018, 'grad_norm': 0.8133287014159245, 'learning_rate': 9.845901949550436e-05, 'epoch': 0.04}
{'loss': 4.7595, 'grad_norm': 0.8770815066098707, 'learning_rate': 9.847239950506057e-05, 'epoch': 0.04}
{'loss': 4.6302, 'grad_norm': 0.7659397152081381, 'learning_rate': 9.84857642840961e-05, 'epoch': 0.04}
{'loss': 4.4902, 'grad_norm': 0.8532060060554301, 'learning_rate': 9.84991138672454e-05, 'epoch': 0.04}
[2024-04-12 01:59:07,926] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4400 is about to be saved!
[2024-04-12 01:59:07,932] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-4400/global_step4400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 01:59:07,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4400/global_step4400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 01:59:07,938] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4400/global_step4400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 01:59:07,938] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4400/global_step4400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 01:59:11,056] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4400/global_step4400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 01:59:11,056] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-4400/global_step4400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 01:59:11,056] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4400 is ready now!
{'loss': 4.7242, 'grad_norm': 0.7178937296240185, 'learning_rate': 9.8512448289025e-05, 'epoch': 0.04}
{'loss': 4.546, 'grad_norm': 0.7487374022415798, 'learning_rate': 9.852576758383395e-05, 'epoch': 0.04}
{'loss': 4.5984, 'grad_norm': 0.6915783972899375, 'learning_rate': 9.853907178595432e-05, 'epoch': 0.04}
{'loss': 4.6533, 'grad_norm': 0.9101328745094678, 'learning_rate': 9.855236092955182e-05, 'epoch': 0.04}
{'loss': 4.5777, 'grad_norm': 0.8658711641791768, 'learning_rate': 9.856563504867626e-05, 'epoch': 0.04}
{'loss': 4.6275, 'grad_norm': 0.7071931098159069, 'learning_rate': 9.857889417726204e-05, 'epoch': 0.04}
{'loss': 4.6377, 'grad_norm': 0.7417809386746884, 'learning_rate': 9.859213834912886e-05, 'epoch': 0.04}
{'loss': 4.5715, 'grad_norm': 0.8038633840023902, 'learning_rate': 9.860536759798193e-05, 'epoch': 0.04}
{'loss': 4.4093, 'grad_norm': 0.7856961692279439, 'learning_rate': 9.861858195741278e-05, 'epoch': 0.04}
{'loss': 4.5987, 'grad_norm': 0.7549476663685089, 'learning_rate': 9.863178146089962e-05, 'epoch': 0.04}
{'loss': 4.4114, 'grad_norm': 0.8666294583436632, 'learning_rate': 9.864496614180783e-05, 'epoch': 0.04}
{'loss': 4.3433, 'grad_norm': 0.8838130561182593, 'learning_rate': 9.865813603339054e-05, 'epoch': 0.04}
{'loss': 4.4637, 'grad_norm': 0.726852026219425, 'learning_rate': 9.867129116878913e-05, 'epoch': 0.04}
{'loss': 4.696, 'grad_norm': 0.8503145215448586, 'learning_rate': 9.868443158103365e-05, 'epoch': 0.04}
{'loss': 4.5002, 'grad_norm': 0.7835641338769455, 'learning_rate': 9.869755730304344e-05, 'epoch': 0.04}
{'loss': 4.3683, 'grad_norm': 0.7732588574449385, 'learning_rate': 9.871066836762749e-05, 'epoch': 0.04}
{'loss': 4.4302, 'grad_norm': 0.7160549753794958, 'learning_rate': 9.872376480748504e-05, 'epoch': 0.04}
{'loss': 3.9263, 'grad_norm': 0.9707304545198624, 'learning_rate': 9.873684665520604e-05, 'epoch': 0.04}
{'loss': 4.5253, 'grad_norm': 0.7845860263457117, 'learning_rate': 9.874991394327158e-05, 'epoch': 0.04}
{'loss': 4.5569, 'grad_norm': 0.8048603054456572, 'learning_rate': 9.876296670405445e-05, 'epoch': 0.04}
[2024-04-12 02:03:49,792] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4500 is about to be saved!
[2024-04-12 02:03:49,799] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-4500/global_step4500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 02:03:49,799] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4500/global_step4500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 02:03:49,805] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4500/global_step4500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 02:03:49,805] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4500/global_step4500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 02:03:52,887] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4500/global_step4500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 02:03:52,887] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-4500/global_step4500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 02:03:52,888] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4500 is ready now!
{'loss': 4.6521, 'grad_norm': 0.758747049506476, 'learning_rate': 9.877600496981959e-05, 'epoch': 0.04}
{'loss': 4.499, 'grad_norm': 0.7839182578267944, 'learning_rate': 9.878902877272455e-05, 'epoch': 0.04}
{'loss': 4.5627, 'grad_norm': 0.7668708424474012, 'learning_rate': 9.880203814482003e-05, 'epoch': 0.04}
{'loss': 4.5744, 'grad_norm': 0.7231325702175035, 'learning_rate': 9.881503311805025e-05, 'epoch': 0.04}
{'loss': 4.4624, 'grad_norm': 1.3650447705444364, 'learning_rate': 9.882801372425351e-05, 'epoch': 0.04}
{'loss': 4.4993, 'grad_norm': 0.7386335756139243, 'learning_rate': 9.884097999516265e-05, 'epoch': 0.04}
{'loss': 4.4301, 'grad_norm': 0.8784034317209862, 'learning_rate': 9.885393196240544e-05, 'epoch': 0.04}
{'loss': 4.4309, 'grad_norm': 0.7272783198008483, 'learning_rate': 9.886686965750513e-05, 'epoch': 0.04}
{'loss': 4.4492, 'grad_norm': 0.7625478391883749, 'learning_rate': 9.887979311188086e-05, 'epoch': 0.04}
{'loss': 4.5661, 'grad_norm': 0.7867701660055596, 'learning_rate': 9.889270235684815e-05, 'epoch': 0.04}
{'loss': 4.5581, 'grad_norm': 0.7699660608048972, 'learning_rate': 9.890559742361931e-05, 'epoch': 0.04}
{'loss': 4.4497, 'grad_norm': 0.7877417700354648, 'learning_rate': 9.891847834330398e-05, 'epoch': 0.04}
{'loss': 4.6198, 'grad_norm': 0.8112328398029651, 'learning_rate': 9.893134514690943e-05, 'epoch': 0.04}
{'loss': 4.5269, 'grad_norm': 0.9105873668619919, 'learning_rate': 9.894419786534118e-05, 'epoch': 0.04}
{'loss': 4.4334, 'grad_norm': 0.9587986972271919, 'learning_rate': 9.895703652940336e-05, 'epoch': 0.04}
{'loss': 4.551, 'grad_norm': 0.7499219457482896, 'learning_rate': 9.89698611697991e-05, 'epoch': 0.04}
{'loss': 4.519, 'grad_norm': 0.7651153206191782, 'learning_rate': 9.898267181713106e-05, 'epoch': 0.04}
{'loss': 4.4102, 'grad_norm': 0.8862330494062968, 'learning_rate': 9.899546850190186e-05, 'epoch': 0.04}
{'loss': 4.3842, 'grad_norm': 1.215418358662074, 'learning_rate': 9.900825125451447e-05, 'epoch': 0.04}
{'loss': 4.6124, 'grad_norm': 0.7916558596144739, 'learning_rate': 9.902102010527266e-05, 'epoch': 0.04}
[2024-04-12 02:08:31,616] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4600 is about to be saved!
[2024-04-12 02:08:31,622] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-4600/global_step4600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 02:08:31,622] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4600/global_step4600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 02:08:31,628] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4600/global_step4600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 02:08:31,628] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4600/global_step4600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 02:08:34,642] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4600/global_step4600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 02:08:34,643] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-4600/global_step4600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 02:08:34,643] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4600 is ready now!
{'loss': 4.5663, 'grad_norm': 1.3004639263103492, 'learning_rate': 9.903377508438144e-05, 'epoch': 0.04}
{'loss': 4.5366, 'grad_norm': 0.7240022272866584, 'learning_rate': 9.904651622194753e-05, 'epoch': 0.04}
{'loss': 4.523, 'grad_norm': 0.8620632454567667, 'learning_rate': 9.905924354797965e-05, 'epoch': 0.04}
{'loss': 4.3693, 'grad_norm': 0.7852202063264999, 'learning_rate': 9.907195709238913e-05, 'epoch': 0.04}
{'loss': 4.4636, 'grad_norm': 0.8239985506125915, 'learning_rate': 9.90846568849902e-05, 'epoch': 0.04}
{'loss': 4.5826, 'grad_norm': 0.8757094580769044, 'learning_rate': 9.90973429555004e-05, 'epoch': 0.04}
{'loss': 4.6098, 'grad_norm': 1.0279304594158376, 'learning_rate': 9.911001533354114e-05, 'epoch': 0.04}
{'loss': 4.4713, 'grad_norm': 0.8661631205383675, 'learning_rate': 9.912267404863794e-05, 'epoch': 0.04}
{'loss': 4.4959, 'grad_norm': 0.7556645534679407, 'learning_rate': 9.913531913022097e-05, 'epoch': 0.04}
{'loss': 4.467, 'grad_norm': 0.8621887205121181, 'learning_rate': 9.914795060762538e-05, 'epoch': 0.04}
{'loss': 4.622, 'grad_norm': 0.7980513855768586, 'learning_rate': 9.916056851009172e-05, 'epoch': 0.04}
{'loss': 4.4071, 'grad_norm': 0.8156357267210883, 'learning_rate': 9.91731728667665e-05, 'epoch': 0.04}
{'loss': 4.579, 'grad_norm': 1.1254860454358264, 'learning_rate': 9.918576370670225e-05, 'epoch': 0.04}
{'loss': 4.4911, 'grad_norm': 0.6921348183032484, 'learning_rate': 9.919834105885836e-05, 'epoch': 0.04}
{'loss': 4.395, 'grad_norm': 0.8553678549411267, 'learning_rate': 9.921090495210106e-05, 'epoch': 0.04}
{'loss': 4.5121, 'grad_norm': 0.6983307729841177, 'learning_rate': 9.922345541520412e-05, 'epoch': 0.04}
{'loss': 4.4894, 'grad_norm': 0.7430952864101948, 'learning_rate': 9.923599247684911e-05, 'epoch': 0.04}
{'loss': 4.2779, 'grad_norm': 0.7556586327345147, 'learning_rate': 9.924851616562581e-05, 'epoch': 0.04}
{'loss': 4.6172, 'grad_norm': 0.7369867886168802, 'learning_rate': 9.926102651003258e-05, 'epoch': 0.04}
{'loss': 4.289, 'grad_norm': 1.9715277509370353, 'learning_rate': 9.927352353847688e-05, 'epoch': 0.04}
[2024-04-12 02:13:13,370] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4700 is about to be saved!
[2024-04-12 02:13:13,376] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-4700/global_step4700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 02:13:13,376] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4700/global_step4700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 02:13:13,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4700/global_step4700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 02:13:13,384] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4700/global_step4700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 02:13:16,456] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4700/global_step4700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 02:13:16,456] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-4700/global_step4700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 02:13:16,456] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4700 is ready now!
{'loss': 4.2644, 'grad_norm': 1.8500707379364285, 'learning_rate': 9.928600727927547e-05, 'epoch': 0.04}
{'loss': 2.9394, 'grad_norm': 0.8632087451635261, 'learning_rate': 9.929847776065486e-05, 'epoch': 0.04}
{'loss': 4.4936, 'grad_norm': 0.8002358146938873, 'learning_rate': 9.93109350107518e-05, 'epoch': 0.04}
{'loss': 4.4997, 'grad_norm': 0.7782876448767614, 'learning_rate': 9.932337905761354e-05, 'epoch': 0.04}
{'loss': 4.1962, 'grad_norm': 0.723923003782254, 'learning_rate': 9.933580992919818e-05, 'epoch': 0.04}
{'loss': 4.5656, 'grad_norm': 0.7166458203652694, 'learning_rate': 9.934822765337523e-05, 'epoch': 0.04}
{'loss': 4.3279, 'grad_norm': 0.9293702757337176, 'learning_rate': 9.936063225792575e-05, 'epoch': 0.04}
{'loss': 4.3467, 'grad_norm': 0.7641713092901506, 'learning_rate': 9.937302377054293e-05, 'epoch': 0.04}
{'loss': 4.3379, 'grad_norm': 0.812332965092799, 'learning_rate': 9.938540221883231e-05, 'epoch': 0.04}
{'loss': 4.3576, 'grad_norm': 0.7340883365784049, 'learning_rate': 9.939776763031222e-05, 'epoch': 0.04}
{'loss': 4.4202, 'grad_norm': 0.6982911706903265, 'learning_rate': 9.941012003241418e-05, 'epoch': 0.04}
{'loss': 4.3985, 'grad_norm': 0.9502476896840865, 'learning_rate': 9.942245945248316e-05, 'epoch': 0.04}
{'loss': 4.2654, 'grad_norm': 0.7300884340418653, 'learning_rate': 9.943478591777802e-05, 'epoch': 0.04}
{'loss': 4.4356, 'grad_norm': 0.8580525384467403, 'learning_rate': 9.94470994554719e-05, 'epoch': 0.04}
{'loss': 4.441, 'grad_norm': 0.7811934617741377, 'learning_rate': 9.945940009265246e-05, 'epoch': 0.04}
{'loss': 4.4449, 'grad_norm': 0.6898102438711642, 'learning_rate': 9.947168785632237e-05, 'epoch': 0.04}
{'loss': 4.4678, 'grad_norm': 0.8234283706604222, 'learning_rate': 9.948396277339959e-05, 'epoch': 0.04}
{'loss': 4.5103, 'grad_norm': 0.760125928771638, 'learning_rate': 9.949622487071775e-05, 'epoch': 0.04}
{'loss': 4.2189, 'grad_norm': 0.7586067224373141, 'learning_rate': 9.950847417502646e-05, 'epoch': 0.04}
{'loss': 4.4056, 'grad_norm': 0.7981361910731126, 'learning_rate': 9.952071071299174e-05, 'epoch': 0.04}
[2024-04-12 02:17:55,493] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4800 is about to be saved!
[2024-04-12 02:17:55,499] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-4800/global_step4800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 02:17:55,499] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4800/global_step4800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 02:17:55,505] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4800/global_step4800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 02:17:55,505] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4800/global_step4800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 02:17:58,582] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4800/global_step4800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 02:17:58,582] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-4800/global_step4800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 02:17:58,582] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4800 is ready now!
{'loss': 4.4769, 'grad_norm': 0.7268160020853471, 'learning_rate': 9.953293451119627e-05, 'epoch': 0.04}
{'loss': 4.3961, 'grad_norm': 0.7245884715815516, 'learning_rate': 9.954514559613988e-05, 'epoch': 0.04}
{'loss': 4.2297, 'grad_norm': 0.7656789432888925, 'learning_rate': 9.955734399423969e-05, 'epoch': 0.04}
{'loss': 4.5077, 'grad_norm': 0.7989132572595653, 'learning_rate': 9.956952973183062e-05, 'epoch': 0.04}
{'loss': 4.5117, 'grad_norm': 0.758604159093072, 'learning_rate': 9.958170283516574e-05, 'epoch': 0.04}
{'loss': 4.396, 'grad_norm': 0.7134270283331261, 'learning_rate': 9.95938633304164e-05, 'epoch': 0.04}
{'loss': 4.379, 'grad_norm': 0.6845254993526115, 'learning_rate': 9.960601124367282e-05, 'epoch': 0.04}
{'loss': 4.2342, 'grad_norm': 0.7139160632206405, 'learning_rate': 9.961814660094432e-05, 'epoch': 0.04}
{'loss': 4.4352, 'grad_norm': 0.6848955501110852, 'learning_rate': 9.963026942815964e-05, 'epoch': 0.04}
{'loss': 4.3506, 'grad_norm': 1.8463378458004989, 'learning_rate': 9.964237975116724e-05, 'epoch': 0.04}
{'loss': 4.2788, 'grad_norm': 0.7242237329402952, 'learning_rate': 9.965447759573574e-05, 'epoch': 0.04}
{'loss': 4.2914, 'grad_norm': 0.772662840121636, 'learning_rate': 9.966656298755416e-05, 'epoch': 0.04}
{'loss': 4.4805, 'grad_norm': 0.7175025899103848, 'learning_rate': 9.967863595223226e-05, 'epoch': 0.04}
{'loss': 4.2881, 'grad_norm': 0.7992543440515613, 'learning_rate': 9.969069651530092e-05, 'epoch': 0.04}
{'loss': 4.4114, 'grad_norm': 0.7759191520900043, 'learning_rate': 9.970274470221239e-05, 'epoch': 0.04}
{'loss': 4.4598, 'grad_norm': 0.9208619991088162, 'learning_rate': 9.971478053834064e-05, 'epoch': 0.04}
{'loss': 4.3457, 'grad_norm': 0.7791907027862878, 'learning_rate': 9.97268040489817e-05, 'epoch': 0.04}
{'loss': 4.3902, 'grad_norm': 1.0574413399857419, 'learning_rate': 9.973881525935398e-05, 'epoch': 0.04}
{'loss': 4.1529, 'grad_norm': 0.9745827354411802, 'learning_rate': 9.975081419459855e-05, 'epoch': 0.04}
{'loss': 4.3891, 'grad_norm': 0.7625722643361216, 'learning_rate': 9.976280087977951e-05, 'epoch': 0.04}
[2024-04-12 02:22:37,500] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4900 is about to be saved!
[2024-04-12 02:22:37,507] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-4900/global_step4900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 02:22:37,507] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4900/global_step4900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 02:22:37,513] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4900/global_step4900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 02:22:37,513] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4900/global_step4900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 02:22:40,528] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4900/global_step4900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 02:22:40,528] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-4900/global_step4900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 02:22:40,529] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4900 is ready now!
{'loss': 4.3099, 'grad_norm': 0.7135887771866133, 'learning_rate': 9.977477533988418e-05, 'epoch': 0.04}
{'loss': 4.5041, 'grad_norm': 0.8494258126015463, 'learning_rate': 9.978673759982365e-05, 'epoch': 0.04}
{'loss': 4.487, 'grad_norm': 0.812117584530399, 'learning_rate': 9.979868768443282e-05, 'epoch': 0.04}
{'loss': 4.5107, 'grad_norm': 0.7297859980496372, 'learning_rate': 9.981062561847088e-05, 'epoch': 0.04}
{'loss': 4.3907, 'grad_norm': 1.0329901936051142, 'learning_rate': 9.982255142662162e-05, 'epoch': 0.04}
{'loss': 4.4999, 'grad_norm': 1.0296914820541896, 'learning_rate': 9.983446513349362e-05, 'epoch': 0.04}
{'loss': 4.3309, 'grad_norm': 0.8246086296655878, 'learning_rate': 9.984636676362062e-05, 'epoch': 0.04}
{'loss': 4.4075, 'grad_norm': 1.673344589756813, 'learning_rate': 9.985825634146192e-05, 'epoch': 0.04}
{'loss': 4.3526, 'grad_norm': 0.6997144980659289, 'learning_rate': 9.987013389140249e-05, 'epoch': 0.04}
{'loss': 4.5176, 'grad_norm': 0.6917715285749344, 'learning_rate': 9.988199943775337e-05, 'epoch': 0.04}
{'loss': 4.3743, 'grad_norm': 1.057230566276691, 'learning_rate': 9.989385300475205e-05, 'epoch': 0.04}
{'loss': 4.1792, 'grad_norm': 0.7461037204084124, 'learning_rate': 9.990569461656266e-05, 'epoch': 0.04}
{'loss': 4.5017, 'grad_norm': 0.8035298784652686, 'learning_rate': 9.99175242972762e-05, 'epoch': 0.04}
{'loss': 4.4539, 'grad_norm': 0.7194861509931405, 'learning_rate': 9.992934207091101e-05, 'epoch': 0.04}
{'loss': 4.2431, 'grad_norm': 0.738000135119495, 'learning_rate': 9.994114796141296e-05, 'epoch': 0.04}
{'loss': 4.1714, 'grad_norm': 0.7678187924489103, 'learning_rate': 9.995294199265576e-05, 'epoch': 0.04}
{'loss': 4.4055, 'grad_norm': 0.7027873905482359, 'learning_rate': 9.996472418844122e-05, 'epoch': 0.04}
{'loss': 4.4236, 'grad_norm': 0.7343986468881766, 'learning_rate': 9.997649457249965e-05, 'epoch': 0.04}
{'loss': 4.0793, 'grad_norm': 0.8540172816224032, 'learning_rate': 9.998825316848992e-05, 'epoch': 0.04}
{'loss': 4.2201, 'grad_norm': 0.9615592864862071, 'learning_rate': 0.0001, 'epoch': 0.04}
[2024-04-12 02:27:18,780] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5000 is about to be saved!
[2024-04-12 02:27:18,786] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-5000/global_step5000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 02:27:18,786] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5000/global_step5000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 02:27:18,792] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5000/global_step5000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 02:27:18,792] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5000/global_step5000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 02:27:21,787] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5000/global_step5000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 02:27:21,787] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-5000/global_step5000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 02:27:21,787] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
{'loss': 4.4546, 'grad_norm': 0.7243924387146707, 'learning_rate': 9.999881506058004e-05, 'epoch': 0.04}
{'loss': 4.3592, 'grad_norm': 0.7736055012766944, 'learning_rate': 9.999733388630506e-05, 'epoch': 0.04}
{'loss': 4.2555, 'grad_norm': 0.7556739777489414, 'learning_rate': 9.99958527120301e-05, 'epoch': 0.04}
{'loss': 3.9843, 'grad_norm': 3.912568832808329, 'learning_rate': 9.999437153775514e-05, 'epoch': 0.04}
{'loss': 4.4246, 'grad_norm': 0.7423485451895779, 'learning_rate': 9.999289036348017e-05, 'epoch': 0.04}
{'loss': 4.4295, 'grad_norm': 0.7678501634024728, 'learning_rate': 9.999140918920521e-05, 'epoch': 0.04}
{'loss': 4.483, 'grad_norm': 0.6968204789274169, 'learning_rate': 9.998992801493025e-05, 'epoch': 0.04}
{'loss': 4.3232, 'grad_norm': 0.7100022061307312, 'learning_rate': 9.998844684065528e-05, 'epoch': 0.04}
{'loss': 4.2682, 'grad_norm': 0.9333985397517752, 'learning_rate': 9.99869656663803e-05, 'epoch': 0.04}
{'loss': 4.3972, 'grad_norm': 0.7338824902257903, 'learning_rate': 9.998548449210534e-05, 'epoch': 0.04}
{'loss': 4.1927, 'grad_norm': 0.8465051534700527, 'learning_rate': 9.998400331783038e-05, 'epoch': 0.04}
{'loss': 4.3378, 'grad_norm': 0.8973959727288973, 'learning_rate': 9.998252214355541e-05, 'epoch': 0.04}
{'loss': 4.3176, 'grad_norm': 0.7058370921722609, 'learning_rate': 9.998104096928045e-05, 'epoch': 0.04}
{'loss': 4.4431, 'grad_norm': 0.7824426263172555, 'learning_rate': 9.997955979500549e-05, 'epoch': 0.04}
{'loss': 4.1469, 'grad_norm': 0.7723553537888794, 'learning_rate': 9.997807862073052e-05, 'epoch': 0.04}
{'loss': 4.4528, 'grad_norm': 0.7290422271817489, 'learning_rate': 9.997659744645555e-05, 'epoch': 0.04}
{'loss': 4.4571, 'grad_norm': 0.7350472567034185, 'learning_rate': 9.99751162721806e-05, 'epoch': 0.04}
{'loss': 4.4483, 'grad_norm': 0.7671610046930032, 'learning_rate': 9.997363509790562e-05, 'epoch': 0.04}
{'loss': 4.3752, 'grad_norm': 0.705882874316575, 'learning_rate': 9.997215392363066e-05, 'epoch': 0.04}
{'loss': 4.1838, 'grad_norm': 0.6853662021930791, 'learning_rate': 9.99706727493557e-05, 'epoch': 0.04}
[2024-04-12 02:31:59,222] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5100 is about to be saved!
[2024-04-12 02:31:59,227] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-5100/global_step5100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 02:31:59,227] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5100/global_step5100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 02:31:59,233] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5100/global_step5100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 02:31:59,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5100/global_step5100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 02:32:02,204] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5100/global_step5100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 02:32:02,204] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-5100/global_step5100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 02:32:02,204] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5100 is ready now!
{'loss': 4.096, 'grad_norm': 0.7959940712732709, 'learning_rate': 9.996919157508073e-05, 'epoch': 0.04}
{'loss': 4.4626, 'grad_norm': 0.7077545275643314, 'learning_rate': 9.996771040080575e-05, 'epoch': 0.04}
{'loss': 4.4121, 'grad_norm': 0.8279904077752889, 'learning_rate': 9.99662292265308e-05, 'epoch': 0.04}
{'loss': 4.3988, 'grad_norm': 0.8054190552283551, 'learning_rate': 9.996474805225583e-05, 'epoch': 0.04}
{'loss': 4.3043, 'grad_norm': 0.8201338424600653, 'learning_rate': 9.996326687798086e-05, 'epoch': 0.04}
{'loss': 4.3821, 'grad_norm': 0.8311862433045603, 'learning_rate': 9.99617857037059e-05, 'epoch': 0.04}
{'loss': 4.3469, 'grad_norm': 0.7255712620837305, 'learning_rate': 9.996030452943094e-05, 'epoch': 0.04}
{'loss': 4.3372, 'grad_norm': 0.7275621272067704, 'learning_rate': 9.995882335515597e-05, 'epoch': 0.05}
{'loss': 4.3382, 'grad_norm': 0.7679464368548375, 'learning_rate': 9.995734218088101e-05, 'epoch': 0.05}
{'loss': 4.1645, 'grad_norm': 0.7205106575047827, 'learning_rate': 9.995586100660605e-05, 'epoch': 0.05}
{'loss': 4.4337, 'grad_norm': 0.6908670704026134, 'learning_rate': 9.995437983233107e-05, 'epoch': 0.05}
{'loss': 4.2473, 'grad_norm': 0.7352863961027114, 'learning_rate': 9.995289865805611e-05, 'epoch': 0.05}
{'loss': 4.3147, 'grad_norm': 0.742824905634501, 'learning_rate': 9.995141748378115e-05, 'epoch': 0.05}
{'loss': 4.4548, 'grad_norm': 0.7903553747666601, 'learning_rate': 9.994993630950618e-05, 'epoch': 0.05}
{'loss': 4.1652, 'grad_norm': 0.9264575620021674, 'learning_rate': 9.99484551352312e-05, 'epoch': 0.05}
{'loss': 4.3864, 'grad_norm': 0.7151750941051993, 'learning_rate': 9.994697396095625e-05, 'epoch': 0.05}
{'loss': 4.5343, 'grad_norm': 0.7579849006396623, 'learning_rate': 9.994549278668129e-05, 'epoch': 0.05}
{'loss': 4.424, 'grad_norm': 0.7354501530202258, 'learning_rate': 9.994401161240631e-05, 'epoch': 0.05}
{'loss': 4.172, 'grad_norm': 0.985029990035649, 'learning_rate': 9.994253043813135e-05, 'epoch': 0.05}
{'loss': 2.9541, 'grad_norm': 1.1938030851023382, 'learning_rate': 9.994104926385639e-05, 'epoch': 0.05}
[2024-04-12 02:36:39,617] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5200 is about to be saved!
[2024-04-12 02:36:39,622] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-5200/global_step5200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 02:36:39,622] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5200/global_step5200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 02:36:39,628] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5200/global_step5200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 02:36:39,629] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5200/global_step5200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 02:36:42,644] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5200/global_step5200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 02:36:42,645] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-5200/global_step5200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 02:36:42,645] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5200 is ready now!
{'loss': 4.3302, 'grad_norm': 0.7785345633850189, 'learning_rate': 9.993956808958143e-05, 'epoch': 0.05}
{'loss': 4.4632, 'grad_norm': 0.7418179056521926, 'learning_rate': 9.993808691530646e-05, 'epoch': 0.05}
{'loss': 4.3678, 'grad_norm': 0.7667218998743186, 'learning_rate': 9.99366057410315e-05, 'epoch': 0.05}
{'loss': 4.3644, 'grad_norm': 0.7610800514520283, 'learning_rate': 9.993512456675654e-05, 'epoch': 0.05}
{'loss': 4.3721, 'grad_norm': 0.7040653732390758, 'learning_rate': 9.993364339248156e-05, 'epoch': 0.05}
{'loss': 4.4115, 'grad_norm': 0.7226372801276008, 'learning_rate': 9.99321622182066e-05, 'epoch': 0.05}
{'loss': 4.343, 'grad_norm': 0.6864141223422539, 'learning_rate': 9.993068104393163e-05, 'epoch': 0.05}
{'loss': 4.5409, 'grad_norm': 0.6916108506404778, 'learning_rate': 9.992919986965666e-05, 'epoch': 0.05}
{'loss': 4.3456, 'grad_norm': 0.7210863202889823, 'learning_rate': 9.99277186953817e-05, 'epoch': 0.05}
{'loss': 4.3008, 'grad_norm': 0.7341135024541157, 'learning_rate': 9.992623752110674e-05, 'epoch': 0.05}
{'loss': 4.3468, 'grad_norm': 0.784292625040793, 'learning_rate': 9.992475634683178e-05, 'epoch': 0.05}
{'loss': 4.5538, 'grad_norm': 0.8391213308276161, 'learning_rate': 9.99232751725568e-05, 'epoch': 0.05}
{'loss': 3.8901, 'grad_norm': 0.897063545424374, 'learning_rate': 9.992179399828184e-05, 'epoch': 0.05}
{'loss': 4.1668, 'grad_norm': 0.7898579729127763, 'learning_rate': 9.992031282400688e-05, 'epoch': 0.05}
{'loss': 4.1781, 'grad_norm': 0.7509742470920028, 'learning_rate': 9.991883164973191e-05, 'epoch': 0.05}
{'loss': 4.3416, 'grad_norm': 0.7519900209235154, 'learning_rate': 9.991735047545695e-05, 'epoch': 0.05}
{'loss': 4.356, 'grad_norm': 0.7074287738785862, 'learning_rate': 9.991586930118199e-05, 'epoch': 0.05}
{'loss': 4.337, 'grad_norm': 0.7476369850480789, 'learning_rate': 9.991438812690702e-05, 'epoch': 0.05}
{'loss': 4.3293, 'grad_norm': 0.764524921036647, 'learning_rate': 9.991290695263204e-05, 'epoch': 0.05}
{'loss': 4.3534, 'grad_norm': 0.6970954412429969, 'learning_rate': 9.991142577835708e-05, 'epoch': 0.05}
[2024-04-12 02:41:19,945] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5300 is about to be saved!
[2024-04-12 02:41:19,951] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-5300/global_step5300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 02:41:19,951] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5300/global_step5300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 02:41:19,957] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5300/global_step5300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 02:41:19,957] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5300/global_step5300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 02:41:22,983] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5300/global_step5300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 02:41:22,984] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-5300/global_step5300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 02:41:22,984] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5300 is ready now!
{'loss': 4.2983, 'grad_norm': 0.7344552967225778, 'learning_rate': 9.990994460408212e-05, 'epoch': 0.05}
{'loss': 4.2159, 'grad_norm': 0.8000212719703328, 'learning_rate': 9.990846342980715e-05, 'epoch': 0.05}
{'loss': 4.2773, 'grad_norm': 0.7151715494026127, 'learning_rate': 9.990698225553219e-05, 'epoch': 0.05}
{'loss': 4.1487, 'grad_norm': 0.7167712569024499, 'learning_rate': 9.990550108125723e-05, 'epoch': 0.05}
{'loss': 4.3017, 'grad_norm': 0.7280392879734934, 'learning_rate': 9.990401990698226e-05, 'epoch': 0.05}
{'loss': 4.3124, 'grad_norm': 0.7364757368841206, 'learning_rate': 9.99025387327073e-05, 'epoch': 0.05}
{'loss': 4.4132, 'grad_norm': 0.7260967239212486, 'learning_rate': 9.990105755843234e-05, 'epoch': 0.05}
{'loss': 4.0671, 'grad_norm': 0.7100080429060036, 'learning_rate': 9.989957638415736e-05, 'epoch': 0.05}
{'loss': 4.1195, 'grad_norm': 0.7587198792875245, 'learning_rate': 9.98980952098824e-05, 'epoch': 0.05}
{'loss': 4.2266, 'grad_norm': 0.8968936544396952, 'learning_rate': 9.989661403560744e-05, 'epoch': 0.05}
{'loss': 3.3081, 'grad_norm': 0.9841513054711563, 'learning_rate': 9.989513286133247e-05, 'epoch': 0.05}
{'loss': 4.4077, 'grad_norm': 0.9223494433055444, 'learning_rate': 9.98936516870575e-05, 'epoch': 0.05}
{'loss': 4.3776, 'grad_norm': 0.754182146999366, 'learning_rate': 9.989217051278254e-05, 'epoch': 0.05}
{'loss': 4.4783, 'grad_norm': 0.7947519420478384, 'learning_rate': 9.989068933850758e-05, 'epoch': 0.05}
{'loss': 4.1439, 'grad_norm': 0.7713083447614679, 'learning_rate': 9.98892081642326e-05, 'epoch': 0.05}
{'loss': 4.0514, 'grad_norm': 0.7822095369892108, 'learning_rate': 9.988772698995764e-05, 'epoch': 0.05}
{'loss': 4.4317, 'grad_norm': 0.7470202676541117, 'learning_rate': 9.988624581568268e-05, 'epoch': 0.05}
{'loss': 4.2761, 'grad_norm': 0.7234944101678565, 'learning_rate': 9.988476464140771e-05, 'epoch': 0.05}
{'loss': 4.282, 'grad_norm': 0.7042977753711673, 'learning_rate': 9.988328346713275e-05, 'epoch': 0.05}
{'loss': 4.2269, 'grad_norm': 0.868487906867271, 'learning_rate': 9.988180229285779e-05, 'epoch': 0.05}
[2024-04-12 02:46:00,365] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5400 is about to be saved!
[2024-04-12 02:46:00,371] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-5400/global_step5400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 02:46:00,371] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5400/global_step5400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 02:46:00,377] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5400/global_step5400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 02:46:00,377] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5400/global_step5400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 02:46:03,396] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5400/global_step5400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 02:46:03,396] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-5400/global_step5400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 02:46:03,397] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5400 is ready now!
{'loss': 4.2433, 'grad_norm': 0.7102634609974386, 'learning_rate': 9.988032111858281e-05, 'epoch': 0.05}
{'loss': 4.1264, 'grad_norm': 0.7488619690978419, 'learning_rate': 9.987883994430785e-05, 'epoch': 0.05}
{'loss': 4.1466, 'grad_norm': 0.7970758696898421, 'learning_rate': 9.98773587700329e-05, 'epoch': 0.05}
{'loss': 3.977, 'grad_norm': 0.8345475261062689, 'learning_rate': 9.987587759575792e-05, 'epoch': 0.05}
{'loss': 4.3556, 'grad_norm': 0.6939170890295489, 'learning_rate': 9.987439642148295e-05, 'epoch': 0.05}
{'loss': 4.34, 'grad_norm': 0.733889946357134, 'learning_rate': 9.987291524720799e-05, 'epoch': 0.05}
{'loss': 4.1132, 'grad_norm': 0.7920221317224545, 'learning_rate': 9.987143407293303e-05, 'epoch': 0.05}
{'loss': 4.2996, 'grad_norm': 0.7957563456923638, 'learning_rate': 9.986995289865805e-05, 'epoch': 0.05}
{'loss': 4.2286, 'grad_norm': 0.7174411680738852, 'learning_rate': 9.986847172438309e-05, 'epoch': 0.05}
{'loss': 4.3875, 'grad_norm': 0.7674926952798214, 'learning_rate': 9.986699055010813e-05, 'epoch': 0.05}
{'loss': 4.2782, 'grad_norm': 0.7482059870121531, 'learning_rate': 9.986550937583316e-05, 'epoch': 0.05}
{'loss': 4.2655, 'grad_norm': 0.7801998026288044, 'learning_rate': 9.98640282015582e-05, 'epoch': 0.05}
{'loss': 4.4121, 'grad_norm': 0.726777180894829, 'learning_rate': 9.986254702728324e-05, 'epoch': 0.05}
{'loss': 4.3416, 'grad_norm': 0.7389569856229793, 'learning_rate': 9.986106585300828e-05, 'epoch': 0.05}
{'loss': 4.2319, 'grad_norm': 0.8082265640810711, 'learning_rate': 9.98595846787333e-05, 'epoch': 0.05}
{'loss': 4.4153, 'grad_norm': 0.9755522603761972, 'learning_rate': 9.985810350445835e-05, 'epoch': 0.05}
{'loss': 4.4696, 'grad_norm': 0.8222229856025469, 'learning_rate': 9.985662233018337e-05, 'epoch': 0.05}
{'loss': 4.3961, 'grad_norm': 0.7788832290684574, 'learning_rate': 9.98551411559084e-05, 'epoch': 0.05}
{'loss': 4.3982, 'grad_norm': 0.6962627943010138, 'learning_rate': 9.985365998163344e-05, 'epoch': 0.05}
{'loss': 4.2206, 'grad_norm': 0.6868133826376137, 'learning_rate': 9.985217880735848e-05, 'epoch': 0.05}
[2024-04-12 02:50:40,638] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5500 is about to be saved!
[2024-04-12 02:50:40,643] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-5500/global_step5500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 02:50:40,644] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5500/global_step5500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 02:50:40,650] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5500/global_step5500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 02:50:40,650] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5500/global_step5500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 02:50:43,661] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5500/global_step5500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 02:50:43,661] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-5500/global_step5500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 02:50:43,662] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5500 is ready now!
{'loss': 4.2854, 'grad_norm': 0.8047354444283127, 'learning_rate': 9.985069763308352e-05, 'epoch': 0.05}
{'loss': 4.2215, 'grad_norm': 0.8442656896709132, 'learning_rate': 9.984921645880855e-05, 'epoch': 0.05}
{'loss': 4.2651, 'grad_norm': 0.7444070795832236, 'learning_rate': 9.984773528453359e-05, 'epoch': 0.05}
{'loss': 4.3142, 'grad_norm': 0.7461961214076891, 'learning_rate': 9.984625411025862e-05, 'epoch': 0.05}
{'loss': 4.2688, 'grad_norm': 0.6744002241970224, 'learning_rate': 9.984477293598365e-05, 'epoch': 0.05}
{'loss': 4.3416, 'grad_norm': 0.7080275431020818, 'learning_rate': 9.984329176170869e-05, 'epoch': 0.05}
{'loss': 3.9169, 'grad_norm': 1.363699710176695, 'learning_rate': 9.984181058743373e-05, 'epoch': 0.05}
{'loss': 3.8175, 'grad_norm': 0.8248202525237265, 'learning_rate': 9.984032941315876e-05, 'epoch': 0.05}
{'loss': 4.1827, 'grad_norm': 0.7449905364734508, 'learning_rate': 9.983884823888378e-05, 'epoch': 0.05}
{'loss': 4.0233, 'grad_norm': 0.6983965855606076, 'learning_rate': 9.983736706460882e-05, 'epoch': 0.05}
{'loss': 4.3429, 'grad_norm': 0.703425280652133, 'learning_rate': 9.983588589033386e-05, 'epoch': 0.05}
{'loss': 4.3173, 'grad_norm': 0.7311187605597391, 'learning_rate': 9.983440471605889e-05, 'epoch': 0.05}
{'loss': 4.2153, 'grad_norm': 0.7236815620946263, 'learning_rate': 9.983292354178393e-05, 'epoch': 0.05}
{'loss': 4.4536, 'grad_norm': 0.7851767665860833, 'learning_rate': 9.983144236750897e-05, 'epoch': 0.05}
{'loss': 4.2477, 'grad_norm': 0.8533904478901074, 'learning_rate': 9.9829961193234e-05, 'epoch': 0.05}
{'loss': 4.3774, 'grad_norm': 0.8332231102477116, 'learning_rate': 9.982848001895904e-05, 'epoch': 0.05}
{'loss': 4.33, 'grad_norm': 0.7705979833816891, 'learning_rate': 9.982699884468408e-05, 'epoch': 0.05}
{'loss': 4.2172, 'grad_norm': 0.6884948548413783, 'learning_rate': 9.98255176704091e-05, 'epoch': 0.05}
{'loss': 4.2542, 'grad_norm': 0.7800856921591204, 'learning_rate': 9.982403649613414e-05, 'epoch': 0.05}
{'loss': 4.348, 'grad_norm': 0.6785366745181339, 'learning_rate': 9.982255532185918e-05, 'epoch': 0.05}
[2024-04-12 02:55:20,746] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5600 is about to be saved!
[2024-04-12 02:55:20,751] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-5600/global_step5600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 02:55:20,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5600/global_step5600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 02:55:20,757] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5600/global_step5600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 02:55:20,758] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5600/global_step5600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 02:55:23,795] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5600/global_step5600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 02:55:23,795] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-5600/global_step5600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 02:55:23,796] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5600 is ready now!
{'loss': 4.0954, 'grad_norm': 0.8415297854980842, 'learning_rate': 9.982107414758421e-05, 'epoch': 0.05}
{'loss': 4.1657, 'grad_norm': 0.8136760446383915, 'learning_rate': 9.981959297330924e-05, 'epoch': 0.05}
{'loss': 4.2443, 'grad_norm': 0.7349222091992907, 'learning_rate': 9.981811179903428e-05, 'epoch': 0.05}
{'loss': 4.2054, 'grad_norm': 0.7671644643657455, 'learning_rate': 9.981663062475932e-05, 'epoch': 0.05}
{'loss': 4.315, 'grad_norm': 0.7187431779283906, 'learning_rate': 9.981514945048434e-05, 'epoch': 0.05}
{'loss': 4.1269, 'grad_norm': 0.7375064485377336, 'learning_rate': 9.981366827620938e-05, 'epoch': 0.05}
{'loss': 4.3381, 'grad_norm': 0.6957397870464929, 'learning_rate': 9.981218710193442e-05, 'epoch': 0.05}
{'loss': 4.391, 'grad_norm': 0.6683028060232311, 'learning_rate': 9.981070592765945e-05, 'epoch': 0.05}
{'loss': 4.2639, 'grad_norm': 0.7592341052829056, 'learning_rate': 9.980922475338449e-05, 'epoch': 0.05}
{'loss': 4.1325, 'grad_norm': 0.8700593191394681, 'learning_rate': 9.980774357910953e-05, 'epoch': 0.05}
{'loss': 4.3032, 'grad_norm': 0.7660017555180211, 'learning_rate': 9.980626240483456e-05, 'epoch': 0.05}
{'loss': 4.3012, 'grad_norm': 0.7519686508249297, 'learning_rate': 9.98047812305596e-05, 'epoch': 0.05}
{'loss': 4.0862, 'grad_norm': 0.7912784646915959, 'learning_rate': 9.980330005628464e-05, 'epoch': 0.05}
{'loss': 4.1838, 'grad_norm': 0.7353379751909623, 'learning_rate': 9.980181888200966e-05, 'epoch': 0.05}
{'loss': 4.1647, 'grad_norm': 0.8138614136069633, 'learning_rate': 9.980033770773469e-05, 'epoch': 0.05}
{'loss': 4.1133, 'grad_norm': 0.6900900536164865, 'learning_rate': 9.979885653345973e-05, 'epoch': 0.05}
{'loss': 4.294, 'grad_norm': 0.7044804647674527, 'learning_rate': 9.979737535918477e-05, 'epoch': 0.05}
{'loss': 4.3921, 'grad_norm': 0.7331472328765369, 'learning_rate': 9.97958941849098e-05, 'epoch': 0.05}
{'loss': 4.2315, 'grad_norm': 0.7371668034109277, 'learning_rate': 9.979441301063483e-05, 'epoch': 0.05}
{'loss': 4.3074, 'grad_norm': 0.7389771387876126, 'learning_rate': 9.979293183635987e-05, 'epoch': 0.05}
[2024-04-12 03:00:00,901] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5700 is about to be saved!
[2024-04-12 03:00:00,907] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-5700/global_step5700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 03:00:00,907] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5700/global_step5700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 03:00:00,913] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5700/global_step5700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 03:00:00,913] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5700/global_step5700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 03:00:03,989] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5700/global_step5700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 03:00:03,989] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-5700/global_step5700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 03:00:03,990] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5700 is ready now!
{'loss': 4.2924, 'grad_norm': 0.7584892505019645, 'learning_rate': 9.97914506620849e-05, 'epoch': 0.05}
{'loss': 4.2789, 'grad_norm': 0.7253424054152504, 'learning_rate': 9.978996948780994e-05, 'epoch': 0.05}
{'loss': 4.2112, 'grad_norm': 0.6878895700010224, 'learning_rate': 9.978848831353498e-05, 'epoch': 0.05}
{'loss': 4.1196, 'grad_norm': 0.6717903198053803, 'learning_rate': 9.978700713926001e-05, 'epoch': 0.05}
{'loss': 4.0961, 'grad_norm': 0.8147950896739635, 'learning_rate': 9.978552596498505e-05, 'epoch': 0.05}
{'loss': 4.277, 'grad_norm': 0.7099068166692301, 'learning_rate': 9.978404479071009e-05, 'epoch': 0.05}
{'loss': 4.0662, 'grad_norm': 0.7247434607142278, 'learning_rate': 9.978256361643511e-05, 'epoch': 0.05}
{'loss': 4.1116, 'grad_norm': 0.725991779788907, 'learning_rate': 9.978108244216014e-05, 'epoch': 0.05}
{'loss': 4.0232, 'grad_norm': 0.7494021999014062, 'learning_rate': 9.977960126788518e-05, 'epoch': 0.05}
{'loss': 4.2534, 'grad_norm': 0.6932097736255611, 'learning_rate': 9.977812009361022e-05, 'epoch': 0.05}
{'loss': 4.3446, 'grad_norm': 0.6851919929785774, 'learning_rate': 9.977663891933525e-05, 'epoch': 0.05}
{'loss': 4.3133, 'grad_norm': 0.7192838448845356, 'learning_rate': 9.977515774506029e-05, 'epoch': 0.05}
{'loss': 3.6298, 'grad_norm': 0.7201290786025607, 'learning_rate': 9.977367657078533e-05, 'epoch': 0.05}
{'loss': 4.3129, 'grad_norm': 0.7558917991559031, 'learning_rate': 9.977219539651037e-05, 'epoch': 0.05}
{'loss': 4.1068, 'grad_norm': 0.7996414466792005, 'learning_rate': 9.977071422223539e-05, 'epoch': 0.05}
{'loss': 4.1805, 'grad_norm': 0.7296502973481924, 'learning_rate': 9.976923304796043e-05, 'epoch': 0.05}
{'loss': 4.235, 'grad_norm': 0.72983667436292, 'learning_rate': 9.976775187368547e-05, 'epoch': 0.05}
{'loss': 4.2641, 'grad_norm': 0.7178591137730078, 'learning_rate': 9.97662706994105e-05, 'epoch': 0.05}
{'loss': 4.2195, 'grad_norm': 0.680924498932656, 'learning_rate': 9.976478952513553e-05, 'epoch': 0.05}
{'loss': 4.38, 'grad_norm': 0.7679415611939503, 'learning_rate': 9.976330835086057e-05, 'epoch': 0.05}
[2024-04-12 03:04:41,259] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5800 is about to be saved!
[2024-04-12 03:04:41,264] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-5800/global_step5800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 03:04:41,264] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5800/global_step5800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 03:04:41,270] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5800/global_step5800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 03:04:41,270] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5800/global_step5800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 03:04:44,337] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5800/global_step5800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 03:04:44,337] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-5800/global_step5800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 03:04:44,338] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5800 is ready now!
{'loss': 4.2341, 'grad_norm': 0.8475750327641186, 'learning_rate': 9.97618271765856e-05, 'epoch': 0.05}
{'loss': 4.1221, 'grad_norm': 0.6900980913525684, 'learning_rate': 9.976034600231063e-05, 'epoch': 0.05}
{'loss': 4.1726, 'grad_norm': 0.7084155006391151, 'learning_rate': 9.975886482803567e-05, 'epoch': 0.05}
{'loss': 4.2128, 'grad_norm': 0.7188780665248886, 'learning_rate': 9.975738365376071e-05, 'epoch': 0.05}
{'loss': 4.1201, 'grad_norm': 1.0333959813739286, 'learning_rate': 9.975590247948574e-05, 'epoch': 0.05}
{'loss': 4.3456, 'grad_norm': 0.7167958202984871, 'learning_rate': 9.975442130521078e-05, 'epoch': 0.05}
{'loss': 4.4135, 'grad_norm': 0.7173299669315277, 'learning_rate': 9.975294013093582e-05, 'epoch': 0.05}
{'loss': 4.0975, 'grad_norm': 0.75645848509192, 'learning_rate': 9.975145895666084e-05, 'epoch': 0.05}
{'loss': 4.2444, 'grad_norm': 0.6976405628649078, 'learning_rate': 9.974997778238588e-05, 'epoch': 0.05}
{'loss': 4.325, 'grad_norm': 0.9330736791000015, 'learning_rate': 9.974849660811092e-05, 'epoch': 0.05}
{'loss': 4.1037, 'grad_norm': 0.6885871062906891, 'learning_rate': 9.974701543383595e-05, 'epoch': 0.05}
{'loss': 4.2806, 'grad_norm': 0.7107927662878254, 'learning_rate': 9.974553425956098e-05, 'epoch': 0.05}
{'loss': 4.2519, 'grad_norm': 0.6773915962599463, 'learning_rate': 9.974405308528602e-05, 'epoch': 0.05}
{'loss': 4.3184, 'grad_norm': 0.7222915183036397, 'learning_rate': 9.974257191101106e-05, 'epoch': 0.05}
{'loss': 4.0354, 'grad_norm': 0.6978016035692975, 'learning_rate': 9.974109073673608e-05, 'epoch': 0.05}
{'loss': 3.855, 'grad_norm': 1.0760897278764776, 'learning_rate': 9.973960956246112e-05, 'epoch': 0.05}
{'loss': 3.8815, 'grad_norm': 0.7455620313963921, 'learning_rate': 9.973812838818616e-05, 'epoch': 0.05}
{'loss': 4.4085, 'grad_norm': 0.7083100937108152, 'learning_rate': 9.973664721391119e-05, 'epoch': 0.05}
{'loss': 4.0057, 'grad_norm': 1.4680837136622227, 'learning_rate': 9.973516603963623e-05, 'epoch': 0.05}
{'loss': 4.339, 'grad_norm': 0.69548747742455, 'learning_rate': 9.973368486536127e-05, 'epoch': 0.05}
[2024-04-12 03:09:21,383] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5900 is about to be saved!
[2024-04-12 03:09:21,388] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-5900/global_step5900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 03:09:21,388] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5900/global_step5900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 03:09:21,394] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5900/global_step5900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 03:09:21,395] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5900/global_step5900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 03:09:24,413] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5900/global_step5900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 03:09:24,413] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-5900/global_step5900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 03:09:24,414] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5900 is ready now!
{'loss': 4.343, 'grad_norm': 0.7153529051281133, 'learning_rate': 9.97322036910863e-05, 'epoch': 0.05}
{'loss': 3.9527, 'grad_norm': 1.760677874802195, 'learning_rate': 9.973072251681134e-05, 'epoch': 0.05}
{'loss': 4.1999, 'grad_norm': 0.7200464381776327, 'learning_rate': 9.972924134253638e-05, 'epoch': 0.05}
{'loss': 4.2537, 'grad_norm': 0.7106398566382184, 'learning_rate': 9.97277601682614e-05, 'epoch': 0.05}
{'loss': 4.0179, 'grad_norm': 0.8004995005585198, 'learning_rate': 9.972627899398643e-05, 'epoch': 0.05}
{'loss': 4.1037, 'grad_norm': 0.6851672190342557, 'learning_rate': 9.972479781971147e-05, 'epoch': 0.05}
{'loss': 4.208, 'grad_norm': 0.7954043708256477, 'learning_rate': 9.972331664543651e-05, 'epoch': 0.05}
{'loss': 4.2464, 'grad_norm': 0.989982324874768, 'learning_rate': 9.972183547116154e-05, 'epoch': 0.05}
{'loss': 4.1418, 'grad_norm': 1.216564194360456, 'learning_rate': 9.972035429688658e-05, 'epoch': 0.05}
{'loss': 4.2683, 'grad_norm': 0.7054616280247586, 'learning_rate': 9.971887312261162e-05, 'epoch': 0.05}
{'loss': 4.2661, 'grad_norm': 0.7150923935946459, 'learning_rate': 9.971739194833664e-05, 'epoch': 0.05}
{'loss': 4.2625, 'grad_norm': 0.9254889074009666, 'learning_rate': 9.971591077406168e-05, 'epoch': 0.05}
{'loss': 4.1254, 'grad_norm': 0.7152953970968181, 'learning_rate': 9.971442959978672e-05, 'epoch': 0.05}
{'loss': 4.1327, 'grad_norm': 0.6847274714453235, 'learning_rate': 9.971294842551175e-05, 'epoch': 0.05}
{'loss': 4.2919, 'grad_norm': 0.693952903481393, 'learning_rate': 9.971146725123679e-05, 'epoch': 0.05}
{'loss': 4.121, 'grad_norm': 0.7334029460847943, 'learning_rate': 9.970998607696183e-05, 'epoch': 0.05}
{'loss': 4.1579, 'grad_norm': 0.6673029031569406, 'learning_rate': 9.970850490268685e-05, 'epoch': 0.05}
{'loss': 4.0464, 'grad_norm': 0.7585893337862689, 'learning_rate': 9.970702372841188e-05, 'epoch': 0.05}
{'loss': 3.889, 'grad_norm': 1.286373881271962, 'learning_rate': 9.970554255413692e-05, 'epoch': 0.05}
{'loss': 4.1664, 'grad_norm': 0.6698341659493658, 'learning_rate': 9.970406137986196e-05, 'epoch': 0.05}
[2024-04-12 03:14:01,716] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6000 is about to be saved!
[2024-04-12 03:14:01,722] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-6000/global_step6000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 03:14:01,722] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6000/global_step6000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 03:14:01,728] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6000/global_step6000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 03:14:01,728] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6000/global_step6000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 03:14:04,797] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6000/global_step6000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 03:14:04,797] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-6000/global_step6000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 03:14:04,798] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
{'loss': 4.1356, 'grad_norm': 0.6713225293279392, 'learning_rate': 9.970258020558699e-05, 'epoch': 0.05}
{'loss': 4.2992, 'grad_norm': 0.6722754796056148, 'learning_rate': 9.970109903131203e-05, 'epoch': 0.05}
{'loss': 4.1867, 'grad_norm': 0.7081501051344101, 'learning_rate': 9.969961785703707e-05, 'epoch': 0.05}
{'loss': 4.2181, 'grad_norm': 0.6752527835119247, 'learning_rate': 9.96981366827621e-05, 'epoch': 0.05}
{'loss': 4.3865, 'grad_norm': 0.8279301427093403, 'learning_rate': 9.969665550848713e-05, 'epoch': 0.05}
{'loss': 4.1853, 'grad_norm': 0.6888181518613787, 'learning_rate': 9.969517433421217e-05, 'epoch': 0.05}
{'loss': 4.3382, 'grad_norm': 0.7647861760604828, 'learning_rate': 9.969369315993721e-05, 'epoch': 0.05}
{'loss': 4.0105, 'grad_norm': 0.7325673987690261, 'learning_rate': 9.969221198566224e-05, 'epoch': 0.05}
{'loss': 4.2056, 'grad_norm': 0.8352462977993746, 'learning_rate': 9.969073081138727e-05, 'epoch': 0.05}
{'loss': 4.1824, 'grad_norm': 0.6838395692497887, 'learning_rate': 9.96892496371123e-05, 'epoch': 0.05}
{'loss': 3.7746, 'grad_norm': 2.1854820816538285, 'learning_rate': 9.968776846283733e-05, 'epoch': 0.05}
{'loss': 4.2369, 'grad_norm': 0.6966677819734489, 'learning_rate': 9.968628728856237e-05, 'epoch': 0.05}
{'loss': 4.2422, 'grad_norm': 0.7503775711817973, 'learning_rate': 9.968480611428741e-05, 'epoch': 0.05}
{'loss': 4.2291, 'grad_norm': 0.6674790570923788, 'learning_rate': 9.968332494001245e-05, 'epoch': 0.05}
{'loss': 4.1251, 'grad_norm': 0.7587322277338725, 'learning_rate': 9.968184376573748e-05, 'epoch': 0.05}
{'loss': 4.2717, 'grad_norm': 0.8428650991295774, 'learning_rate': 9.968036259146252e-05, 'epoch': 0.05}
{'loss': 4.0668, 'grad_norm': 1.2929257918059984, 'learning_rate': 9.967888141718756e-05, 'epoch': 0.05}
{'loss': 4.3527, 'grad_norm': 0.8102559585074754, 'learning_rate': 9.967740024291259e-05, 'epoch': 0.05}
{'loss': 4.0438, 'grad_norm': 1.197440026289512, 'learning_rate': 9.967591906863763e-05, 'epoch': 0.05}
{'loss': 4.3371, 'grad_norm': 0.7599105708839984, 'learning_rate': 9.967443789436267e-05, 'epoch': 0.05}
[2024-04-12 03:18:41,997] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6100 is about to be saved!
[2024-04-12 03:18:42,003] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-6100/global_step6100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 03:18:42,003] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6100/global_step6100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 03:18:42,009] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6100/global_step6100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 03:18:42,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6100/global_step6100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 03:18:44,998] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6100/global_step6100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 03:18:44,998] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-6100/global_step6100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 03:18:44,999] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6100 is ready now!
{'loss': 4.2283, 'grad_norm': 0.713471860626589, 'learning_rate': 9.967295672008769e-05, 'epoch': 0.05}
{'loss': 4.2143, 'grad_norm': 0.7132675501460575, 'learning_rate': 9.967147554581272e-05, 'epoch': 0.05}
{'loss': 4.118, 'grad_norm': 0.8002833237230783, 'learning_rate': 9.966999437153776e-05, 'epoch': 0.05}
{'loss': 4.1695, 'grad_norm': 0.6987644181311867, 'learning_rate': 9.96685131972628e-05, 'epoch': 0.05}
{'loss': 4.1476, 'grad_norm': 0.7987317998739321, 'learning_rate': 9.966703202298782e-05, 'epoch': 0.05}
{'loss': 4.3039, 'grad_norm': 0.859490561209877, 'learning_rate': 9.966555084871286e-05, 'epoch': 0.05}
{'loss': 4.3492, 'grad_norm': 0.7389619912337468, 'learning_rate': 9.96640696744379e-05, 'epoch': 0.05}
{'loss': 4.3187, 'grad_norm': 0.7907027956801523, 'learning_rate': 9.966258850016293e-05, 'epoch': 0.05}
{'loss': 4.1194, 'grad_norm': 0.6825052280649947, 'learning_rate': 9.966110732588797e-05, 'epoch': 0.05}
{'loss': 3.9736, 'grad_norm': 0.7266866065686102, 'learning_rate': 9.965962615161301e-05, 'epoch': 0.05}
{'loss': 4.2068, 'grad_norm': 0.707000847452648, 'learning_rate': 9.965814497733804e-05, 'epoch': 0.05}
{'loss': 4.1846, 'grad_norm': 0.6793341513412736, 'learning_rate': 9.965666380306308e-05, 'epoch': 0.05}
{'loss': 3.961, 'grad_norm': 0.7173779742337884, 'learning_rate': 9.965518262878812e-05, 'epoch': 0.05}
{'loss': 4.2699, 'grad_norm': 0.689696600932751, 'learning_rate': 9.965370145451314e-05, 'epoch': 0.05}
{'loss': 4.3046, 'grad_norm': 0.8526848427438652, 'learning_rate': 9.965222028023817e-05, 'epoch': 0.05}
{'loss': 4.1688, 'grad_norm': 0.8134471158730412, 'learning_rate': 9.965073910596321e-05, 'epoch': 0.05}
{'loss': 4.0397, 'grad_norm': 0.8757443889596376, 'learning_rate': 9.964925793168825e-05, 'epoch': 0.05}
{'loss': 4.1109, 'grad_norm': 0.7215461573265625, 'learning_rate': 9.964777675741328e-05, 'epoch': 0.05}
{'loss': 3.9964, 'grad_norm': 0.8700071509950568, 'learning_rate': 9.964629558313832e-05, 'epoch': 0.05}
{'loss': 4.1856, 'grad_norm': 0.6968112414664887, 'learning_rate': 9.964481440886336e-05, 'epoch': 0.05}
[2024-04-12 03:23:22,365] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6200 is about to be saved!
[2024-04-12 03:23:22,370] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-6200/global_step6200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 03:23:22,370] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6200/global_step6200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 03:23:22,376] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6200/global_step6200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 03:23:22,377] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6200/global_step6200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 03:23:25,443] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6200/global_step6200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 03:23:25,444] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-6200/global_step6200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 03:23:25,444] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6200 is ready now!
{'loss': 3.5295, 'grad_norm': 1.313586295270063, 'learning_rate': 9.964333323458838e-05, 'epoch': 0.05}
{'loss': 4.2536, 'grad_norm': 0.7006552904204765, 'learning_rate': 9.964185206031342e-05, 'epoch': 0.05}
{'loss': 4.0863, 'grad_norm': 0.7389678020461405, 'learning_rate': 9.964037088603846e-05, 'epoch': 0.05}
{'loss': 4.2033, 'grad_norm': 0.6817094510221003, 'learning_rate': 9.963888971176349e-05, 'epoch': 0.05}
{'loss': 4.2375, 'grad_norm': 0.7051814423994205, 'learning_rate': 9.963740853748853e-05, 'epoch': 0.05}
{'loss': 3.9803, 'grad_norm': 0.7028840460791703, 'learning_rate': 9.963592736321357e-05, 'epoch': 0.05}
{'loss': 4.1838, 'grad_norm': 0.6994819050334034, 'learning_rate': 9.96344461889386e-05, 'epoch': 0.05}
{'loss': 3.9785, 'grad_norm': 0.7431437048325602, 'learning_rate': 9.963296501466362e-05, 'epoch': 0.05}
{'loss': 4.1241, 'grad_norm': 0.7273863468589412, 'learning_rate': 9.963148384038866e-05, 'epoch': 0.05}
{'loss': 4.1352, 'grad_norm': 0.6915667933718234, 'learning_rate': 9.96300026661137e-05, 'epoch': 0.05}
{'loss': 4.265, 'grad_norm': 0.6920001435423063, 'learning_rate': 9.962852149183873e-05, 'epoch': 0.05}
{'loss': 4.3259, 'grad_norm': 0.9244037026503304, 'learning_rate': 9.962704031756377e-05, 'epoch': 0.05}
{'loss': 4.3007, 'grad_norm': 0.7011489277351879, 'learning_rate': 9.962555914328881e-05, 'epoch': 0.05}
{'loss': 4.2468, 'grad_norm': 0.7808307672797135, 'learning_rate': 9.962407796901383e-05, 'epoch': 0.05}
{'loss': 4.1087, 'grad_norm': 0.8064044149146743, 'learning_rate': 9.962259679473887e-05, 'epoch': 0.05}
{'loss': 3.8682, 'grad_norm': 0.7809986877072775, 'learning_rate': 9.962111562046391e-05, 'epoch': 0.05}
{'loss': 4.1635, 'grad_norm': 0.7075120433034713, 'learning_rate': 9.961963444618895e-05, 'epoch': 0.06}
{'loss': 3.8557, 'grad_norm': 0.694901574661196, 'learning_rate': 9.961815327191398e-05, 'epoch': 0.06}
{'loss': 4.2031, 'grad_norm': 0.8016030006158479, 'learning_rate': 9.961667209763901e-05, 'epoch': 0.06}
{'loss': 3.9435, 'grad_norm': 0.7082419805028095, 'learning_rate': 9.961519092336405e-05, 'epoch': 0.06}
[2024-04-12 03:28:02,803] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6300 is about to be saved!
[2024-04-12 03:28:02,810] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-6300/global_step6300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 03:28:02,810] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6300/global_step6300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 03:28:02,821] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6300/global_step6300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 03:28:02,822] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6300/global_step6300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 03:28:05,839] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6300/global_step6300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 03:28:05,839] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-6300/global_step6300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 03:28:05,840] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6300 is ready now!
{'loss': 4.0187, 'grad_norm': 0.7570522182597225, 'learning_rate': 9.961370974908907e-05, 'epoch': 0.06}
{'loss': 4.1751, 'grad_norm': 0.6934692749714774, 'learning_rate': 9.961222857481411e-05, 'epoch': 0.06}
{'loss': 4.2252, 'grad_norm': 0.6861400704171688, 'learning_rate': 9.961074740053915e-05, 'epoch': 0.06}
{'loss': 3.9427, 'grad_norm': 0.6767555917284575, 'learning_rate': 9.960926622626418e-05, 'epoch': 0.06}
{'loss': 4.2251, 'grad_norm': 0.7472873068653069, 'learning_rate': 9.960778505198922e-05, 'epoch': 0.06}
{'loss': 4.1006, 'grad_norm': 0.7439376073996841, 'learning_rate': 9.960630387771426e-05, 'epoch': 0.06}
{'loss': 4.3266, 'grad_norm': 0.6754131743198346, 'learning_rate': 9.96048227034393e-05, 'epoch': 0.06}
{'loss': 4.2475, 'grad_norm': 0.6724935421839452, 'learning_rate': 9.960334152916433e-05, 'epoch': 0.06}
{'loss': 4.1078, 'grad_norm': 0.696309926019507, 'learning_rate': 9.960186035488937e-05, 'epoch': 0.06}
{'loss': 4.1436, 'grad_norm': 1.076069338437364, 'learning_rate': 9.96003791806144e-05, 'epoch': 0.06}
{'loss': 4.0946, 'grad_norm': 0.7512688422013365, 'learning_rate': 9.959889800633943e-05, 'epoch': 0.06}
{'loss': 3.9578, 'grad_norm': 0.9598271603031834, 'learning_rate': 9.959741683206446e-05, 'epoch': 0.06}
{'loss': 4.2173, 'grad_norm': 0.6822054340024358, 'learning_rate': 9.95959356577895e-05, 'epoch': 0.06}
{'loss': 4.0851, 'grad_norm': 0.7025355178066234, 'learning_rate': 9.959445448351454e-05, 'epoch': 0.06}
{'loss': 4.2014, 'grad_norm': 0.6593324522535055, 'learning_rate': 9.959297330923957e-05, 'epoch': 0.06}
{'loss': 4.1292, 'grad_norm': 0.7469433257853967, 'learning_rate': 9.95914921349646e-05, 'epoch': 0.06}
{'loss': 4.2953, 'grad_norm': 0.7081822479458569, 'learning_rate': 9.959001096068965e-05, 'epoch': 0.06}
{'loss': 4.1061, 'grad_norm': 0.7439488380474129, 'learning_rate': 9.958852978641467e-05, 'epoch': 0.06}
{'loss': 4.0586, 'grad_norm': 0.8482103465181708, 'learning_rate': 9.958704861213971e-05, 'epoch': 0.06}
{'loss': 4.2532, 'grad_norm': 0.7854506317419229, 'learning_rate': 9.958556743786475e-05, 'epoch': 0.06}
[2024-04-12 03:32:43,153] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6400 is about to be saved!
[2024-04-12 03:32:43,158] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-6400/global_step6400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 03:32:43,158] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6400/global_step6400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 03:32:43,164] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6400/global_step6400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 03:32:43,165] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6400/global_step6400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 03:32:46,141] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6400/global_step6400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 03:32:46,141] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-6400/global_step6400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 03:32:46,141] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6400 is ready now!
{'loss': 4.1471, 'grad_norm': 0.6657213158867955, 'learning_rate': 9.958408626358978e-05, 'epoch': 0.06}
{'loss': 4.1926, 'grad_norm': 0.6972277876170071, 'learning_rate': 9.958260508931482e-05, 'epoch': 0.06}
{'loss': 4.1975, 'grad_norm': 0.7123869210344189, 'learning_rate': 9.958112391503986e-05, 'epoch': 0.06}
{'loss': 4.2281, 'grad_norm': 0.7209487516092097, 'learning_rate': 9.957964274076488e-05, 'epoch': 0.06}
{'loss': 4.0594, 'grad_norm': 0.6845408345134524, 'learning_rate': 9.957816156648991e-05, 'epoch': 0.06}
{'loss': 4.0109, 'grad_norm': 0.6705872917737675, 'learning_rate': 9.957668039221495e-05, 'epoch': 0.06}
{'loss': 4.0161, 'grad_norm': 0.6642705018223722, 'learning_rate': 9.957519921793999e-05, 'epoch': 0.06}
{'loss': 4.2247, 'grad_norm': 0.7694330067181837, 'learning_rate': 9.957371804366502e-05, 'epoch': 0.06}
{'loss': 3.9631, 'grad_norm': 0.828403321040942, 'learning_rate': 9.957223686939006e-05, 'epoch': 0.06}
{'loss': 4.2112, 'grad_norm': 0.7022981468070753, 'learning_rate': 9.95707556951151e-05, 'epoch': 0.06}
{'loss': 4.2304, 'grad_norm': 0.6448687231172586, 'learning_rate': 9.956927452084012e-05, 'epoch': 0.06}
{'loss': 4.157, 'grad_norm': 0.6811646911830238, 'learning_rate': 9.956779334656516e-05, 'epoch': 0.06}
{'loss': 3.8439, 'grad_norm': 0.7091645617206463, 'learning_rate': 9.95663121722902e-05, 'epoch': 0.06}
{'loss': 4.0518, 'grad_norm': 0.8020916434404879, 'learning_rate': 9.956483099801523e-05, 'epoch': 0.06}
{'loss': 4.058, 'grad_norm': 0.677024617627375, 'learning_rate': 9.956334982374027e-05, 'epoch': 0.06}
{'loss': 4.159, 'grad_norm': 0.7189503388689176, 'learning_rate': 9.956186864946531e-05, 'epoch': 0.06}
{'loss': 4.089, 'grad_norm': 0.7126643701176253, 'learning_rate': 9.956038747519034e-05, 'epoch': 0.06}
{'loss': 4.1768, 'grad_norm': 0.71265402296017, 'learning_rate': 9.955890630091536e-05, 'epoch': 0.06}
{'loss': 4.0897, 'grad_norm': 0.6534439427888415, 'learning_rate': 9.95574251266404e-05, 'epoch': 0.06}
{'loss': 3.9264, 'grad_norm': 1.2287624127842762, 'learning_rate': 9.955594395236544e-05, 'epoch': 0.06}
[2024-04-12 03:37:23,407] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6500 is about to be saved!
[2024-04-12 03:37:23,413] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-6500/global_step6500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 03:37:23,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6500/global_step6500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 03:37:23,419] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6500/global_step6500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 03:37:23,419] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6500/global_step6500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 03:37:26,475] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6500/global_step6500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 03:37:26,475] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-6500/global_step6500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 03:37:26,475] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6500 is ready now!
{'loss': 3.5163, 'grad_norm': 0.7745564988566261, 'learning_rate': 9.955446277809047e-05, 'epoch': 0.06}
{'loss': 4.1448, 'grad_norm': 0.7624114532459669, 'learning_rate': 9.955298160381551e-05, 'epoch': 0.06}
{'loss': 3.9401, 'grad_norm': 0.7147645884858123, 'learning_rate': 9.955150042954055e-05, 'epoch': 0.06}
{'loss': 4.0704, 'grad_norm': 0.6692417299973167, 'learning_rate': 9.955001925526558e-05, 'epoch': 0.06}
{'loss': 4.0848, 'grad_norm': 0.6644218077213114, 'learning_rate': 9.954853808099062e-05, 'epoch': 0.06}
{'loss': 4.0172, 'grad_norm': 0.8123570780314688, 'learning_rate': 9.954705690671566e-05, 'epoch': 0.06}
{'loss': 4.1273, 'grad_norm': 0.6808554823498765, 'learning_rate': 9.954557573244068e-05, 'epoch': 0.06}
{'loss': 4.2371, 'grad_norm': 0.7447130335883435, 'learning_rate': 9.954409455816572e-05, 'epoch': 0.06}
{'loss': 4.1326, 'grad_norm': 0.7039459582333624, 'learning_rate': 9.954261338389075e-05, 'epoch': 0.06}
{'loss': 4.1883, 'grad_norm': 0.6506981491530981, 'learning_rate': 9.954113220961579e-05, 'epoch': 0.06}
{'loss': 4.0849, 'grad_norm': 0.7074757377330982, 'learning_rate': 9.953965103534081e-05, 'epoch': 0.06}
{'loss': 4.0379, 'grad_norm': 0.6684744467456213, 'learning_rate': 9.953816986106585e-05, 'epoch': 0.06}
{'loss': 3.9585, 'grad_norm': 0.9328274337135009, 'learning_rate': 9.95366886867909e-05, 'epoch': 0.06}
{'loss': 4.1022, 'grad_norm': 0.7365016921843489, 'learning_rate': 9.953520751251592e-05, 'epoch': 0.06}
{'loss': 4.1274, 'grad_norm': 0.7083349424331755, 'learning_rate': 9.953372633824096e-05, 'epoch': 0.06}
{'loss': 4.1676, 'grad_norm': 0.6444554152063793, 'learning_rate': 9.9532245163966e-05, 'epoch': 0.06}
{'loss': 4.1359, 'grad_norm': 0.8074513616502531, 'learning_rate': 9.953076398969103e-05, 'epoch': 0.06}
{'loss': 4.1242, 'grad_norm': 0.6724154717777663, 'learning_rate': 9.952928281541607e-05, 'epoch': 0.06}
{'loss': 4.2044, 'grad_norm': 0.6649186889666205, 'learning_rate': 9.952780164114111e-05, 'epoch': 0.06}
{'loss': 4.0215, 'grad_norm': 0.6437404292541782, 'learning_rate': 9.952632046686615e-05, 'epoch': 0.06}
[2024-04-12 03:42:03,747] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6600 is about to be saved!
[2024-04-12 03:42:03,752] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-6600/global_step6600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 03:42:03,752] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6600/global_step6600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 03:42:03,758] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6600/global_step6600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 03:42:03,758] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6600/global_step6600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 03:42:06,829] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6600/global_step6600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 03:42:06,830] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-6600/global_step6600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 03:42:06,830] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6600 is ready now!
{'loss': 4.1751, 'grad_norm': 0.672744059419674, 'learning_rate': 9.952483929259117e-05, 'epoch': 0.06}
{'loss': 4.0869, 'grad_norm': 1.3979703573599673, 'learning_rate': 9.95233581183162e-05, 'epoch': 0.06}
{'loss': 3.9297, 'grad_norm': 0.7369775482660542, 'learning_rate': 9.952187694404124e-05, 'epoch': 0.06}
{'loss': 4.1804, 'grad_norm': 0.7237723849948353, 'learning_rate': 9.952039576976627e-05, 'epoch': 0.06}
{'loss': 3.9212, 'grad_norm': 0.9713430416089749, 'learning_rate': 9.95189145954913e-05, 'epoch': 0.06}
{'loss': 4.057, 'grad_norm': 0.8717214456637218, 'learning_rate': 9.951743342121635e-05, 'epoch': 0.06}
{'loss': 4.1561, 'grad_norm': 0.7049279291337153, 'learning_rate': 9.951595224694139e-05, 'epoch': 0.06}
{'loss': 4.0304, 'grad_norm': 0.6972655454356376, 'learning_rate': 9.951447107266641e-05, 'epoch': 0.06}
{'loss': 4.2447, 'grad_norm': 0.7161591145815507, 'learning_rate': 9.951298989839145e-05, 'epoch': 0.06}
{'loss': 3.9813, 'grad_norm': 0.7051281315291509, 'learning_rate': 9.951150872411649e-05, 'epoch': 0.06}
{'loss': 3.9628, 'grad_norm': 0.6815060441272767, 'learning_rate': 9.951002754984152e-05, 'epoch': 0.06}
{'loss': 4.0231, 'grad_norm': 1.4189058352744623, 'learning_rate': 9.950854637556656e-05, 'epoch': 0.06}
{'loss': 4.1328, 'grad_norm': 0.7150052742157637, 'learning_rate': 9.95070652012916e-05, 'epoch': 0.06}
{'loss': 4.074, 'grad_norm': 0.6818535234571892, 'learning_rate': 9.950558402701663e-05, 'epoch': 0.06}
{'loss': 4.0975, 'grad_norm': 0.6826852513959965, 'learning_rate': 9.950410285274165e-05, 'epoch': 0.06}
{'loss': 3.676, 'grad_norm': 0.6725861793109857, 'learning_rate': 9.950262167846669e-05, 'epoch': 0.06}
{'loss': 3.9879, 'grad_norm': 0.6629189584227484, 'learning_rate': 9.950114050419173e-05, 'epoch': 0.06}
{'loss': 4.0956, 'grad_norm': 0.6976998071782644, 'learning_rate': 9.949965932991676e-05, 'epoch': 0.06}
{'loss': 4.1752, 'grad_norm': 0.945798620202468, 'learning_rate': 9.94981781556418e-05, 'epoch': 0.06}
{'loss': 4.2518, 'grad_norm': 0.8123035826015503, 'learning_rate': 9.949669698136684e-05, 'epoch': 0.06}
[2024-04-12 03:46:44,199] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6700 is about to be saved!
[2024-04-12 03:46:44,205] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-6700/global_step6700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 03:46:44,205] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6700/global_step6700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 03:46:44,211] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6700/global_step6700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 03:46:44,211] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6700/global_step6700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 03:46:47,228] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6700/global_step6700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 03:46:47,229] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-6700/global_step6700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 03:46:47,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6700 is ready now!
{'loss': 4.1058, 'grad_norm': 0.696838907010864, 'learning_rate': 9.949521580709186e-05, 'epoch': 0.06}
{'loss': 3.6629, 'grad_norm': 0.7482491574126013, 'learning_rate': 9.94937346328169e-05, 'epoch': 0.06}
{'loss': 4.0012, 'grad_norm': 0.6910054017021923, 'learning_rate': 9.949225345854194e-05, 'epoch': 0.06}
{'loss': 4.1337, 'grad_norm': 0.7098431661689969, 'learning_rate': 9.949077228426697e-05, 'epoch': 0.06}
{'loss': 4.186, 'grad_norm': 0.6939166326404063, 'learning_rate': 9.948929110999201e-05, 'epoch': 0.06}
{'loss': 4.1615, 'grad_norm': 0.785349727988538, 'learning_rate': 9.948780993571705e-05, 'epoch': 0.06}
{'loss': 3.8234, 'grad_norm': 0.7824377725955374, 'learning_rate': 9.948632876144208e-05, 'epoch': 0.06}
{'loss': 4.1775, 'grad_norm': 0.733490110182239, 'learning_rate': 9.94848475871671e-05, 'epoch': 0.06}
{'loss': 3.9416, 'grad_norm': 0.6705636868138212, 'learning_rate': 9.948336641289214e-05, 'epoch': 0.06}
{'loss': 4.0533, 'grad_norm': 0.6795720295213794, 'learning_rate': 9.948188523861718e-05, 'epoch': 0.06}
{'loss': 4.2497, 'grad_norm': 0.7197418292510862, 'learning_rate': 9.948040406434221e-05, 'epoch': 0.06}
{'loss': 4.0894, 'grad_norm': 0.6884225477582352, 'learning_rate': 9.947892289006725e-05, 'epoch': 0.06}
{'loss': 3.9916, 'grad_norm': 0.6582202704670972, 'learning_rate': 9.947744171579229e-05, 'epoch': 0.06}
{'loss': 4.1344, 'grad_norm': 0.6700604080336199, 'learning_rate': 9.947596054151732e-05, 'epoch': 0.06}
{'loss': 4.0548, 'grad_norm': 0.7268336297231647, 'learning_rate': 9.947447936724236e-05, 'epoch': 0.06}
{'loss': 4.142, 'grad_norm': 0.6836620030935907, 'learning_rate': 9.94729981929674e-05, 'epoch': 0.06}
{'loss': 4.2669, 'grad_norm': 0.6626084993601167, 'learning_rate': 9.947151701869242e-05, 'epoch': 0.06}
{'loss': 4.0269, 'grad_norm': 0.6707780515031373, 'learning_rate': 9.947003584441746e-05, 'epoch': 0.06}
{'loss': 4.1999, 'grad_norm': 0.6792297181687557, 'learning_rate': 9.946855467014249e-05, 'epoch': 0.06}
{'loss': 4.2338, 'grad_norm': 0.660320088412157, 'learning_rate': 9.946707349586753e-05, 'epoch': 0.06}
[2024-04-12 03:51:24,444] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6800 is about to be saved!
[2024-04-12 03:51:24,449] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-6800/global_step6800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 03:51:24,449] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6800/global_step6800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 03:51:24,455] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6800/global_step6800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 03:51:24,455] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6800/global_step6800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 03:51:27,382] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6800/global_step6800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 03:51:27,382] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-6800/global_step6800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 03:51:27,383] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6800 is ready now!
{'loss': 4.2302, 'grad_norm': 0.6810056794292662, 'learning_rate': 9.946559232159256e-05, 'epoch': 0.06}
{'loss': 3.9191, 'grad_norm': 0.9015299873228331, 'learning_rate': 9.94641111473176e-05, 'epoch': 0.06}
{'loss': 4.1205, 'grad_norm': 0.780562766679768, 'learning_rate': 9.946262997304264e-05, 'epoch': 0.06}
{'loss': 4.2749, 'grad_norm': 0.7592711746827504, 'learning_rate': 9.946114879876766e-05, 'epoch': 0.06}
{'loss': 3.8734, 'grad_norm': 0.8252502761660041, 'learning_rate': 9.94596676244927e-05, 'epoch': 0.06}
{'loss': 4.1738, 'grad_norm': 0.7094277701365372, 'learning_rate': 9.945818645021774e-05, 'epoch': 0.06}
{'loss': 3.831, 'grad_norm': 0.8854031215956606, 'learning_rate': 9.945670527594277e-05, 'epoch': 0.06}
{'loss': 3.9939, 'grad_norm': 0.7668767088822016, 'learning_rate': 9.945522410166781e-05, 'epoch': 0.06}
{'loss': 3.861, 'grad_norm': 0.6488134094232066, 'learning_rate': 9.945374292739285e-05, 'epoch': 0.06}
{'loss': 4.0938, 'grad_norm': 0.6756664266929303, 'learning_rate': 9.945226175311789e-05, 'epoch': 0.06}
{'loss': 3.902, 'grad_norm': 0.6532963158330621, 'learning_rate': 9.94507805788429e-05, 'epoch': 0.06}
{'loss': 4.1762, 'grad_norm': 0.6503629561885964, 'learning_rate': 9.944929940456794e-05, 'epoch': 0.06}
{'loss': 3.855, 'grad_norm': 0.6958944866120302, 'learning_rate': 9.944781823029298e-05, 'epoch': 0.06}
{'loss': 4.0702, 'grad_norm': 0.6632758512033486, 'learning_rate': 9.944633705601801e-05, 'epoch': 0.06}
{'loss': 4.006, 'grad_norm': 0.6691737585779672, 'learning_rate': 9.944485588174305e-05, 'epoch': 0.06}
{'loss': 4.1619, 'grad_norm': 0.798357393424216, 'learning_rate': 9.944337470746809e-05, 'epoch': 0.06}
{'loss': 4.0659, 'grad_norm': 0.6743191175850313, 'learning_rate': 9.944189353319311e-05, 'epoch': 0.06}
{'loss': 4.0548, 'grad_norm': 0.778703817396531, 'learning_rate': 9.944041235891815e-05, 'epoch': 0.06}
{'loss': 3.991, 'grad_norm': 0.7785888377683012, 'learning_rate': 9.94389311846432e-05, 'epoch': 0.06}
{'loss': 4.1401, 'grad_norm': 0.6675241839583296, 'learning_rate': 9.943745001036823e-05, 'epoch': 0.06}
[2024-04-12 03:56:04,697] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6900 is about to be saved!
[2024-04-12 03:56:04,703] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-6900/global_step6900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 03:56:04,703] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6900/global_step6900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 03:56:04,709] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6900/global_step6900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 03:56:04,709] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6900/global_step6900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 03:56:07,735] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6900/global_step6900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 03:56:07,735] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-6900/global_step6900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 03:56:07,736] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6900 is ready now!
{'loss': 4.1331, 'grad_norm': 0.7229372626042809, 'learning_rate': 9.943596883609326e-05, 'epoch': 0.06}
{'loss': 4.0102, 'grad_norm': 0.7105652903967609, 'learning_rate': 9.94344876618183e-05, 'epoch': 0.06}
{'loss': 3.8399, 'grad_norm': 0.8355056959339696, 'learning_rate': 9.943300648754334e-05, 'epoch': 0.06}
{'loss': 3.7084, 'grad_norm': 1.7074548440321904, 'learning_rate': 9.943152531326835e-05, 'epoch': 0.06}
{'loss': 3.9755, 'grad_norm': 0.6858619899313038, 'learning_rate': 9.943004413899339e-05, 'epoch': 0.06}
{'loss': 3.9551, 'grad_norm': 0.6924532540169326, 'learning_rate': 9.942856296471843e-05, 'epoch': 0.06}
{'loss': 2.8025, 'grad_norm': 0.7247471262250823, 'learning_rate': 9.942708179044347e-05, 'epoch': 0.06}
{'loss': 4.2391, 'grad_norm': 0.6525464913937674, 'learning_rate': 9.94256006161685e-05, 'epoch': 0.06}
{'loss': 4.1028, 'grad_norm': 0.683130438841653, 'learning_rate': 9.942411944189354e-05, 'epoch': 0.06}
{'loss': 4.1246, 'grad_norm': 0.669111272037735, 'learning_rate': 9.942263826761858e-05, 'epoch': 0.06}
{'loss': 3.8632, 'grad_norm': 0.8047291233393126, 'learning_rate': 9.94211570933436e-05, 'epoch': 0.06}
{'loss': 3.8875, 'grad_norm': 0.7145267365870133, 'learning_rate': 9.941967591906865e-05, 'epoch': 0.06}
{'loss': 3.9949, 'grad_norm': 0.7277998867158628, 'learning_rate': 9.941819474479369e-05, 'epoch': 0.06}
{'loss': 3.9659, 'grad_norm': 0.6782749414932395, 'learning_rate': 9.941671357051871e-05, 'epoch': 0.06}
{'loss': 4.0622, 'grad_norm': 0.7563198737021883, 'learning_rate': 9.941523239624375e-05, 'epoch': 0.06}
{'loss': 3.6633, 'grad_norm': 0.686571168486308, 'learning_rate': 9.941375122196878e-05, 'epoch': 0.06}
{'loss': 3.9825, 'grad_norm': 0.6942573288942437, 'learning_rate': 9.941227004769382e-05, 'epoch': 0.06}
{'loss': 4.1733, 'grad_norm': 0.7094322869033726, 'learning_rate': 9.941078887341884e-05, 'epoch': 0.06}
{'loss': 4.1904, 'grad_norm': 0.6657468171946412, 'learning_rate': 9.940930769914388e-05, 'epoch': 0.06}
{'loss': 4.0478, 'grad_norm': 0.6996916117572227, 'learning_rate': 9.940782652486892e-05, 'epoch': 0.06}
[2024-04-12 04:00:45,234] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7000 is about to be saved!
[2024-04-12 04:00:45,239] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-7000/global_step7000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 04:00:45,239] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7000/global_step7000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 04:00:45,245] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7000/global_step7000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 04:00:45,246] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7000/global_step7000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 04:00:48,195] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7000/global_step7000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 04:00:48,196] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-7000/global_step7000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 04:00:48,196] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
{'loss': 3.9165, 'grad_norm': 0.7183184611405927, 'learning_rate': 9.940634535059395e-05, 'epoch': 0.06}
{'loss': 4.0584, 'grad_norm': 0.6795660515009865, 'learning_rate': 9.940486417631899e-05, 'epoch': 0.06}
{'loss': 4.2058, 'grad_norm': 0.7005238029174128, 'learning_rate': 9.940338300204403e-05, 'epoch': 0.06}
{'loss': 3.9638, 'grad_norm': 0.7278601765159001, 'learning_rate': 9.940190182776906e-05, 'epoch': 0.06}
{'loss': 4.0115, 'grad_norm': 0.6698413308946218, 'learning_rate': 9.94004206534941e-05, 'epoch': 0.06}
{'loss': 3.9226, 'grad_norm': 0.6884034278235676, 'learning_rate': 9.939893947921914e-05, 'epoch': 0.06}
{'loss': 3.4531, 'grad_norm': 0.6873217420554956, 'learning_rate': 9.939745830494416e-05, 'epoch': 0.06}
{'loss': 3.9333, 'grad_norm': 0.6886955858223194, 'learning_rate': 9.93959771306692e-05, 'epoch': 0.06}
{'loss': 4.1876, 'grad_norm': 0.7255255488726056, 'learning_rate': 9.939449595639423e-05, 'epoch': 0.06}
{'loss': 4.076, 'grad_norm': 0.6837979923078591, 'learning_rate': 9.939301478211927e-05, 'epoch': 0.06}
{'loss': 3.9616, 'grad_norm': 0.7120126169763002, 'learning_rate': 9.93915336078443e-05, 'epoch': 0.06}
{'loss': 3.8553, 'grad_norm': 0.6543430361804581, 'learning_rate': 9.939005243356934e-05, 'epoch': 0.06}
{'loss': 3.9624, 'grad_norm': 0.6686633409114882, 'learning_rate': 9.938857125929438e-05, 'epoch': 0.06}
{'loss': 4.1627, 'grad_norm': 0.6857087951040346, 'learning_rate': 9.93870900850194e-05, 'epoch': 0.06}
{'loss': 4.0617, 'grad_norm': 0.6475523813074988, 'learning_rate': 9.938560891074444e-05, 'epoch': 0.06}
{'loss': 4.1873, 'grad_norm': 0.8108781422366665, 'learning_rate': 9.938412773646948e-05, 'epoch': 0.06}
{'loss': 4.1347, 'grad_norm': 0.7758058271515614, 'learning_rate': 9.938264656219451e-05, 'epoch': 0.06}
{'loss': 3.9235, 'grad_norm': 0.8313530702632046, 'learning_rate': 9.938116538791955e-05, 'epoch': 0.06}
{'loss': 3.9011, 'grad_norm': 0.6924124205033633, 'learning_rate': 9.937968421364459e-05, 'epoch': 0.06}
{'loss': 3.9333, 'grad_norm': 0.7124998367103493, 'learning_rate': 9.937820303936962e-05, 'epoch': 0.06}
[2024-04-12 04:05:25,581] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7100 is about to be saved!
[2024-04-12 04:05:25,586] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-7100/global_step7100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 04:05:25,586] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7100/global_step7100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 04:05:25,592] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7100/global_step7100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 04:05:25,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7100/global_step7100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 04:05:28,556] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7100/global_step7100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 04:05:28,556] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-7100/global_step7100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 04:05:28,557] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7100 is ready now!
{'loss': 3.986, 'grad_norm': 0.7084148075371022, 'learning_rate': 9.937672186509464e-05, 'epoch': 0.06}
{'loss': 3.9034, 'grad_norm': 0.7142432965628539, 'learning_rate': 9.937524069081968e-05, 'epoch': 0.06}
{'loss': 4.1038, 'grad_norm': 0.6272912670229, 'learning_rate': 9.937375951654472e-05, 'epoch': 0.06}
{'loss': 3.966, 'grad_norm': 0.6833570855247585, 'learning_rate': 9.937227834226975e-05, 'epoch': 0.06}
{'loss': 4.0814, 'grad_norm': 0.6652175453401213, 'learning_rate': 9.937079716799479e-05, 'epoch': 0.06}
{'loss': 4.0937, 'grad_norm': 0.6500778532560948, 'learning_rate': 9.936931599371983e-05, 'epoch': 0.06}
{'loss': 4.1384, 'grad_norm': 0.6609042570608757, 'learning_rate': 9.936783481944485e-05, 'epoch': 0.06}
{'loss': 3.9909, 'grad_norm': 0.6956795837888031, 'learning_rate': 9.93663536451699e-05, 'epoch': 0.06}
{'loss': 4.0467, 'grad_norm': 0.6639022104047414, 'learning_rate': 9.936487247089493e-05, 'epoch': 0.06}
{'loss': 4.1451, 'grad_norm': 0.6543371826772857, 'learning_rate': 9.936339129661997e-05, 'epoch': 0.06}
{'loss': 4.0502, 'grad_norm': 0.7337535483805555, 'learning_rate': 9.9361910122345e-05, 'epoch': 0.06}
{'loss': 4.2617, 'grad_norm': 0.6603599164217357, 'learning_rate': 9.936042894807004e-05, 'epoch': 0.06}
{'loss': 3.8604, 'grad_norm': 0.622752857573678, 'learning_rate': 9.935894777379508e-05, 'epoch': 0.06}
{'loss': 4.0788, 'grad_norm': 0.6726696052866404, 'learning_rate': 9.93574665995201e-05, 'epoch': 0.06}
{'loss': 4.1368, 'grad_norm': 0.6721959044179547, 'learning_rate': 9.935598542524513e-05, 'epoch': 0.06}
{'loss': 4.1279, 'grad_norm': 0.718309467986906, 'learning_rate': 9.935450425097017e-05, 'epoch': 0.06}
{'loss': 4.0902, 'grad_norm': 0.7039135163434243, 'learning_rate': 9.93530230766952e-05, 'epoch': 0.06}
{'loss': 3.8434, 'grad_norm': 0.9964227387886603, 'learning_rate': 9.935154190242024e-05, 'epoch': 0.06}
{'loss': 4.1348, 'grad_norm': 0.6966057683416472, 'learning_rate': 9.935006072814528e-05, 'epoch': 0.06}
{'loss': 3.9974, 'grad_norm': 0.6957966494248246, 'learning_rate': 9.934857955387032e-05, 'epoch': 0.06}
[2024-04-12 04:10:05,985] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7200 is about to be saved!
[2024-04-12 04:10:05,990] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-7200/global_step7200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 04:10:05,990] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7200/global_step7200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 04:10:05,996] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7200/global_step7200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 04:10:05,996] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7200/global_step7200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 04:10:09,073] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7200/global_step7200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 04:10:09,073] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-7200/global_step7200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 04:10:09,074] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7200 is ready now!
{'loss': 4.0909, 'grad_norm': 0.670515479485408, 'learning_rate': 9.934709837959535e-05, 'epoch': 0.06}
{'loss': 4.159, 'grad_norm': 0.6910784012083001, 'learning_rate': 9.934561720532039e-05, 'epoch': 0.06}
{'loss': 4.0456, 'grad_norm': 0.6575336067343782, 'learning_rate': 9.934413603104543e-05, 'epoch': 0.06}
{'loss': 4.0683, 'grad_norm': 0.6448425808541632, 'learning_rate': 9.934265485677045e-05, 'epoch': 0.06}
{'loss': 3.8426, 'grad_norm': 0.6955675129011352, 'learning_rate': 9.934117368249549e-05, 'epoch': 0.06}
{'loss': 3.9905, 'grad_norm': 0.6652984240968871, 'learning_rate': 9.933969250822052e-05, 'epoch': 0.06}
{'loss': 4.0634, 'grad_norm': 0.6677444880944196, 'learning_rate': 9.933821133394556e-05, 'epoch': 0.06}
{'loss': 4.029, 'grad_norm': 0.6735309749235974, 'learning_rate': 9.933673015967059e-05, 'epoch': 0.06}
{'loss': 4.013, 'grad_norm': 1.0138114859982832, 'learning_rate': 9.933524898539563e-05, 'epoch': 0.06}
{'loss': 3.8467, 'grad_norm': 0.7266386663584777, 'learning_rate': 9.933376781112067e-05, 'epoch': 0.06}
{'loss': 4.086, 'grad_norm': 0.7148659942821922, 'learning_rate': 9.933228663684569e-05, 'epoch': 0.06}
{'loss': 4.0488, 'grad_norm': 0.6507475814481045, 'learning_rate': 9.933080546257073e-05, 'epoch': 0.06}
{'loss': 4.1532, 'grad_norm': 0.798901714395155, 'learning_rate': 9.932932428829577e-05, 'epoch': 0.06}
{'loss': 4.0407, 'grad_norm': 0.6321783327627402, 'learning_rate': 9.93278431140208e-05, 'epoch': 0.06}
{'loss': 3.9433, 'grad_norm': 0.6520190357156227, 'learning_rate': 9.932636193974584e-05, 'epoch': 0.06}
{'loss': 4.0402, 'grad_norm': 0.6963044821907247, 'learning_rate': 9.932488076547088e-05, 'epoch': 0.06}
{'loss': 3.9753, 'grad_norm': 0.7016049067858567, 'learning_rate': 9.93233995911959e-05, 'epoch': 0.06}
{'loss': 3.9267, 'grad_norm': 0.8051908801413246, 'learning_rate': 9.932191841692094e-05, 'epoch': 0.06}
{'loss': 4.2, 'grad_norm': 0.6771631748437804, 'learning_rate': 9.932043724264597e-05, 'epoch': 0.06}
{'loss': 3.9609, 'grad_norm': 0.7636634457715765, 'learning_rate': 9.931895606837101e-05, 'epoch': 0.06}
[2024-04-12 04:14:46,341] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7300 is about to be saved!
[2024-04-12 04:14:46,346] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-7300/global_step7300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 04:14:46,346] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7300/global_step7300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 04:14:46,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7300/global_step7300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 04:14:46,352] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7300/global_step7300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 04:14:49,314] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7300/global_step7300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 04:14:49,314] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-7300/global_step7300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 04:14:49,314] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7300 is ready now!
{'loss': 4.1097, 'grad_norm': 0.7324620727127275, 'learning_rate': 9.931747489409604e-05, 'epoch': 0.06}
{'loss': 4.1928, 'grad_norm': 0.7165117898263045, 'learning_rate': 9.931599371982108e-05, 'epoch': 0.06}
{'loss': 4.0514, 'grad_norm': 0.8182932917550959, 'learning_rate': 9.931451254554612e-05, 'epoch': 0.06}
{'loss': 4.1579, 'grad_norm': 0.6979379858185975, 'learning_rate': 9.931303137127114e-05, 'epoch': 0.06}
{'loss': 3.9775, 'grad_norm': 0.6486057100383672, 'learning_rate': 9.931155019699618e-05, 'epoch': 0.06}
{'loss': 3.9999, 'grad_norm': 0.6931987624523289, 'learning_rate': 9.931006902272122e-05, 'epoch': 0.06}
{'loss': 4.0069, 'grad_norm': 0.6383517992069014, 'learning_rate': 9.930858784844625e-05, 'epoch': 0.06}
{'loss': 4.0915, 'grad_norm': 0.7318102215001699, 'learning_rate': 9.930710667417129e-05, 'epoch': 0.06}
{'loss': 3.9781, 'grad_norm': 0.7145109625818953, 'learning_rate': 9.930562549989633e-05, 'epoch': 0.06}
{'loss': 4.0891, 'grad_norm': 0.6822414871963656, 'learning_rate': 9.930414432562136e-05, 'epoch': 0.06}
{'loss': 3.8992, 'grad_norm': 0.6460923073245914, 'learning_rate': 9.930266315134638e-05, 'epoch': 0.06}
{'loss': 4.1385, 'grad_norm': 0.67415650025617, 'learning_rate': 9.930118197707142e-05, 'epoch': 0.06}
{'loss': 3.7455, 'grad_norm': 0.708506420617478, 'learning_rate': 9.929970080279646e-05, 'epoch': 0.06}
{'loss': 4.0004, 'grad_norm': 0.7100652813303105, 'learning_rate': 9.929821962852149e-05, 'epoch': 0.06}
{'loss': 3.939, 'grad_norm': 0.6767046699056747, 'learning_rate': 9.929673845424653e-05, 'epoch': 0.06}
{'loss': 3.3937, 'grad_norm': 1.1925885372168998, 'learning_rate': 9.929525727997157e-05, 'epoch': 0.06}
{'loss': 3.8725, 'grad_norm': 0.6639033480404388, 'learning_rate': 9.92937761056966e-05, 'epoch': 0.06}
{'loss': 4.2167, 'grad_norm': 0.712834603722038, 'learning_rate': 9.929229493142164e-05, 'epoch': 0.06}
{'loss': 4.1595, 'grad_norm': 0.7583085394814176, 'learning_rate': 9.929081375714668e-05, 'epoch': 0.06}
{'loss': 3.7769, 'grad_norm': 0.6385858823250306, 'learning_rate': 9.92893325828717e-05, 'epoch': 0.06}
[2024-04-12 04:19:26,669] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7400 is about to be saved!
[2024-04-12 04:19:26,675] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-7400/global_step7400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 04:19:26,675] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7400/global_step7400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 04:19:26,681] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7400/global_step7400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 04:19:26,681] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7400/global_step7400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 04:19:29,622] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7400/global_step7400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 04:19:29,622] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-7400/global_step7400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 04:19:29,623] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7400 is ready now!
{'loss': 4.2688, 'grad_norm': 0.7419584921328981, 'learning_rate': 9.928785140859674e-05, 'epoch': 0.06}
{'loss': 3.7819, 'grad_norm': 0.8235772235603801, 'learning_rate': 9.928637023432178e-05, 'epoch': 0.06}
{'loss': 3.9618, 'grad_norm': 0.678843718759161, 'learning_rate': 9.928488906004682e-05, 'epoch': 0.06}
{'loss': 3.9673, 'grad_norm': 0.6791642548330009, 'learning_rate': 9.928340788577183e-05, 'epoch': 0.06}
{'loss': 4.1424, 'grad_norm': 0.6965406912179077, 'learning_rate': 9.928192671149687e-05, 'epoch': 0.07}
{'loss': 4.3054, 'grad_norm': 0.731348654717148, 'learning_rate': 9.928044553722191e-05, 'epoch': 0.07}
{'loss': 4.0078, 'grad_norm': 0.6691346653159599, 'learning_rate': 9.927896436294694e-05, 'epoch': 0.07}
{'loss': 4.053, 'grad_norm': 0.6958092424306203, 'learning_rate': 9.927748318867198e-05, 'epoch': 0.07}
{'loss': 4.0379, 'grad_norm': 0.6883401622081594, 'learning_rate': 9.927600201439702e-05, 'epoch': 0.07}
{'loss': 4.1154, 'grad_norm': 0.6912026130300204, 'learning_rate': 9.927452084012205e-05, 'epoch': 0.07}
{'loss': 4.152, 'grad_norm': 0.6879511828539914, 'learning_rate': 9.927303966584709e-05, 'epoch': 0.07}
{'loss': 3.5213, 'grad_norm': 0.8774804462976462, 'learning_rate': 9.927155849157213e-05, 'epoch': 0.07}
{'loss': 3.8291, 'grad_norm': 0.6774825244698325, 'learning_rate': 9.927007731729717e-05, 'epoch': 0.07}
{'loss': 3.6941, 'grad_norm': 0.8354671921818122, 'learning_rate': 9.92685961430222e-05, 'epoch': 0.07}
{'loss': 4.0552, 'grad_norm': 0.6602065938717466, 'learning_rate': 9.926711496874723e-05, 'epoch': 0.07}
{'loss': 3.9336, 'grad_norm': 0.6316504547230294, 'learning_rate': 9.926563379447226e-05, 'epoch': 0.07}
{'loss': 3.8584, 'grad_norm': 0.7402540671410015, 'learning_rate': 9.926415262019729e-05, 'epoch': 0.07}
{'loss': 4.0338, 'grad_norm': 0.7653782985099497, 'learning_rate': 9.926267144592233e-05, 'epoch': 0.07}
{'loss': 4.0339, 'grad_norm': 0.6653369022487919, 'learning_rate': 9.926119027164737e-05, 'epoch': 0.07}
{'loss': 3.9805, 'grad_norm': 0.6865412671055889, 'learning_rate': 9.92597090973724e-05, 'epoch': 0.07}
[2024-04-12 04:24:07,143] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7500 is about to be saved!
[2024-04-12 04:24:07,148] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-7500/global_step7500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 04:24:07,148] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7500/global_step7500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 04:24:07,154] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7500/global_step7500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 04:24:07,154] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7500/global_step7500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 04:24:10,152] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7500/global_step7500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 04:24:10,152] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-7500/global_step7500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 04:24:10,153] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7500 is ready now!
{'loss': 4.0466, 'grad_norm': 0.6744362923983325, 'learning_rate': 9.925822792309743e-05, 'epoch': 0.07}
{'loss': 3.9595, 'grad_norm': 0.6294354867085604, 'learning_rate': 9.925674674882247e-05, 'epoch': 0.07}
{'loss': 3.8251, 'grad_norm': 0.6231083313803991, 'learning_rate': 9.925526557454751e-05, 'epoch': 0.07}
{'loss': 4.1296, 'grad_norm': 0.6662745469115396, 'learning_rate': 9.925378440027254e-05, 'epoch': 0.07}
{'loss': 3.9653, 'grad_norm': 0.739123688527664, 'learning_rate': 9.925230322599758e-05, 'epoch': 0.07}
{'loss': 3.9749, 'grad_norm': 0.6656708489884025, 'learning_rate': 9.925082205172262e-05, 'epoch': 0.07}
{'loss': 4.056, 'grad_norm': 0.6596589067956021, 'learning_rate': 9.924934087744765e-05, 'epoch': 0.07}
{'loss': 4.0032, 'grad_norm': 0.6519474273785653, 'learning_rate': 9.924785970317269e-05, 'epoch': 0.07}
{'loss': 4.0623, 'grad_norm': 0.6605850886077343, 'learning_rate': 9.924637852889771e-05, 'epoch': 0.07}
{'loss': 3.8723, 'grad_norm': 0.7437454328140997, 'learning_rate': 9.924489735462275e-05, 'epoch': 0.07}
{'loss': 4.1166, 'grad_norm': 0.715827407823945, 'learning_rate': 9.924341618034778e-05, 'epoch': 0.07}
{'loss': 4.0591, 'grad_norm': 0.6348526906197852, 'learning_rate': 9.924193500607282e-05, 'epoch': 0.07}
{'loss': 3.7735, 'grad_norm': 0.8059175271022978, 'learning_rate': 9.924045383179786e-05, 'epoch': 0.07}
{'loss': 3.8633, 'grad_norm': 0.6533518833219881, 'learning_rate': 9.923897265752288e-05, 'epoch': 0.07}
{'loss': 3.963, 'grad_norm': 0.7387234112559612, 'learning_rate': 9.923749148324792e-05, 'epoch': 0.07}
{'loss': 4.214, 'grad_norm': 0.6553470287861733, 'learning_rate': 9.923601030897296e-05, 'epoch': 0.07}
{'loss': 3.9586, 'grad_norm': 0.6900459899085215, 'learning_rate': 9.923452913469799e-05, 'epoch': 0.07}
{'loss': 4.0875, 'grad_norm': 0.6776427394067018, 'learning_rate': 9.923304796042303e-05, 'epoch': 0.07}
{'loss': 4.0651, 'grad_norm': 0.6527188607304932, 'learning_rate': 9.923156678614807e-05, 'epoch': 0.07}
{'loss': 4.0539, 'grad_norm': 0.6102408937070275, 'learning_rate': 9.92300856118731e-05, 'epoch': 0.07}
[2024-04-12 04:28:47,590] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7600 is about to be saved!
[2024-04-12 04:28:47,595] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-7600/global_step7600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 04:28:47,595] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7600/global_step7600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 04:28:47,601] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7600/global_step7600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 04:28:47,602] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7600/global_step7600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 04:28:50,682] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7600/global_step7600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 04:28:50,682] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-7600/global_step7600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 04:28:50,683] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7600 is ready now!
{'loss': 4.058, 'grad_norm': 0.6768057289328262, 'learning_rate': 9.922860443759812e-05, 'epoch': 0.07}
{'loss': 3.9105, 'grad_norm': 0.7008227952224136, 'learning_rate': 9.922712326332316e-05, 'epoch': 0.07}
{'loss': 3.9895, 'grad_norm': 0.6383400643780077, 'learning_rate': 9.92256420890482e-05, 'epoch': 0.07}
{'loss': 4.0643, 'grad_norm': 0.7146531606499374, 'learning_rate': 9.922416091477323e-05, 'epoch': 0.07}
{'loss': 3.8569, 'grad_norm': 0.7028486033216241, 'learning_rate': 9.922267974049827e-05, 'epoch': 0.07}
{'loss': 4.0962, 'grad_norm': 0.7021337603478149, 'learning_rate': 9.922119856622331e-05, 'epoch': 0.07}
{'loss': 3.8997, 'grad_norm': 0.826018563053883, 'learning_rate': 9.921971739194834e-05, 'epoch': 0.07}
{'loss': 3.9142, 'grad_norm': 0.6599500237570647, 'learning_rate': 9.921823621767338e-05, 'epoch': 0.07}
{'loss': 3.9716, 'grad_norm': 0.6530864797927793, 'learning_rate': 9.921675504339842e-05, 'epoch': 0.07}
{'loss': 3.7938, 'grad_norm': 0.7012171219990938, 'learning_rate': 9.921527386912344e-05, 'epoch': 0.07}
{'loss': 4.0588, 'grad_norm': 0.6788298962777057, 'learning_rate': 9.921379269484848e-05, 'epoch': 0.07}
{'loss': 1.7136, 'grad_norm': 1.5661693694611714, 'learning_rate': 9.921231152057352e-05, 'epoch': 0.07}
{'loss': 3.948, 'grad_norm': 0.8038587823762202, 'learning_rate': 9.921083034629855e-05, 'epoch': 0.07}
{'loss': 3.9599, 'grad_norm': 0.6649280993762131, 'learning_rate': 9.920934917202358e-05, 'epoch': 0.07}
{'loss': 4.0961, 'grad_norm': 0.6879205683079619, 'learning_rate': 9.920786799774862e-05, 'epoch': 0.07}
{'loss': 4.0337, 'grad_norm': 0.6687976024774691, 'learning_rate': 9.920638682347366e-05, 'epoch': 0.07}
{'loss': 3.9971, 'grad_norm': 0.8155231340433137, 'learning_rate': 9.920490564919868e-05, 'epoch': 0.07}
{'loss': 4.0252, 'grad_norm': 0.6554334487061889, 'learning_rate': 9.920342447492372e-05, 'epoch': 0.07}
{'loss': 3.8566, 'grad_norm': 0.7159216188769022, 'learning_rate': 9.920194330064876e-05, 'epoch': 0.07}
{'loss': 4.0614, 'grad_norm': 0.9689436360669115, 'learning_rate': 9.920046212637379e-05, 'epoch': 0.07}
[2024-04-12 04:33:28,097] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7700 is about to be saved!
[2024-04-12 04:33:28,102] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-7700/global_step7700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 04:33:28,103] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7700/global_step7700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 04:33:28,109] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7700/global_step7700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 04:33:28,109] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7700/global_step7700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 04:33:31,182] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7700/global_step7700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 04:33:31,182] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-7700/global_step7700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 04:33:31,182] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7700 is ready now!
{'loss': 4.0575, 'grad_norm': 0.6356937718304572, 'learning_rate': 9.919898095209883e-05, 'epoch': 0.07}
{'loss': 3.9704, 'grad_norm': 0.6850902740473913, 'learning_rate': 9.919749977782387e-05, 'epoch': 0.07}
{'loss': 3.6867, 'grad_norm': 1.950003942779168, 'learning_rate': 9.919601860354891e-05, 'epoch': 0.07}
{'loss': 3.448, 'grad_norm': 0.6976563337202254, 'learning_rate': 9.919453742927393e-05, 'epoch': 0.07}
{'loss': 3.9164, 'grad_norm': 0.6963203803931075, 'learning_rate': 9.919305625499897e-05, 'epoch': 0.07}
{'loss': 3.9179, 'grad_norm': 0.6889010763204308, 'learning_rate': 9.9191575080724e-05, 'epoch': 0.07}
{'loss': 4.0247, 'grad_norm': 0.6622650571339199, 'learning_rate': 9.919009390644903e-05, 'epoch': 0.07}
{'loss': 3.7703, 'grad_norm': 0.6827500982290375, 'learning_rate': 9.918861273217407e-05, 'epoch': 0.07}
{'loss': 4.1354, 'grad_norm': 0.6277790337660742, 'learning_rate': 9.918713155789911e-05, 'epoch': 0.07}
{'loss': 3.5689, 'grad_norm': 1.2514620367670897, 'learning_rate': 9.918565038362413e-05, 'epoch': 0.07}
{'loss': 3.4062, 'grad_norm': 0.6999883199630407, 'learning_rate': 9.918416920934917e-05, 'epoch': 0.07}
{'loss': 4.015, 'grad_norm': 0.6855315506042567, 'learning_rate': 9.918268803507421e-05, 'epoch': 0.07}
{'loss': 3.8734, 'grad_norm': 0.6722964525037197, 'learning_rate': 9.918120686079925e-05, 'epoch': 0.07}
{'loss': 4.0366, 'grad_norm': 0.6506190257603502, 'learning_rate': 9.917972568652428e-05, 'epoch': 0.07}
{'loss': 3.9218, 'grad_norm': 0.661139441158965, 'learning_rate': 9.917824451224932e-05, 'epoch': 0.07}
{'loss': 4.0097, 'grad_norm': 0.6423341442892674, 'learning_rate': 9.917676333797436e-05, 'epoch': 0.07}
{'loss': 3.7718, 'grad_norm': 0.6712438848187837, 'learning_rate': 9.917528216369939e-05, 'epoch': 0.07}
{'loss': 4.1789, 'grad_norm': 0.6952791487644381, 'learning_rate': 9.917380098942443e-05, 'epoch': 0.07}
{'loss': 3.9001, 'grad_norm': 0.6508459573191063, 'learning_rate': 9.917231981514945e-05, 'epoch': 0.07}
{'loss': 4.0541, 'grad_norm': 0.668393290048442, 'learning_rate': 9.917083864087449e-05, 'epoch': 0.07}
[2024-04-12 04:38:08,488] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7800 is about to be saved!
[2024-04-12 04:38:08,493] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-7800/global_step7800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 04:38:08,493] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7800/global_step7800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 04:38:08,499] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7800/global_step7800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 04:38:08,499] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7800/global_step7800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 04:38:11,585] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7800/global_step7800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 04:38:11,585] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-7800/global_step7800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 04:38:11,586] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7800 is ready now!
{'loss': 3.9766, 'grad_norm': 0.6983420206767141, 'learning_rate': 9.916935746659952e-05, 'epoch': 0.07}
{'loss': 3.7983, 'grad_norm': 0.6704890980540494, 'learning_rate': 9.916787629232456e-05, 'epoch': 0.07}
{'loss': 4.006, 'grad_norm': 0.7141708292025204, 'learning_rate': 9.91663951180496e-05, 'epoch': 0.07}
{'loss': 3.9891, 'grad_norm': 0.6737283872099977, 'learning_rate': 9.916491394377463e-05, 'epoch': 0.07}
{'loss': 3.8916, 'grad_norm': 0.6512262763853324, 'learning_rate': 9.916343276949967e-05, 'epoch': 0.07}
{'loss': 3.883, 'grad_norm': 0.6478130988555901, 'learning_rate': 9.91619515952247e-05, 'epoch': 0.07}
{'loss': 4.0376, 'grad_norm': 0.6352536861065304, 'learning_rate': 9.916047042094973e-05, 'epoch': 0.07}
{'loss': 4.212, 'grad_norm': 0.7282715454703464, 'learning_rate': 9.915898924667477e-05, 'epoch': 0.07}
{'loss': 4.0369, 'grad_norm': 0.6573305529321216, 'learning_rate': 9.915750807239981e-05, 'epoch': 0.07}
{'loss': 4.0379, 'grad_norm': 0.6793656783128909, 'learning_rate': 9.915602689812484e-05, 'epoch': 0.07}
{'loss': 3.9016, 'grad_norm': 0.7149188908905237, 'learning_rate': 9.915454572384986e-05, 'epoch': 0.07}
{'loss': 3.693, 'grad_norm': 0.6615506695517573, 'learning_rate': 9.91530645495749e-05, 'epoch': 0.07}
{'loss': 4.0556, 'grad_norm': 0.653743427669142, 'learning_rate': 9.915158337529994e-05, 'epoch': 0.07}
{'loss': 4.0739, 'grad_norm': 0.6476788759647479, 'learning_rate': 9.915010220102497e-05, 'epoch': 0.07}
{'loss': 3.9819, 'grad_norm': 0.7506204458867392, 'learning_rate': 9.914862102675001e-05, 'epoch': 0.07}
{'loss': 4.0654, 'grad_norm': 0.6308727404648256, 'learning_rate': 9.914713985247505e-05, 'epoch': 0.07}
{'loss': 3.9993, 'grad_norm': 0.8008912203695798, 'learning_rate': 9.914565867820008e-05, 'epoch': 0.07}
{'loss': 4.0931, 'grad_norm': 0.6750700493322871, 'learning_rate': 9.914417750392512e-05, 'epoch': 0.07}
{'loss': 4.0458, 'grad_norm': 0.6592624565844052, 'learning_rate': 9.914269632965016e-05, 'epoch': 0.07}
{'loss': 4.2005, 'grad_norm': 0.6468891647846897, 'learning_rate': 9.914121515537518e-05, 'epoch': 0.07}
[2024-04-12 04:42:49,100] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7900 is about to be saved!
[2024-04-12 04:42:49,106] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-7900/global_step7900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 04:42:49,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7900/global_step7900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 04:42:49,112] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7900/global_step7900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 04:42:49,112] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-7900/global_step7900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 04:42:52,094] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-7900/global_step7900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 04:42:52,095] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-7900/global_step7900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 04:42:52,095] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7900 is ready now!
{'loss': 4.0002, 'grad_norm': 0.6911555251651255, 'learning_rate': 9.913973398110022e-05, 'epoch': 0.07}
{'loss': 3.8405, 'grad_norm': 0.6532678018801106, 'learning_rate': 9.913825280682526e-05, 'epoch': 0.07}
{'loss': 3.8119, 'grad_norm': 0.7205052517776017, 'learning_rate': 9.913677163255029e-05, 'epoch': 0.07}
{'loss': 3.9944, 'grad_norm': 0.7233436424223391, 'learning_rate': 9.913529045827532e-05, 'epoch': 0.07}
{'loss': 4.1288, 'grad_norm': 0.6348730270164603, 'learning_rate': 9.913380928400036e-05, 'epoch': 0.07}
{'loss': 3.6904, 'grad_norm': 0.67588867613766, 'learning_rate': 9.91323281097254e-05, 'epoch': 0.07}
{'loss': 3.7274, 'grad_norm': 0.7461190886982668, 'learning_rate': 9.913084693545042e-05, 'epoch': 0.07}
{'loss': 3.8328, 'grad_norm': 0.6983678923155493, 'learning_rate': 9.912936576117546e-05, 'epoch': 0.07}
{'loss': 4.0969, 'grad_norm': 0.6989336825417926, 'learning_rate': 9.91278845869005e-05, 'epoch': 0.07}
{'loss': 4.0759, 'grad_norm': 0.7105879304242833, 'learning_rate': 9.912640341262553e-05, 'epoch': 0.07}
{'loss': 3.8164, 'grad_norm': 0.6459153947497767, 'learning_rate': 9.912492223835057e-05, 'epoch': 0.07}
{'loss': 4.0913, 'grad_norm': 0.6702735672899467, 'learning_rate': 9.912344106407561e-05, 'epoch': 0.07}
{'loss': 4.0172, 'grad_norm': 0.6462584332758714, 'learning_rate': 9.912195988980064e-05, 'epoch': 0.07}
{'loss': 4.0914, 'grad_norm': 0.6756003134951782, 'learning_rate': 9.912047871552568e-05, 'epoch': 0.07}
{'loss': 3.8193, 'grad_norm': 0.8156265492758411, 'learning_rate': 9.911899754125072e-05, 'epoch': 0.07}
{'loss': 4.064, 'grad_norm': 0.6502001070957121, 'learning_rate': 9.911751636697574e-05, 'epoch': 0.07}
{'loss': 3.9996, 'grad_norm': 0.8601194287512434, 'learning_rate': 9.911603519270077e-05, 'epoch': 0.07}
{'loss': 4.0522, 'grad_norm': 0.7499993848624829, 'learning_rate': 9.911455401842581e-05, 'epoch': 0.07}
{'loss': 3.8559, 'grad_norm': 1.9623149311570163, 'learning_rate': 9.911307284415085e-05, 'epoch': 0.07}
{'loss': 3.8197, 'grad_norm': 0.6684293161353715, 'learning_rate': 9.911159166987587e-05, 'epoch': 0.07}
[2024-04-12 04:47:29,531] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8000 is about to be saved!
[2024-04-12 04:47:29,537] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-8000/global_step8000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 04:47:29,537] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8000/global_step8000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 04:47:29,543] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8000/global_step8000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 04:47:29,544] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8000/global_step8000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 04:47:32,669] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8000/global_step8000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 04:47:32,669] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-8000/global_step8000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 04:47:32,669] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
{'loss': 4.084, 'grad_norm': 0.8380251285082883, 'learning_rate': 9.911011049560091e-05, 'epoch': 0.07}
{'loss': 4.6749, 'grad_norm': 0.7725527380306073, 'learning_rate': 9.910862932132595e-05, 'epoch': 0.07}
{'loss': 4.5318, 'grad_norm': 0.7822089499513795, 'learning_rate': 9.9107148147051e-05, 'epoch': 0.07}
{'loss': 4.4904, 'grad_norm': 0.8073508553723774, 'learning_rate': 9.910566697277602e-05, 'epoch': 0.07}
{'loss': 4.4716, 'grad_norm': 0.6660679409796973, 'learning_rate': 9.910418579850106e-05, 'epoch': 0.07}
{'loss': 4.4184, 'grad_norm': 0.7373024120712108, 'learning_rate': 9.91027046242261e-05, 'epoch': 0.07}
{'loss': 4.3107, 'grad_norm': 0.7266921033297351, 'learning_rate': 9.910122344995113e-05, 'epoch': 0.07}
{'loss': 4.4037, 'grad_norm': 0.6751124563533375, 'learning_rate': 9.909974227567617e-05, 'epoch': 0.07}
{'loss': 4.3894, 'grad_norm': 0.694847581464954, 'learning_rate': 9.90982611014012e-05, 'epoch': 0.07}
{'loss': 4.3973, 'grad_norm': 0.6476191203807031, 'learning_rate': 9.909677992712622e-05, 'epoch': 0.07}
{'loss': 4.1975, 'grad_norm': 1.290253408185995, 'learning_rate': 9.909529875285126e-05, 'epoch': 0.07}
{'loss': 4.2518, 'grad_norm': 0.6742376775383202, 'learning_rate': 9.90938175785763e-05, 'epoch': 0.07}
{'loss': 4.3717, 'grad_norm': 0.7269463463321756, 'learning_rate': 9.909233640430134e-05, 'epoch': 0.07}
{'loss': 4.305, 'grad_norm': 0.851361728716318, 'learning_rate': 9.909085523002637e-05, 'epoch': 0.07}
{'loss': 4.3742, 'grad_norm': 0.7805100449464472, 'learning_rate': 9.90893740557514e-05, 'epoch': 0.07}
{'loss': 4.2929, 'grad_norm': 0.7442198686483094, 'learning_rate': 9.908789288147645e-05, 'epoch': 0.07}
{'loss': 4.1962, 'grad_norm': 0.7629502909577213, 'learning_rate': 9.908641170720147e-05, 'epoch': 0.07}
{'loss': 4.3151, 'grad_norm': 0.8209063077473763, 'learning_rate': 9.908493053292651e-05, 'epoch': 0.07}
{'loss': 4.3641, 'grad_norm': 0.6762652206036747, 'learning_rate': 9.908344935865155e-05, 'epoch': 0.07}
{'loss': 4.2984, 'grad_norm': 0.6264667819827822, 'learning_rate': 9.908196818437658e-05, 'epoch': 0.07}
[2024-04-12 04:52:10,082] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8100 is about to be saved!
[2024-04-12 04:52:10,088] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-8100/global_step8100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 04:52:10,088] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8100/global_step8100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 04:52:10,095] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8100/global_step8100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 04:52:10,095] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8100/global_step8100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 04:52:13,126] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8100/global_step8100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 04:52:13,127] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-8100/global_step8100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 04:52:13,127] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8100 is ready now!
{'loss': 4.1869, 'grad_norm': 0.6992415487446577, 'learning_rate': 9.90804870101016e-05, 'epoch': 0.07}
{'loss': 4.299, 'grad_norm': 0.6592809698356493, 'learning_rate': 9.907900583582665e-05, 'epoch': 0.07}
{'loss': 4.2861, 'grad_norm': 0.7403624803803712, 'learning_rate': 9.907752466155169e-05, 'epoch': 0.07}
{'loss': 4.3314, 'grad_norm': 0.6489787388965873, 'learning_rate': 9.907604348727671e-05, 'epoch': 0.07}
{'loss': 4.3504, 'grad_norm': 0.9685375765778823, 'learning_rate': 9.907456231300175e-05, 'epoch': 0.07}
{'loss': 4.2541, 'grad_norm': 0.65220154609299, 'learning_rate': 9.907308113872679e-05, 'epoch': 0.07}
{'loss': 4.2379, 'grad_norm': 0.7118500779981319, 'learning_rate': 9.907159996445182e-05, 'epoch': 0.07}
{'loss': 4.2366, 'grad_norm': 0.6837795240387461, 'learning_rate': 9.907011879017686e-05, 'epoch': 0.07}
{'loss': 4.2255, 'grad_norm': 0.6751937923020064, 'learning_rate': 9.90686376159019e-05, 'epoch': 0.07}
{'loss': 4.1503, 'grad_norm': 0.6565067159645988, 'learning_rate': 9.906715644162692e-05, 'epoch': 0.07}
{'loss': 4.1795, 'grad_norm': 0.6762594994338461, 'learning_rate': 9.906567526735196e-05, 'epoch': 0.07}
{'loss': 4.1931, 'grad_norm': 0.6989468886377105, 'learning_rate': 9.9064194093077e-05, 'epoch': 0.07}
{'loss': 4.3141, 'grad_norm': 0.6451342171129636, 'learning_rate': 9.906271291880203e-05, 'epoch': 0.07}
{'loss': 4.2588, 'grad_norm': 0.7233144765331756, 'learning_rate': 9.906123174452706e-05, 'epoch': 0.07}
{'loss': 4.1528, 'grad_norm': 0.6526888674106736, 'learning_rate': 9.90597505702521e-05, 'epoch': 0.07}
{'loss': 4.1923, 'grad_norm': 0.7002609453771514, 'learning_rate': 9.905826939597714e-05, 'epoch': 0.07}
{'loss': 4.1935, 'grad_norm': 0.6913400040317426, 'learning_rate': 9.905678822170216e-05, 'epoch': 0.07}
{'loss': 4.2161, 'grad_norm': 0.6706818874525071, 'learning_rate': 9.90553070474272e-05, 'epoch': 0.07}
{'loss': 4.2164, 'grad_norm': 0.6668074799207396, 'learning_rate': 9.905382587315224e-05, 'epoch': 0.07}
{'loss': 4.0718, 'grad_norm': 0.7513741586111035, 'learning_rate': 9.905234469887727e-05, 'epoch': 0.07}
[2024-04-12 04:56:50,527] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8200 is about to be saved!
[2024-04-12 04:56:50,532] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-8200/global_step8200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 04:56:50,532] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8200/global_step8200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 04:56:50,538] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8200/global_step8200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 04:56:50,539] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8200/global_step8200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 04:56:53,628] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8200/global_step8200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 04:56:53,629] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-8200/global_step8200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 04:56:53,629] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8200 is ready now!
{'loss': 4.1635, 'grad_norm': 0.6265063085317544, 'learning_rate': 9.905086352460231e-05, 'epoch': 0.07}
{'loss': 4.2716, 'grad_norm': 0.642255067810609, 'learning_rate': 9.904938235032735e-05, 'epoch': 0.07}
{'loss': 4.1655, 'grad_norm': 0.6295149544663933, 'learning_rate': 9.904790117605238e-05, 'epoch': 0.07}
{'loss': 4.224, 'grad_norm': 0.7027819154778695, 'learning_rate': 9.904642000177742e-05, 'epoch': 0.07}
{'loss': 4.1945, 'grad_norm': 0.6950733411856959, 'learning_rate': 9.904493882750246e-05, 'epoch': 0.07}
{'loss': 4.1682, 'grad_norm': 0.6606416277489408, 'learning_rate': 9.904345765322748e-05, 'epoch': 0.07}
{'loss': 4.1083, 'grad_norm': 0.7335176372855865, 'learning_rate': 9.904197647895251e-05, 'epoch': 0.07}
{'loss': 4.2117, 'grad_norm': 0.6650946564518272, 'learning_rate': 9.904049530467755e-05, 'epoch': 0.07}
{'loss': 4.1502, 'grad_norm': 0.6203870099461654, 'learning_rate': 9.903901413040259e-05, 'epoch': 0.07}
{'loss': 4.1603, 'grad_norm': 0.6429373246742699, 'learning_rate': 9.903753295612762e-05, 'epoch': 0.07}
{'loss': 4.1855, 'grad_norm': 0.679985088469018, 'learning_rate': 9.903605178185266e-05, 'epoch': 0.07}
{'loss': 4.1939, 'grad_norm': 0.6314327971927347, 'learning_rate': 9.90345706075777e-05, 'epoch': 0.07}
{'loss': 4.0499, 'grad_norm': 0.7209987205486894, 'learning_rate': 9.903308943330272e-05, 'epoch': 0.07}
{'loss': 4.1548, 'grad_norm': 0.6487300252772208, 'learning_rate': 9.903160825902776e-05, 'epoch': 0.07}
{'loss': 4.1199, 'grad_norm': 0.6545580275203443, 'learning_rate': 9.90301270847528e-05, 'epoch': 0.07}
{'loss': 4.1023, 'grad_norm': 0.7222323841635905, 'learning_rate': 9.902864591047784e-05, 'epoch': 0.07}
{'loss': 4.206, 'grad_norm': 0.667365310711997, 'learning_rate': 9.902716473620287e-05, 'epoch': 0.07}
{'loss': 4.0608, 'grad_norm': 0.6728696780960237, 'learning_rate': 9.902568356192791e-05, 'epoch': 0.07}
{'loss': 4.1219, 'grad_norm': 0.6513565624505643, 'learning_rate': 9.902420238765293e-05, 'epoch': 0.07}
{'loss': 4.126, 'grad_norm': 0.6869753102613535, 'learning_rate': 9.902272121337796e-05, 'epoch': 0.07}
[2024-04-12 05:01:31,063] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8300 is about to be saved!
[2024-04-12 05:01:31,069] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-8300/global_step8300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 05:01:31,069] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8300/global_step8300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 05:01:31,075] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8300/global_step8300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 05:01:31,076] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8300/global_step8300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 05:01:34,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8300/global_step8300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 05:01:34,167] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-8300/global_step8300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 05:01:34,167] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8300 is ready now!
{'loss': 4.1349, 'grad_norm': 0.6745899437670035, 'learning_rate': 9.9021240039103e-05, 'epoch': 0.07}
{'loss': 4.086, 'grad_norm': 0.6967871915050979, 'learning_rate': 9.901975886482804e-05, 'epoch': 0.07}
{'loss': 4.1772, 'grad_norm': 0.6027593968141983, 'learning_rate': 9.901827769055308e-05, 'epoch': 0.07}
{'loss': 4.0531, 'grad_norm': 0.6859171386641615, 'learning_rate': 9.901679651627811e-05, 'epoch': 0.07}
{'loss': 4.0669, 'grad_norm': 0.6672735768686083, 'learning_rate': 9.901531534200315e-05, 'epoch': 0.07}
{'loss': 4.1862, 'grad_norm': 0.6613938400893625, 'learning_rate': 9.901383416772819e-05, 'epoch': 0.07}
{'loss': 4.0323, 'grad_norm': 0.6188662405147022, 'learning_rate': 9.901235299345321e-05, 'epoch': 0.07}
{'loss': 4.1887, 'grad_norm': 0.6441376356124482, 'learning_rate': 9.901087181917825e-05, 'epoch': 0.07}
{'loss': 4.1235, 'grad_norm': 0.6483233776821341, 'learning_rate': 9.90093906449033e-05, 'epoch': 0.07}
{'loss': 4.2627, 'grad_norm': 0.6554361888769581, 'learning_rate': 9.900790947062832e-05, 'epoch': 0.07}
{'loss': 4.0828, 'grad_norm': 0.6355365319564614, 'learning_rate': 9.900642829635335e-05, 'epoch': 0.07}
{'loss': 4.1604, 'grad_norm': 0.6128739411718519, 'learning_rate': 9.900494712207839e-05, 'epoch': 0.07}
{'loss': 4.077, 'grad_norm': 0.6202691534717168, 'learning_rate': 9.900346594780343e-05, 'epoch': 0.07}
{'loss': 4.0414, 'grad_norm': 0.6297325234969726, 'learning_rate': 9.900198477352845e-05, 'epoch': 0.07}
{'loss': 4.1192, 'grad_norm': 0.6319151950619355, 'learning_rate': 9.900050359925349e-05, 'epoch': 0.07}
{'loss': 4.1022, 'grad_norm': 0.6169058522113654, 'learning_rate': 9.899902242497853e-05, 'epoch': 0.07}
{'loss': 4.1012, 'grad_norm': 0.6505969938279668, 'learning_rate': 9.899754125070356e-05, 'epoch': 0.07}
{'loss': 4.0339, 'grad_norm': 0.6329574136777781, 'learning_rate': 9.89960600764286e-05, 'epoch': 0.07}
{'loss': 4.014, 'grad_norm': 0.7249746133655067, 'learning_rate': 9.899457890215364e-05, 'epoch': 0.07}
{'loss': 4.1387, 'grad_norm': 0.6531444955253404, 'learning_rate': 9.899309772787867e-05, 'epoch': 0.07}
[2024-04-12 05:06:11,584] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8400 is about to be saved!
[2024-04-12 05:06:11,590] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-8400/global_step8400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 05:06:11,590] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8400/global_step8400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 05:06:11,596] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8400/global_step8400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 05:06:11,596] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8400/global_step8400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 05:06:14,598] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8400/global_step8400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 05:06:14,598] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-8400/global_step8400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 05:06:14,599] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8400 is ready now!
{'loss': 4.0976, 'grad_norm': 0.7128519807897432, 'learning_rate': 9.89916165536037e-05, 'epoch': 0.07}
{'loss': 4.1012, 'grad_norm': 0.6328548548230775, 'learning_rate': 9.899013537932875e-05, 'epoch': 0.07}
{'loss': 4.1373, 'grad_norm': 0.6582806327138714, 'learning_rate': 9.898865420505377e-05, 'epoch': 0.07}
{'loss': 4.1608, 'grad_norm': 0.6501053592081911, 'learning_rate': 9.89871730307788e-05, 'epoch': 0.07}
{'loss': 4.0838, 'grad_norm': 0.6874071489494263, 'learning_rate': 9.898569185650384e-05, 'epoch': 0.07}
{'loss': 4.0683, 'grad_norm': 0.6862896311619353, 'learning_rate': 9.898421068222888e-05, 'epoch': 0.07}
{'loss': 4.1399, 'grad_norm': 0.679888081137772, 'learning_rate': 9.89827295079539e-05, 'epoch': 0.07}
{'loss': 4.0407, 'grad_norm': 0.6573096894705706, 'learning_rate': 9.898124833367894e-05, 'epoch': 0.07}
{'loss': 4.0724, 'grad_norm': 0.6739747254528973, 'learning_rate': 9.897976715940398e-05, 'epoch': 0.07}
{'loss': 4.1163, 'grad_norm': 0.6369483739594348, 'learning_rate': 9.897828598512901e-05, 'epoch': 0.07}
{'loss': 4.2197, 'grad_norm': 0.6375154124637072, 'learning_rate': 9.897680481085405e-05, 'epoch': 0.07}
{'loss': 4.009, 'grad_norm': 0.718547738219856, 'learning_rate': 9.897532363657909e-05, 'epoch': 0.07}
{'loss': 4.1047, 'grad_norm': 0.8615423617480462, 'learning_rate': 9.897384246230412e-05, 'epoch': 0.07}
{'loss': 4.0514, 'grad_norm': 0.6309287146409766, 'learning_rate': 9.897236128802916e-05, 'epoch': 0.07}
{'loss': 4.0706, 'grad_norm': 0.6689008036730718, 'learning_rate': 9.89708801137542e-05, 'epoch': 0.07}
{'loss': 4.0889, 'grad_norm': 0.7178009131751764, 'learning_rate': 9.896939893947922e-05, 'epoch': 0.07}
{'loss': 4.0764, 'grad_norm': 0.6943401231461542, 'learning_rate': 9.896791776520425e-05, 'epoch': 0.07}
{'loss': 4.1763, 'grad_norm': 0.7016395921219228, 'learning_rate': 9.896643659092929e-05, 'epoch': 0.07}
{'loss': 4.0402, 'grad_norm': 0.6361452896507274, 'learning_rate': 9.896495541665433e-05, 'epoch': 0.07}
{'loss': 3.9992, 'grad_norm': 0.6387911110529273, 'learning_rate': 9.896347424237936e-05, 'epoch': 0.07}
[2024-04-12 05:10:52,033] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8500 is about to be saved!
[2024-04-12 05:10:52,039] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-8500/global_step8500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 05:10:52,040] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8500/global_step8500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 05:10:52,046] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8500/global_step8500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 05:10:52,046] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8500/global_step8500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 05:10:55,097] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8500/global_step8500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 05:10:55,097] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-8500/global_step8500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 05:10:55,097] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8500 is ready now!
{'loss': 3.996, 'grad_norm': 0.7065406146014817, 'learning_rate': 9.89619930681044e-05, 'epoch': 0.07}
{'loss': 3.9895, 'grad_norm': 0.6936820053967914, 'learning_rate': 9.896051189382944e-05, 'epoch': 0.07}
{'loss': 4.0866, 'grad_norm': 0.6203061539585766, 'learning_rate': 9.895903071955446e-05, 'epoch': 0.07}
{'loss': 3.9303, 'grad_norm': 0.608893853692554, 'learning_rate': 9.89575495452795e-05, 'epoch': 0.07}
{'loss': 4.036, 'grad_norm': 0.6389175085292088, 'learning_rate': 9.895606837100454e-05, 'epoch': 0.07}
{'loss': 4.0001, 'grad_norm': 0.6736056411244922, 'learning_rate': 9.895458719672957e-05, 'epoch': 0.07}
{'loss': 4.0812, 'grad_norm': 0.6559553022960014, 'learning_rate': 9.895310602245461e-05, 'epoch': 0.07}
{'loss': 3.9396, 'grad_norm': 0.7416934850470805, 'learning_rate': 9.895162484817965e-05, 'epoch': 0.07}
{'loss': 4.0906, 'grad_norm': 0.6173355795315226, 'learning_rate': 9.895014367390468e-05, 'epoch': 0.07}
{'loss': 3.8647, 'grad_norm': 0.7812972301387928, 'learning_rate': 9.89486624996297e-05, 'epoch': 0.07}
{'loss': 4.0808, 'grad_norm': 0.6826201355170297, 'learning_rate': 9.894718132535474e-05, 'epoch': 0.07}
{'loss': 3.9669, 'grad_norm': 0.6415157824401381, 'learning_rate': 9.894570015107978e-05, 'epoch': 0.07}
{'loss': 4.0584, 'grad_norm': 0.6512384261798749, 'learning_rate': 9.894421897680481e-05, 'epoch': 0.08}
{'loss': 4.1421, 'grad_norm': 0.6677811542866872, 'learning_rate': 9.894273780252985e-05, 'epoch': 0.08}
{'loss': 4.1594, 'grad_norm': 0.6582267494235536, 'learning_rate': 9.894125662825489e-05, 'epoch': 0.08}
{'loss': 3.9977, 'grad_norm': 0.6501888385217409, 'learning_rate': 9.893977545397993e-05, 'epoch': 0.08}
{'loss': 4.0536, 'grad_norm': 0.6551332943492811, 'learning_rate': 9.893829427970495e-05, 'epoch': 0.08}
{'loss': 3.8682, 'grad_norm': 0.6867559750810147, 'learning_rate': 9.893681310543e-05, 'epoch': 0.08}
{'loss': 3.9651, 'grad_norm': 0.6530399475179296, 'learning_rate': 9.893533193115503e-05, 'epoch': 0.08}
{'loss': 4.076, 'grad_norm': 0.6639707071735956, 'learning_rate': 9.893385075688006e-05, 'epoch': 0.08}
[2024-04-12 05:15:32,585] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8600 is about to be saved!
[2024-04-12 05:15:32,591] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-8600/global_step8600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 05:15:32,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8600/global_step8600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 05:15:32,597] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8600/global_step8600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 05:15:32,597] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8600/global_step8600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 05:15:35,586] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8600/global_step8600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 05:15:35,586] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-8600/global_step8600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 05:15:35,587] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8600 is ready now!
{'loss': 3.9914, 'grad_norm': 0.632046476632342, 'learning_rate': 9.893236958260509e-05, 'epoch': 0.08}
{'loss': 4.0014, 'grad_norm': 0.6130349809499316, 'learning_rate': 9.893088840833013e-05, 'epoch': 0.08}
{'loss': 4.0517, 'grad_norm': 0.6927606747495896, 'learning_rate': 9.892940723405515e-05, 'epoch': 0.08}
{'loss': 4.126, 'grad_norm': 0.7235729926930274, 'learning_rate': 9.89279260597802e-05, 'epoch': 0.08}
{'loss': 4.0844, 'grad_norm': 0.7103106443173596, 'learning_rate': 9.892644488550523e-05, 'epoch': 0.08}
{'loss': 4.0021, 'grad_norm': 0.7300239912390456, 'learning_rate': 9.892496371123027e-05, 'epoch': 0.08}
{'loss': 3.8958, 'grad_norm': 0.6336787796444037, 'learning_rate': 9.89234825369553e-05, 'epoch': 0.08}
{'loss': 3.8793, 'grad_norm': 0.7850298006869731, 'learning_rate': 9.892200136268034e-05, 'epoch': 0.08}
{'loss': 3.9936, 'grad_norm': 0.6068335119983579, 'learning_rate': 9.892052018840538e-05, 'epoch': 0.08}
{'loss': 4.0936, 'grad_norm': 0.8851914532347902, 'learning_rate': 9.89190390141304e-05, 'epoch': 0.08}
{'loss': 4.1136, 'grad_norm': 0.6482510217559013, 'learning_rate': 9.891755783985545e-05, 'epoch': 0.08}
{'loss': 3.9366, 'grad_norm': 0.7077096645331584, 'learning_rate': 9.891607666558049e-05, 'epoch': 0.08}
{'loss': 3.9953, 'grad_norm': 0.6507490400229006, 'learning_rate': 9.891459549130551e-05, 'epoch': 0.08}
{'loss': 4.0493, 'grad_norm': 0.6411901553353906, 'learning_rate': 9.891311431703054e-05, 'epoch': 0.08}
{'loss': 3.9698, 'grad_norm': 0.7225477119787869, 'learning_rate': 9.891163314275558e-05, 'epoch': 0.08}
{'loss': 3.9891, 'grad_norm': 0.6118206362345056, 'learning_rate': 9.891015196848062e-05, 'epoch': 0.08}
{'loss': 4.0274, 'grad_norm': 0.6343825601589812, 'learning_rate': 9.890867079420565e-05, 'epoch': 0.08}
{'loss': 4.0674, 'grad_norm': 0.6201512954980075, 'learning_rate': 9.890718961993069e-05, 'epoch': 0.08}
{'loss': 3.8762, 'grad_norm': 0.6638807627111893, 'learning_rate': 9.890570844565573e-05, 'epoch': 0.08}
{'loss': 4.0079, 'grad_norm': 0.6066780072925875, 'learning_rate': 9.890422727138075e-05, 'epoch': 0.08}
[2024-04-12 05:20:12,981] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8700 is about to be saved!
[2024-04-12 05:20:12,987] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-8700/global_step8700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 05:20:12,987] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8700/global_step8700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 05:20:12,993] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8700/global_step8700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 05:20:12,994] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8700/global_step8700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 05:20:16,064] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8700/global_step8700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 05:20:16,064] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-8700/global_step8700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 05:20:16,065] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8700 is ready now!
{'loss': 3.9274, 'grad_norm': 0.7611425969219995, 'learning_rate': 9.890274609710579e-05, 'epoch': 0.08}
{'loss': 3.9747, 'grad_norm': 0.6653272372777179, 'learning_rate': 9.890126492283083e-05, 'epoch': 0.08}
{'loss': 3.9963, 'grad_norm': 0.6475993325705499, 'learning_rate': 9.889978374855586e-05, 'epoch': 0.08}
{'loss': 4.0462, 'grad_norm': 0.6277382310115224, 'learning_rate': 9.88983025742809e-05, 'epoch': 0.08}
{'loss': 3.9873, 'grad_norm': 0.6739786459623787, 'learning_rate': 9.889682140000594e-05, 'epoch': 0.08}
{'loss': 4.0109, 'grad_norm': 0.6242413190510878, 'learning_rate': 9.889534022573096e-05, 'epoch': 0.08}
{'loss': 4.0306, 'grad_norm': 0.645042131680157, 'learning_rate': 9.889385905145599e-05, 'epoch': 0.08}
{'loss': 4.0347, 'grad_norm': 0.6394582928519431, 'learning_rate': 9.889237787718103e-05, 'epoch': 0.08}
{'loss': 3.9939, 'grad_norm': 0.6581930869705481, 'learning_rate': 9.889089670290607e-05, 'epoch': 0.08}
{'loss': 4.1174, 'grad_norm': 0.6931377195415809, 'learning_rate': 9.88894155286311e-05, 'epoch': 0.08}
{'loss': 4.0015, 'grad_norm': 0.839762972236513, 'learning_rate': 9.888793435435614e-05, 'epoch': 0.08}
{'loss': 4.0239, 'grad_norm': 0.6464019817165967, 'learning_rate': 9.888645318008118e-05, 'epoch': 0.08}
{'loss': 4.0028, 'grad_norm': 0.6946421648653291, 'learning_rate': 9.88849720058062e-05, 'epoch': 0.08}
{'loss': 4.0051, 'grad_norm': 0.6380868538886635, 'learning_rate': 9.888349083153124e-05, 'epoch': 0.08}
{'loss': 3.9746, 'grad_norm': 0.6392105484951468, 'learning_rate': 9.888200965725628e-05, 'epoch': 0.08}
{'loss': 3.8993, 'grad_norm': 0.673731442120003, 'learning_rate': 9.888052848298131e-05, 'epoch': 0.08}
{'loss': 3.9639, 'grad_norm': 0.6856146113836696, 'learning_rate': 9.887904730870635e-05, 'epoch': 0.08}
{'loss': 3.9946, 'grad_norm': 0.6875369928737948, 'learning_rate': 9.887756613443138e-05, 'epoch': 0.08}
{'loss': 4.0115, 'grad_norm': 0.6427164413417675, 'learning_rate': 9.887608496015642e-05, 'epoch': 0.08}
{'loss': 3.9858, 'grad_norm': 0.6175035708859982, 'learning_rate': 9.887460378588144e-05, 'epoch': 0.08}
[2024-04-12 05:24:53,433] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8800 is about to be saved!
[2024-04-12 05:24:53,438] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-8800/global_step8800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 05:24:53,438] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8800/global_step8800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 05:24:53,444] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8800/global_step8800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 05:24:53,445] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8800/global_step8800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 05:24:56,490] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8800/global_step8800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 05:24:56,490] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-8800/global_step8800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 05:24:56,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8800 is ready now!
{'loss': 4.0192, 'grad_norm': 0.6499319833192138, 'learning_rate': 9.887312261160648e-05, 'epoch': 0.08}
{'loss': 3.9532, 'grad_norm': 0.6346307928438876, 'learning_rate': 9.887164143733152e-05, 'epoch': 0.08}
{'loss': 4.0275, 'grad_norm': 0.6287230519595539, 'learning_rate': 9.887016026305655e-05, 'epoch': 0.08}
{'loss': 4.0358, 'grad_norm': 0.638512817813239, 'learning_rate': 9.886867908878159e-05, 'epoch': 0.08}
{'loss': 4.0083, 'grad_norm': 0.6503984660953324, 'learning_rate': 9.886719791450663e-05, 'epoch': 0.08}
{'loss': 4.074, 'grad_norm': 0.6986839560520903, 'learning_rate': 9.886571674023166e-05, 'epoch': 0.08}
{'loss': 3.9669, 'grad_norm': 0.6591354639832899, 'learning_rate': 9.88642355659567e-05, 'epoch': 0.08}
{'loss': 3.9372, 'grad_norm': 0.6734594737814883, 'learning_rate': 9.886275439168174e-05, 'epoch': 0.08}
{'loss': 3.7613, 'grad_norm': 0.7004498956952444, 'learning_rate': 9.886127321740678e-05, 'epoch': 0.08}
{'loss': 4.0352, 'grad_norm': 0.6593718341726299, 'learning_rate': 9.88597920431318e-05, 'epoch': 0.08}
{'loss': 4.1035, 'grad_norm': 0.6627817479872126, 'learning_rate': 9.885831086885683e-05, 'epoch': 0.08}
{'loss': 3.9665, 'grad_norm': 0.6426900368401997, 'learning_rate': 9.885682969458187e-05, 'epoch': 0.08}
{'loss': 3.9283, 'grad_norm': 0.6075845578921656, 'learning_rate': 9.88553485203069e-05, 'epoch': 0.08}
{'loss': 3.9497, 'grad_norm': 0.6157983151072848, 'learning_rate': 9.885386734603193e-05, 'epoch': 0.08}
{'loss': 4.0082, 'grad_norm': 0.6609992156999293, 'learning_rate': 9.885238617175697e-05, 'epoch': 0.08}
{'loss': 3.9397, 'grad_norm': 0.7471643779342759, 'learning_rate': 9.885090499748201e-05, 'epoch': 0.08}
{'loss': 3.9433, 'grad_norm': 0.6730815326845303, 'learning_rate': 9.884942382320704e-05, 'epoch': 0.08}
{'loss': 3.9628, 'grad_norm': 0.6308030959530745, 'learning_rate': 9.884794264893208e-05, 'epoch': 0.08}
{'loss': 4.018, 'grad_norm': 0.7384465629812874, 'learning_rate': 9.884646147465712e-05, 'epoch': 0.08}
{'loss': 3.8997, 'grad_norm': 0.6865550975218706, 'learning_rate': 9.884498030038215e-05, 'epoch': 0.08}
[2024-04-12 05:29:34,101] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8900 is about to be saved!
[2024-04-12 05:29:34,106] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-8900/global_step8900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 05:29:34,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8900/global_step8900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 05:29:34,112] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8900/global_step8900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 05:29:34,112] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-8900/global_step8900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 05:29:37,093] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-8900/global_step8900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 05:29:37,093] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-8900/global_step8900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 05:29:37,094] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8900 is ready now!
{'loss': 4.0578, 'grad_norm': 0.8691032822230572, 'learning_rate': 9.884349912610719e-05, 'epoch': 0.08}
{'loss': 3.7792, 'grad_norm': 0.633335980454852, 'learning_rate': 9.884201795183223e-05, 'epoch': 0.08}
{'loss': 3.9277, 'grad_norm': 0.65280816688324, 'learning_rate': 9.884053677755724e-05, 'epoch': 0.08}
{'loss': 3.9258, 'grad_norm': 0.6220764241532044, 'learning_rate': 9.883905560328228e-05, 'epoch': 0.08}
{'loss': 3.9321, 'grad_norm': 0.6293160380917643, 'learning_rate': 9.883757442900732e-05, 'epoch': 0.08}
{'loss': 3.7382, 'grad_norm': 0.6677997485921655, 'learning_rate': 9.883609325473236e-05, 'epoch': 0.08}
{'loss': 4.0961, 'grad_norm': 0.6719170894545927, 'learning_rate': 9.883461208045739e-05, 'epoch': 0.08}
{'loss': 3.9921, 'grad_norm': 0.7277804681493816, 'learning_rate': 9.883313090618243e-05, 'epoch': 0.08}
{'loss': 3.8834, 'grad_norm': 0.630747131825218, 'learning_rate': 9.883164973190747e-05, 'epoch': 0.08}
{'loss': 3.867, 'grad_norm': 0.6977435605612938, 'learning_rate': 9.883016855763249e-05, 'epoch': 0.08}
{'loss': 3.9339, 'grad_norm': 0.949800137976175, 'learning_rate': 9.882868738335753e-05, 'epoch': 0.08}
{'loss': 4.0457, 'grad_norm': 0.6607801396807701, 'learning_rate': 9.882720620908257e-05, 'epoch': 0.08}
{'loss': 4.0557, 'grad_norm': 0.7407612748346788, 'learning_rate': 9.88257250348076e-05, 'epoch': 0.08}
{'loss': 4.0029, 'grad_norm': 0.669537232285375, 'learning_rate': 9.882424386053264e-05, 'epoch': 0.08}
{'loss': 3.8811, 'grad_norm': 0.6670509722506927, 'learning_rate': 9.882276268625768e-05, 'epoch': 0.08}
{'loss': 3.9826, 'grad_norm': 0.641903880618677, 'learning_rate': 9.88212815119827e-05, 'epoch': 0.08}
{'loss': 3.9636, 'grad_norm': 0.6404581361247013, 'learning_rate': 9.881980033770773e-05, 'epoch': 0.08}
{'loss': 3.9214, 'grad_norm': 0.6111067699841437, 'learning_rate': 9.881831916343277e-05, 'epoch': 0.08}
{'loss': 3.8902, 'grad_norm': 0.6908473893100967, 'learning_rate': 9.881683798915781e-05, 'epoch': 0.08}
{'loss': 3.9396, 'grad_norm': 0.6273277587313494, 'learning_rate': 9.881535681488284e-05, 'epoch': 0.08}
[2024-04-12 05:34:14,522] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step9000 is about to be saved!
[2024-04-12 05:34:14,528] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-9000/global_step9000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 05:34:14,529] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9000/global_step9000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 05:34:14,534] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9000/global_step9000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 05:34:14,535] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9000/global_step9000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 05:34:17,637] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9000/global_step9000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 05:34:17,637] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-9000/global_step9000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 05:34:17,637] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
{'loss': 3.9486, 'grad_norm': 0.6332500223301003, 'learning_rate': 9.881387564060788e-05, 'epoch': 0.08}
{'loss': 3.8115, 'grad_norm': 0.5990763352910299, 'learning_rate': 9.881239446633292e-05, 'epoch': 0.08}
{'loss': 3.7684, 'grad_norm': 0.6556072370174418, 'learning_rate': 9.881091329205794e-05, 'epoch': 0.08}
{'loss': 3.9387, 'grad_norm': 0.7035764816542507, 'learning_rate': 9.880943211778298e-05, 'epoch': 0.08}
{'loss': 3.9136, 'grad_norm': 0.6399926097124908, 'learning_rate': 9.880795094350802e-05, 'epoch': 0.08}
{'loss': 3.948, 'grad_norm': 0.6460491502901937, 'learning_rate': 9.880646976923305e-05, 'epoch': 0.08}
{'loss': 3.8445, 'grad_norm': 0.7043095896661778, 'learning_rate': 9.880498859495809e-05, 'epoch': 0.08}
{'loss': 3.8949, 'grad_norm': 0.6447740217810052, 'learning_rate': 9.880350742068312e-05, 'epoch': 0.08}
{'loss': 4.0232, 'grad_norm': 0.8182051567643716, 'learning_rate': 9.880202624640816e-05, 'epoch': 0.08}
{'loss': 3.9439, 'grad_norm': 0.648160752409783, 'learning_rate': 9.880054507213318e-05, 'epoch': 0.08}
{'loss': 3.8362, 'grad_norm': 0.7276507410396155, 'learning_rate': 9.879906389785822e-05, 'epoch': 0.08}
{'loss': 3.9898, 'grad_norm': 0.6136558827736625, 'learning_rate': 9.879758272358326e-05, 'epoch': 0.08}
{'loss': 3.8945, 'grad_norm': 0.597827957509784, 'learning_rate': 9.879610154930829e-05, 'epoch': 0.08}
{'loss': 3.9557, 'grad_norm': 0.6933821154893697, 'learning_rate': 9.879462037503333e-05, 'epoch': 0.08}
{'loss': 3.9734, 'grad_norm': 0.6115275832972851, 'learning_rate': 9.879313920075837e-05, 'epoch': 0.08}
{'loss': 3.9647, 'grad_norm': 0.6654216921940581, 'learning_rate': 9.87916580264834e-05, 'epoch': 0.08}
{'loss': 3.9049, 'grad_norm': 0.6574287381268819, 'learning_rate': 9.879017685220844e-05, 'epoch': 0.08}
{'loss': 3.9398, 'grad_norm': 0.6561619336453294, 'learning_rate': 9.878869567793348e-05, 'epoch': 0.08}
{'loss': 3.7786, 'grad_norm': 1.0037641609445145, 'learning_rate': 9.87872145036585e-05, 'epoch': 0.08}
{'loss': 3.9509, 'grad_norm': 0.6140562904065572, 'learning_rate': 9.878573332938354e-05, 'epoch': 0.08}
[2024-04-12 05:38:54,900] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step9100 is about to be saved!
[2024-04-12 05:38:54,905] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-9100/global_step9100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 05:38:54,905] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9100/global_step9100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 05:38:54,911] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9100/global_step9100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 05:38:54,912] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9100/global_step9100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 05:38:57,872] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9100/global_step9100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 05:38:57,872] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-9100/global_step9100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 05:38:57,873] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9100 is ready now!
{'loss': 3.8841, 'grad_norm': 0.6258935732496614, 'learning_rate': 9.878425215510857e-05, 'epoch': 0.08}
{'loss': 3.9225, 'grad_norm': 0.7105502947031294, 'learning_rate': 9.878277098083361e-05, 'epoch': 0.08}
{'loss': 4.0263, 'grad_norm': 0.6653375058801938, 'learning_rate': 9.878128980655864e-05, 'epoch': 0.08}
{'loss': 3.9794, 'grad_norm': 0.6414537753799349, 'learning_rate': 9.877980863228368e-05, 'epoch': 0.08}
{'loss': 3.857, 'grad_norm': 0.6307639871770859, 'learning_rate': 9.877832745800872e-05, 'epoch': 0.08}
{'loss': 3.9567, 'grad_norm': 0.6223094368973972, 'learning_rate': 9.877684628373374e-05, 'epoch': 0.08}
{'loss': 3.9885, 'grad_norm': 0.6785757522202135, 'learning_rate': 9.877536510945878e-05, 'epoch': 0.08}
{'loss': 3.7975, 'grad_norm': 0.5803168469334027, 'learning_rate': 9.877388393518382e-05, 'epoch': 0.08}
{'loss': 3.8746, 'grad_norm': 0.676100825125002, 'learning_rate': 9.877240276090886e-05, 'epoch': 0.08}
{'loss': 3.9631, 'grad_norm': 0.6424297837310879, 'learning_rate': 9.877092158663389e-05, 'epoch': 0.08}
{'loss': 3.8112, 'grad_norm': 0.6258930134194203, 'learning_rate': 9.876944041235893e-05, 'epoch': 0.08}
{'loss': 3.999, 'grad_norm': 0.6224957730277367, 'learning_rate': 9.876795923808397e-05, 'epoch': 0.08}
{'loss': 3.7376, 'grad_norm': 0.6249850935830781, 'learning_rate': 9.876647806380898e-05, 'epoch': 0.08}
{'loss': 3.9739, 'grad_norm': 0.6663069675542593, 'learning_rate': 9.876499688953402e-05, 'epoch': 0.08}
{'loss': 3.986, 'grad_norm': 0.6547972565516338, 'learning_rate': 9.876351571525906e-05, 'epoch': 0.08}
{'loss': 3.8646, 'grad_norm': 0.6464053623907532, 'learning_rate': 9.87620345409841e-05, 'epoch': 0.08}
{'loss': 4.1067, 'grad_norm': 0.6618702451593549, 'learning_rate': 9.876055336670913e-05, 'epoch': 0.08}
{'loss': 3.9907, 'grad_norm': 0.6073568950778869, 'learning_rate': 9.875907219243417e-05, 'epoch': 0.08}
{'loss': 3.9174, 'grad_norm': 0.6126094748202779, 'learning_rate': 9.875759101815921e-05, 'epoch': 0.08}
{'loss': 3.9391, 'grad_norm': 0.6384723758393989, 'learning_rate': 9.875610984388423e-05, 'epoch': 0.08}
[2024-04-12 05:43:34,906] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step9200 is about to be saved!
[2024-04-12 05:43:34,911] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-9200/global_step9200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 05:43:34,911] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9200/global_step9200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 05:43:34,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9200/global_step9200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 05:43:34,917] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9200/global_step9200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 05:43:37,987] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9200/global_step9200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 05:43:37,988] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-9200/global_step9200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 05:43:37,988] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9200 is ready now!
{'loss': 3.8885, 'grad_norm': 0.6218419611117922, 'learning_rate': 9.875462866960927e-05, 'epoch': 0.08}
{'loss': 4.0142, 'grad_norm': 0.6161211771672602, 'learning_rate': 9.875314749533431e-05, 'epoch': 0.08}
{'loss': 3.933, 'grad_norm': 0.6740658017531617, 'learning_rate': 9.875166632105934e-05, 'epoch': 0.08}
{'loss': 3.9151, 'grad_norm': 0.6417579210670042, 'learning_rate': 9.875018514678438e-05, 'epoch': 0.08}
{'loss': 3.8754, 'grad_norm': 0.6294094182094011, 'learning_rate': 9.874870397250942e-05, 'epoch': 0.08}
{'loss': 4.0365, 'grad_norm': 0.6716653702648695, 'learning_rate': 9.874722279823445e-05, 'epoch': 0.08}
{'loss': 3.7539, 'grad_norm': 0.7109212696217512, 'learning_rate': 9.874574162395947e-05, 'epoch': 0.08}
{'loss': 3.7912, 'grad_norm': 0.646190066826453, 'learning_rate': 9.874426044968451e-05, 'epoch': 0.08}
{'loss': 3.855, 'grad_norm': 0.6412584027199187, 'learning_rate': 9.874277927540955e-05, 'epoch': 0.08}
{'loss': 3.9229, 'grad_norm': 0.6306083963531377, 'learning_rate': 9.874129810113458e-05, 'epoch': 0.08}
{'loss': 3.9241, 'grad_norm': 0.6943759006732405, 'learning_rate': 9.873981692685962e-05, 'epoch': 0.08}
{'loss': 3.9369, 'grad_norm': 0.6435591252253531, 'learning_rate': 9.873833575258466e-05, 'epoch': 0.08}
{'loss': 3.9406, 'grad_norm': 0.6398222953604225, 'learning_rate': 9.873685457830969e-05, 'epoch': 0.08}
{'loss': 3.8901, 'grad_norm': 0.7130697841510477, 'learning_rate': 9.873537340403473e-05, 'epoch': 0.08}
{'loss': 3.8761, 'grad_norm': 0.5997376668899278, 'learning_rate': 9.873389222975977e-05, 'epoch': 0.08}
{'loss': 3.9024, 'grad_norm': 0.6170727538287667, 'learning_rate': 9.873241105548479e-05, 'epoch': 0.08}
{'loss': 3.8446, 'grad_norm': 0.7838781008342091, 'learning_rate': 9.873092988120983e-05, 'epoch': 0.08}
{'loss': 3.9797, 'grad_norm': 0.6194939037912306, 'learning_rate': 9.872944870693486e-05, 'epoch': 0.08}
{'loss': 3.9016, 'grad_norm': 0.6217502215590778, 'learning_rate': 9.87279675326599e-05, 'epoch': 0.08}
{'loss': 3.9862, 'grad_norm': 0.6267825297780937, 'learning_rate': 9.872648635838492e-05, 'epoch': 0.08}
[2024-04-12 05:48:14,934] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step9300 is about to be saved!
[2024-04-12 05:48:14,939] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-9300/global_step9300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 05:48:14,939] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9300/global_step9300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 05:48:14,946] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9300/global_step9300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 05:48:14,946] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9300/global_step9300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 05:48:17,956] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9300/global_step9300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 05:48:17,957] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-9300/global_step9300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 05:48:17,957] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9300 is ready now!
{'loss': 3.8795, 'grad_norm': 0.6418116986892892, 'learning_rate': 9.872500518410996e-05, 'epoch': 0.08}
{'loss': 3.9663, 'grad_norm': 0.6015331531284485, 'learning_rate': 9.8723524009835e-05, 'epoch': 0.08}
{'loss': 3.8091, 'grad_norm': 0.5855592371462274, 'learning_rate': 9.872204283556003e-05, 'epoch': 0.08}
{'loss': 3.9071, 'grad_norm': 0.6002677904689552, 'learning_rate': 9.872056166128507e-05, 'epoch': 0.08}
{'loss': 3.8463, 'grad_norm': 0.6162032737400186, 'learning_rate': 9.871908048701011e-05, 'epoch': 0.08}
{'loss': 3.7688, 'grad_norm': 0.6234878394154426, 'learning_rate': 9.871759931273514e-05, 'epoch': 0.08}
{'loss': 3.8738, 'grad_norm': 0.6430091634293317, 'learning_rate': 9.871611813846018e-05, 'epoch': 0.08}
{'loss': 3.8025, 'grad_norm': 0.6500253261284414, 'learning_rate': 9.871463696418522e-05, 'epoch': 0.08}
{'loss': 3.8453, 'grad_norm': 0.7252031813041709, 'learning_rate': 9.871315578991024e-05, 'epoch': 0.08}
{'loss': 3.6749, 'grad_norm': 0.7319842549431917, 'learning_rate': 9.871167461563528e-05, 'epoch': 0.08}
{'loss': 3.8921, 'grad_norm': 0.6358983055967985, 'learning_rate': 9.871019344136031e-05, 'epoch': 0.08}
{'loss': 3.972, 'grad_norm': 0.7605564494775122, 'learning_rate': 9.870871226708535e-05, 'epoch': 0.08}
{'loss': 3.8956, 'grad_norm': 0.6893741115681327, 'learning_rate': 9.870723109281038e-05, 'epoch': 0.08}
{'loss': 3.9366, 'grad_norm': 0.6851976554572863, 'learning_rate': 9.870574991853542e-05, 'epoch': 0.08}
{'loss': 3.9632, 'grad_norm': 0.6685889150025747, 'learning_rate': 9.870426874426046e-05, 'epoch': 0.08}
{'loss': 3.9469, 'grad_norm': 0.6183662906494276, 'learning_rate': 9.870278756998548e-05, 'epoch': 0.08}
{'loss': 3.8603, 'grad_norm': 0.6639997460960615, 'learning_rate': 9.870130639571052e-05, 'epoch': 0.08}
{'loss': 3.9359, 'grad_norm': 0.5962499096275234, 'learning_rate': 9.869982522143556e-05, 'epoch': 0.08}
{'loss': 3.8736, 'grad_norm': 0.6331889571454316, 'learning_rate': 9.869834404716059e-05, 'epoch': 0.08}
{'loss': 3.975, 'grad_norm': 0.6925674790740545, 'learning_rate': 9.869686287288563e-05, 'epoch': 0.08}
[2024-04-12 05:52:55,005] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step9400 is about to be saved!
[2024-04-12 05:52:55,011] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-9400/global_step9400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 05:52:55,011] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9400/global_step9400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 05:52:55,016] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9400/global_step9400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 05:52:55,017] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9400/global_step9400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 05:52:58,082] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9400/global_step9400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 05:52:58,082] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-9400/global_step9400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 05:52:58,083] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9400 is ready now!
{'loss': 3.8912, 'grad_norm': 0.615815243065902, 'learning_rate': 9.869538169861067e-05, 'epoch': 0.08}
{'loss': 3.9395, 'grad_norm': 0.6018416057051067, 'learning_rate': 9.869390052433571e-05, 'epoch': 0.08}
{'loss': 3.9498, 'grad_norm': 0.6469425656212248, 'learning_rate': 9.869241935006072e-05, 'epoch': 0.08}
{'loss': 3.8274, 'grad_norm': 0.6656709638134433, 'learning_rate': 9.869093817578576e-05, 'epoch': 0.08}
{'loss': 4.0982, 'grad_norm': 0.6645823888530688, 'learning_rate': 9.86894570015108e-05, 'epoch': 0.08}
{'loss': 3.7067, 'grad_norm': 0.6535197471538804, 'learning_rate': 9.868797582723583e-05, 'epoch': 0.08}
{'loss': 3.8136, 'grad_norm': 0.6437791473669433, 'learning_rate': 9.868649465296087e-05, 'epoch': 0.08}
{'loss': 3.8208, 'grad_norm': 0.629852805516117, 'learning_rate': 9.868501347868591e-05, 'epoch': 0.08}
{'loss': 3.8538, 'grad_norm': 0.6718378268899553, 'learning_rate': 9.868353230441095e-05, 'epoch': 0.08}
{'loss': 3.7313, 'grad_norm': 0.6228353280660498, 'learning_rate': 9.868205113013597e-05, 'epoch': 0.08}
{'loss': 3.7944, 'grad_norm': 0.6303147987354213, 'learning_rate': 9.868056995586101e-05, 'epoch': 0.08}
{'loss': 3.9441, 'grad_norm': 0.7257344629823244, 'learning_rate': 9.867908878158605e-05, 'epoch': 0.08}
{'loss': 3.8265, 'grad_norm': 0.6818100777424085, 'learning_rate': 9.867760760731108e-05, 'epoch': 0.08}
{'loss': 3.8051, 'grad_norm': 0.6294868078726571, 'learning_rate': 9.867612643303612e-05, 'epoch': 0.08}
{'loss': 3.8629, 'grad_norm': 0.5633330404077712, 'learning_rate': 9.867464525876116e-05, 'epoch': 0.08}
{'loss': 3.8793, 'grad_norm': 0.6459873417847689, 'learning_rate': 9.867316408448617e-05, 'epoch': 0.08}
{'loss': 3.8776, 'grad_norm': 0.6069990167645096, 'learning_rate': 9.867168291021121e-05, 'epoch': 0.08}
{'loss': 3.9972, 'grad_norm': 0.5929857468548968, 'learning_rate': 9.867020173593625e-05, 'epoch': 0.08}
{'loss': 3.8487, 'grad_norm': 0.6234622720893305, 'learning_rate': 9.86687205616613e-05, 'epoch': 0.08}
{'loss': 3.815, 'grad_norm': 0.6317650289034119, 'learning_rate': 9.866723938738632e-05, 'epoch': 0.08}
[2024-04-12 05:57:35,151] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step9500 is about to be saved!
[2024-04-12 05:57:35,157] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-9500/global_step9500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 05:57:35,157] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9500/global_step9500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 05:57:35,163] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9500/global_step9500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 05:57:35,163] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9500/global_step9500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 05:57:38,226] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9500/global_step9500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 05:57:38,226] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-9500/global_step9500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 05:57:38,227] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9500 is ready now!
{'loss': 3.7242, 'grad_norm': 0.605732687711094, 'learning_rate': 9.866575821311136e-05, 'epoch': 0.08}
{'loss': 3.9048, 'grad_norm': 0.638542504340177, 'learning_rate': 9.86642770388364e-05, 'epoch': 0.08}
{'loss': 3.6392, 'grad_norm': 0.656527508662744, 'learning_rate': 9.866279586456143e-05, 'epoch': 0.08}
{'loss': 3.9105, 'grad_norm': 0.6380881186187559, 'learning_rate': 9.866131469028647e-05, 'epoch': 0.08}
{'loss': 3.814, 'grad_norm': 0.6718981978891101, 'learning_rate': 9.86598335160115e-05, 'epoch': 0.08}
{'loss': 3.8263, 'grad_norm': 0.6062677553249534, 'learning_rate': 9.865835234173653e-05, 'epoch': 0.08}
{'loss': 3.8147, 'grad_norm': 0.6167548881639491, 'learning_rate': 9.865687116746157e-05, 'epoch': 0.08}
{'loss': 3.8446, 'grad_norm': 0.6265915576744191, 'learning_rate': 9.86553899931866e-05, 'epoch': 0.08}
{'loss': 3.8767, 'grad_norm': 0.5948755335130486, 'learning_rate': 9.865390881891164e-05, 'epoch': 0.08}
{'loss': 3.9211, 'grad_norm': 0.6214096042610759, 'learning_rate': 9.865242764463667e-05, 'epoch': 0.08}
{'loss': 3.8337, 'grad_norm': 0.6262788559236093, 'learning_rate': 9.86509464703617e-05, 'epoch': 0.08}
{'loss': 3.8329, 'grad_norm': 0.6538568289534122, 'learning_rate': 9.864946529608675e-05, 'epoch': 0.08}
{'loss': 3.8166, 'grad_norm': 0.60977631536693, 'learning_rate': 9.864798412181177e-05, 'epoch': 0.08}
{'loss': 3.8782, 'grad_norm': 0.6634070530802183, 'learning_rate': 9.864650294753681e-05, 'epoch': 0.08}
{'loss': 3.9351, 'grad_norm': 0.6612403220374584, 'learning_rate': 9.864502177326185e-05, 'epoch': 0.08}
{'loss': 3.8739, 'grad_norm': 0.6044458913072779, 'learning_rate': 9.864354059898688e-05, 'epoch': 0.08}
{'loss': 3.8338, 'grad_norm': 0.6340728233033751, 'learning_rate': 9.864205942471192e-05, 'epoch': 0.08}
{'loss': 3.8524, 'grad_norm': 0.6137029389406653, 'learning_rate': 9.864057825043696e-05, 'epoch': 0.08}
{'loss': 3.8966, 'grad_norm': 0.5935004827901098, 'learning_rate': 9.863909707616198e-05, 'epoch': 0.08}
{'loss': 3.9458, 'grad_norm': 0.697616344024293, 'learning_rate': 9.863761590188702e-05, 'epoch': 0.08}
[2024-04-12 06:02:15,403] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step9600 is about to be saved!
[2024-04-12 06:02:15,409] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-9600/global_step9600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 06:02:15,409] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9600/global_step9600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 06:02:15,416] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9600/global_step9600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 06:02:15,416] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9600/global_step9600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 06:02:18,456] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9600/global_step9600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 06:02:18,456] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-9600/global_step9600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 06:02:18,457] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9600 is ready now!
{'loss': 3.8592, 'grad_norm': 0.6394694235274878, 'learning_rate': 9.863613472761205e-05, 'epoch': 0.08}
{'loss': 3.9414, 'grad_norm': 0.6053224880168272, 'learning_rate': 9.863465355333709e-05, 'epoch': 0.08}
{'loss': 3.8516, 'grad_norm': 0.6157064856691357, 'learning_rate': 9.863317237906212e-05, 'epoch': 0.08}
{'loss': 3.8516, 'grad_norm': 0.6205691183036076, 'learning_rate': 9.863169120478716e-05, 'epoch': 0.08}
{'loss': 3.7792, 'grad_norm': 0.6483538402107983, 'learning_rate': 9.86302100305122e-05, 'epoch': 0.08}
{'loss': 3.7921, 'grad_norm': 0.7001920623599702, 'learning_rate': 9.862872885623722e-05, 'epoch': 0.08}
{'loss': 3.8767, 'grad_norm': 0.6232744661144066, 'learning_rate': 9.862724768196226e-05, 'epoch': 0.08}
{'loss': 3.8024, 'grad_norm': 0.8769464585714875, 'learning_rate': 9.86257665076873e-05, 'epoch': 0.08}
{'loss': 3.8392, 'grad_norm': 0.6021845098404143, 'learning_rate': 9.862428533341233e-05, 'epoch': 0.08}
{'loss': 3.8136, 'grad_norm': 0.6126470199304774, 'learning_rate': 9.862280415913737e-05, 'epoch': 0.08}
{'loss': 3.9273, 'grad_norm': 0.6582292188926989, 'learning_rate': 9.862132298486241e-05, 'epoch': 0.08}
{'loss': 3.8591, 'grad_norm': 0.678645807588537, 'learning_rate': 9.861984181058745e-05, 'epoch': 0.08}
{'loss': 3.8153, 'grad_norm': 0.652312155214792, 'learning_rate': 9.861836063631246e-05, 'epoch': 0.08}
{'loss': 3.7839, 'grad_norm': 0.6276849996302468, 'learning_rate': 9.86168794620375e-05, 'epoch': 0.08}
{'loss': 3.8932, 'grad_norm': 0.6030772461652247, 'learning_rate': 9.861539828776254e-05, 'epoch': 0.08}
{'loss': 3.9636, 'grad_norm': 0.8702468345185703, 'learning_rate': 9.861391711348757e-05, 'epoch': 0.08}
{'loss': 3.7645, 'grad_norm': 0.6680005937929254, 'learning_rate': 9.861243593921261e-05, 'epoch': 0.08}
{'loss': 3.8717, 'grad_norm': 0.645114780120548, 'learning_rate': 9.861095476493765e-05, 'epoch': 0.08}
{'loss': 3.6105, 'grad_norm': 0.7183125291079099, 'learning_rate': 9.860947359066268e-05, 'epoch': 0.08}
{'loss': 3.7997, 'grad_norm': 0.6086047893132163, 'learning_rate': 9.860799241638772e-05, 'epoch': 0.08}
[2024-04-12 06:06:55,619] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step9700 is about to be saved!
[2024-04-12 06:06:55,624] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-9700/global_step9700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 06:06:55,624] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9700/global_step9700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 06:06:55,630] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9700/global_step9700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 06:06:55,630] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9700/global_step9700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 06:06:58,688] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9700/global_step9700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 06:06:58,688] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-9700/global_step9700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 06:06:58,689] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9700 is ready now!
{'loss': 3.8267, 'grad_norm': 0.6064046980375084, 'learning_rate': 9.860651124211276e-05, 'epoch': 0.08}
{'loss': 3.7325, 'grad_norm': 0.6554594787561496, 'learning_rate': 9.86050300678378e-05, 'epoch': 0.09}
{'loss': 3.784, 'grad_norm': 0.66240823387285, 'learning_rate': 9.860354889356282e-05, 'epoch': 0.09}
{'loss': 3.6997, 'grad_norm': 0.7043746795523562, 'learning_rate': 9.860206771928786e-05, 'epoch': 0.09}
{'loss': 3.7471, 'grad_norm': 0.7094727147759146, 'learning_rate': 9.86005865450129e-05, 'epoch': 0.09}
{'loss': 3.7793, 'grad_norm': 0.789244156630743, 'learning_rate': 9.859910537073791e-05, 'epoch': 0.09}
{'loss': 3.8986, 'grad_norm': 0.6404998310633787, 'learning_rate': 9.859762419646295e-05, 'epoch': 0.09}
{'loss': 3.8342, 'grad_norm': 0.6966903941699562, 'learning_rate': 9.8596143022188e-05, 'epoch': 0.09}
{'loss': 3.8903, 'grad_norm': 0.6230481693431675, 'learning_rate': 9.859466184791303e-05, 'epoch': 0.09}
{'loss': 3.738, 'grad_norm': 0.6415881100452435, 'learning_rate': 9.859318067363806e-05, 'epoch': 0.09}
{'loss': 3.8843, 'grad_norm': 0.5978925337083297, 'learning_rate': 9.85916994993631e-05, 'epoch': 0.09}
{'loss': 3.8022, 'grad_norm': 0.5850440921874358, 'learning_rate': 9.859021832508814e-05, 'epoch': 0.09}
{'loss': 3.9544, 'grad_norm': 0.6348661941735482, 'learning_rate': 9.858873715081317e-05, 'epoch': 0.09}
{'loss': 3.9391, 'grad_norm': 0.6762597069804598, 'learning_rate': 9.858725597653821e-05, 'epoch': 0.09}
{'loss': 3.8502, 'grad_norm': 0.6651278914625088, 'learning_rate': 9.858577480226325e-05, 'epoch': 0.09}
{'loss': 3.7708, 'grad_norm': 0.6037501967900544, 'learning_rate': 9.858429362798827e-05, 'epoch': 0.09}
{'loss': 3.7288, 'grad_norm': 0.7235492981256124, 'learning_rate': 9.858281245371331e-05, 'epoch': 0.09}
{'loss': 3.8564, 'grad_norm': 0.6777130057865233, 'learning_rate': 9.858133127943834e-05, 'epoch': 0.09}
{'loss': 3.638, 'grad_norm': 0.6140900107704883, 'learning_rate': 9.857985010516338e-05, 'epoch': 0.09}
{'loss': 3.7959, 'grad_norm': 0.7265481724385244, 'learning_rate': 9.85783689308884e-05, 'epoch': 0.09}
[2024-04-12 06:11:35,744] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step9800 is about to be saved!
[2024-04-12 06:11:35,749] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-9800/global_step9800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 06:11:35,749] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9800/global_step9800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 06:11:35,755] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9800/global_step9800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 06:11:35,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9800/global_step9800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 06:11:38,813] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9800/global_step9800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 06:11:38,813] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-9800/global_step9800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 06:11:38,813] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9800 is ready now!
{'loss': 3.9451, 'grad_norm': 0.6358431130865781, 'learning_rate': 9.857688775661345e-05, 'epoch': 0.09}
{'loss': 3.701, 'grad_norm': 0.6433446771326394, 'learning_rate': 9.857540658233849e-05, 'epoch': 0.09}
{'loss': 3.799, 'grad_norm': 0.6246553559244391, 'learning_rate': 9.857392540806351e-05, 'epoch': 0.09}
{'loss': 3.6171, 'grad_norm': 0.910166008525251, 'learning_rate': 9.857244423378855e-05, 'epoch': 0.09}
{'loss': 3.8168, 'grad_norm': 0.6011401803853483, 'learning_rate': 9.857096305951359e-05, 'epoch': 0.09}
{'loss': 3.6605, 'grad_norm': 0.6283815092054346, 'learning_rate': 9.856948188523862e-05, 'epoch': 0.09}
{'loss': 3.8161, 'grad_norm': 0.6542358792104732, 'learning_rate': 9.856800071096366e-05, 'epoch': 0.09}
{'loss': 3.8658, 'grad_norm': 0.6149433214714175, 'learning_rate': 9.85665195366887e-05, 'epoch': 0.09}
{'loss': 3.8409, 'grad_norm': 0.6293958780157133, 'learning_rate': 9.856503836241373e-05, 'epoch': 0.09}
{'loss': 3.8757, 'grad_norm': 0.6216210427334083, 'learning_rate': 9.856355718813877e-05, 'epoch': 0.09}
{'loss': 3.902, 'grad_norm': 0.6957064040745145, 'learning_rate': 9.856207601386379e-05, 'epoch': 0.09}
{'loss': 3.8341, 'grad_norm': 0.5977849820323583, 'learning_rate': 9.856059483958883e-05, 'epoch': 0.09}
{'loss': 3.8958, 'grad_norm': 0.646650322463662, 'learning_rate': 9.855911366531386e-05, 'epoch': 0.09}
{'loss': 3.6494, 'grad_norm': 0.6638403196928315, 'learning_rate': 9.85576324910389e-05, 'epoch': 0.09}
{'loss': 3.8134, 'grad_norm': 0.6515044562240513, 'learning_rate': 9.855615131676394e-05, 'epoch': 0.09}
{'loss': 3.8068, 'grad_norm': 0.6456179870422133, 'learning_rate': 9.855467014248896e-05, 'epoch': 0.09}
{'loss': 3.7576, 'grad_norm': 0.7410947832739125, 'learning_rate': 9.8553188968214e-05, 'epoch': 0.09}
{'loss': 3.7892, 'grad_norm': 0.7097297586026872, 'learning_rate': 9.855170779393904e-05, 'epoch': 0.09}
{'loss': 3.8572, 'grad_norm': 0.6643642295011092, 'learning_rate': 9.855022661966407e-05, 'epoch': 0.09}
{'loss': 3.7419, 'grad_norm': 0.6544869598157025, 'learning_rate': 9.854874544538911e-05, 'epoch': 0.09}
[2024-04-12 06:16:15,984] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step9900 is about to be saved!
[2024-04-12 06:16:15,990] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-9900/global_step9900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 06:16:15,990] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9900/global_step9900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 06:16:15,996] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9900/global_step9900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 06:16:15,996] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-9900/global_step9900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 06:16:19,092] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-9900/global_step9900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 06:16:19,092] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-9900/global_step9900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 06:16:19,093] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9900 is ready now!
{'loss': 3.7665, 'grad_norm': 0.6580531523461258, 'learning_rate': 9.854726427111415e-05, 'epoch': 0.09}
{'loss': 3.7906, 'grad_norm': 0.6096508955231382, 'learning_rate': 9.854578309683918e-05, 'epoch': 0.09}
{'loss': 3.8157, 'grad_norm': 0.6022104066471834, 'learning_rate': 9.85443019225642e-05, 'epoch': 0.09}
{'loss': 3.8997, 'grad_norm': 0.6433218891821562, 'learning_rate': 9.854282074828924e-05, 'epoch': 0.09}
{'loss': 3.8937, 'grad_norm': 0.6401905866596957, 'learning_rate': 9.854133957401428e-05, 'epoch': 0.09}
{'loss': 3.8193, 'grad_norm': 0.6155776704808321, 'learning_rate': 9.853985839973931e-05, 'epoch': 0.09}
{'loss': 3.8339, 'grad_norm': 0.6124023954920348, 'learning_rate': 9.853837722546435e-05, 'epoch': 0.09}
{'loss': 3.6802, 'grad_norm': 0.6144201086613107, 'learning_rate': 9.853689605118939e-05, 'epoch': 0.09}
{'loss': 3.7936, 'grad_norm': 0.6992742914225621, 'learning_rate': 9.853541487691442e-05, 'epoch': 0.09}
{'loss': 3.7559, 'grad_norm': 0.6243263104381596, 'learning_rate': 9.853393370263946e-05, 'epoch': 0.09}
{'loss': 3.7539, 'grad_norm': 0.5914074924641388, 'learning_rate': 9.85324525283645e-05, 'epoch': 0.09}
{'loss': 3.8875, 'grad_norm': 0.5950273845785209, 'learning_rate': 9.853097135408952e-05, 'epoch': 0.09}
{'loss': 3.7404, 'grad_norm': 0.6513480114617096, 'learning_rate': 9.852949017981456e-05, 'epoch': 0.09}
{'loss': 3.7161, 'grad_norm': 0.724478780764458, 'learning_rate': 9.85280090055396e-05, 'epoch': 0.09}
{'loss': 3.7656, 'grad_norm': 0.6027300956248082, 'learning_rate': 9.852652783126464e-05, 'epoch': 0.09}
{'loss': 3.8626, 'grad_norm': 0.6097843730294379, 'learning_rate': 9.852504665698966e-05, 'epoch': 0.09}
{'loss': 3.8765, 'grad_norm': 0.6197866222420504, 'learning_rate': 9.85235654827147e-05, 'epoch': 0.09}
{'loss': 3.6924, 'grad_norm': 0.6576311694152173, 'learning_rate': 9.852208430843974e-05, 'epoch': 0.09}
{'loss': 3.8361, 'grad_norm': 0.6112005841963897, 'learning_rate': 9.852060313416476e-05, 'epoch': 0.09}
{'loss': 3.7774, 'grad_norm': 0.6613347494859744, 'learning_rate': 9.85191219598898e-05, 'epoch': 0.09}
[2024-04-12 06:20:56,160] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step10000 is about to be saved!
[2024-04-12 06:20:56,166] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-10000/global_step10000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 06:20:56,166] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10000/global_step10000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 06:20:56,172] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10000/global_step10000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 06:20:56,172] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10000/global_step10000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 06:20:59,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10000/global_step10000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 06:20:59,211] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-10000/global_step10000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 06:20:59,211] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
{'loss': 3.6736, 'grad_norm': 1.2774389986504864, 'learning_rate': 9.851764078561484e-05, 'epoch': 0.09}
{'loss': 3.6881, 'grad_norm': 0.69332932308917, 'learning_rate': 9.851615961133988e-05, 'epoch': 0.09}
{'loss': 3.8351, 'grad_norm': 0.6372782300889926, 'learning_rate': 9.851467843706491e-05, 'epoch': 0.09}
{'loss': 3.5982, 'grad_norm': 0.672234145364389, 'learning_rate': 9.851319726278995e-05, 'epoch': 0.09}
{'loss': 3.6549, 'grad_norm': 0.8690462116525918, 'learning_rate': 9.851171608851499e-05, 'epoch': 0.09}
{'loss': 3.9081, 'grad_norm': 0.6211305456034054, 'learning_rate': 9.851023491424001e-05, 'epoch': 0.09}
{'loss': 3.7899, 'grad_norm': 0.6811454234282032, 'learning_rate': 9.850875373996505e-05, 'epoch': 0.09}
{'loss': 3.7855, 'grad_norm': 0.6262641718708885, 'learning_rate': 9.850727256569008e-05, 'epoch': 0.09}
{'loss': 3.8671, 'grad_norm': 0.7174127416051976, 'learning_rate': 9.850579139141512e-05, 'epoch': 0.09}
{'loss': 3.7877, 'grad_norm': 0.6441563748886265, 'learning_rate': 9.850431021714015e-05, 'epoch': 0.09}
{'loss': 3.8985, 'grad_norm': 0.6459792590722592, 'learning_rate': 9.850282904286519e-05, 'epoch': 0.09}
{'loss': 3.7686, 'grad_norm': 0.6218382641865086, 'learning_rate': 9.850134786859023e-05, 'epoch': 0.09}
{'loss': 3.7727, 'grad_norm': 0.7131513823635052, 'learning_rate': 9.849986669431525e-05, 'epoch': 0.09}
{'loss': 3.8605, 'grad_norm': 0.6785423637571184, 'learning_rate': 9.84983855200403e-05, 'epoch': 0.09}
{'loss': 3.8107, 'grad_norm': 0.6962279025314985, 'learning_rate': 9.849690434576533e-05, 'epoch': 0.09}
{'loss': 3.767, 'grad_norm': 0.6067806709083307, 'learning_rate': 9.849542317149036e-05, 'epoch': 0.09}
{'loss': 3.637, 'grad_norm': 0.6649759394004281, 'learning_rate': 9.84939419972154e-05, 'epoch': 0.09}
{'loss': 3.9396, 'grad_norm': 0.6502164379875592, 'learning_rate': 9.849246082294044e-05, 'epoch': 0.09}
{'loss': 3.8788, 'grad_norm': 0.6485814613153323, 'learning_rate': 9.849097964866547e-05, 'epoch': 0.09}
{'loss': 3.8388, 'grad_norm': 0.622369746012986, 'learning_rate': 9.84894984743905e-05, 'epoch': 0.09}
[2024-04-12 06:25:36,284] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step10100 is about to be saved!
[2024-04-12 06:25:36,290] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-10100/global_step10100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 06:25:36,290] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10100/global_step10100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 06:25:36,296] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10100/global_step10100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 06:25:36,296] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10100/global_step10100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 06:25:39,370] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10100/global_step10100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 06:25:39,371] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-10100/global_step10100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 06:25:39,371] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10100 is ready now!
{'loss': 3.7601, 'grad_norm': 0.6102429949716052, 'learning_rate': 9.848801730011553e-05, 'epoch': 0.09}
{'loss': 3.7724, 'grad_norm': 0.6459306379623824, 'learning_rate': 9.848653612584057e-05, 'epoch': 0.09}
{'loss': 3.6959, 'grad_norm': 0.6717879809051364, 'learning_rate': 9.84850549515656e-05, 'epoch': 0.09}
{'loss': 3.6979, 'grad_norm': 0.8185841553133157, 'learning_rate': 9.848357377729064e-05, 'epoch': 0.09}
{'loss': 3.8286, 'grad_norm': 0.6026682749194674, 'learning_rate': 9.848209260301568e-05, 'epoch': 0.09}
{'loss': 3.8167, 'grad_norm': 0.6648467859191911, 'learning_rate': 9.84806114287407e-05, 'epoch': 0.09}
{'loss': 3.8266, 'grad_norm': 0.6273906887648995, 'learning_rate': 9.847913025446575e-05, 'epoch': 0.09}
{'loss': 3.8041, 'grad_norm': 0.6309041617364026, 'learning_rate': 9.847764908019079e-05, 'epoch': 0.09}
{'loss': 3.9327, 'grad_norm': 0.6387592574816271, 'learning_rate': 9.847616790591581e-05, 'epoch': 0.09}
{'loss': 3.7764, 'grad_norm': 0.6262813099020288, 'learning_rate': 9.847468673164085e-05, 'epoch': 0.09}
{'loss': 3.7322, 'grad_norm': 0.6224336957106714, 'learning_rate': 9.847320555736589e-05, 'epoch': 0.09}
{'loss': 3.8199, 'grad_norm': 0.6362657100799494, 'learning_rate': 9.847172438309092e-05, 'epoch': 0.09}
{'loss': 3.5127, 'grad_norm': 0.6507537839627573, 'learning_rate': 9.847024320881594e-05, 'epoch': 0.09}
{'loss': 3.6951, 'grad_norm': 0.6820691371287603, 'learning_rate': 9.846876203454098e-05, 'epoch': 0.09}
{'loss': 3.7706, 'grad_norm': 0.6259930112200277, 'learning_rate': 9.846728086026602e-05, 'epoch': 0.09}
{'loss': 4.0126, 'grad_norm': 8.555703664261726, 'learning_rate': 9.846579968599105e-05, 'epoch': 0.09}
{'loss': 3.7361, 'grad_norm': 0.6358410467457248, 'learning_rate': 9.846431851171609e-05, 'epoch': 0.09}
{'loss': 3.7219, 'grad_norm': 0.7574908431262676, 'learning_rate': 9.846283733744113e-05, 'epoch': 0.09}
{'loss': 3.8653, 'grad_norm': 0.6090201200311409, 'learning_rate': 9.846135616316616e-05, 'epoch': 0.09}
{'loss': 3.7545, 'grad_norm': 0.6406462452399077, 'learning_rate': 9.84598749888912e-05, 'epoch': 0.09}
[2024-04-12 06:30:16,736] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step10200 is about to be saved!
[2024-04-12 06:30:16,741] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-10200/global_step10200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 06:30:16,741] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10200/global_step10200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 06:30:16,747] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10200/global_step10200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 06:30:16,748] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10200/global_step10200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 06:30:19,776] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10200/global_step10200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 06:30:19,776] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-10200/global_step10200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 06:30:19,776] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10200 is ready now!
{'loss': 3.7127, 'grad_norm': 0.6125124611663172, 'learning_rate': 9.845839381461624e-05, 'epoch': 0.09}
{'loss': 3.6999, 'grad_norm': 0.6484586544550563, 'learning_rate': 9.845691264034126e-05, 'epoch': 0.09}
{'loss': 3.6237, 'grad_norm': 0.6770193081989753, 'learning_rate': 9.84554314660663e-05, 'epoch': 0.09}
{'loss': 3.7208, 'grad_norm': 0.5790362879982075, 'learning_rate': 9.845395029179134e-05, 'epoch': 0.09}
{'loss': 3.85, 'grad_norm': 0.6053400758168207, 'learning_rate': 9.845246911751638e-05, 'epoch': 0.09}
{'loss': 3.8704, 'grad_norm': 0.6410603839184745, 'learning_rate': 9.84509879432414e-05, 'epoch': 0.09}
{'loss': 3.7639, 'grad_norm': 0.6103344900516935, 'learning_rate': 9.844950676896644e-05, 'epoch': 0.09}
{'loss': 3.7059, 'grad_norm': 0.6137477217398247, 'learning_rate': 9.844802559469148e-05, 'epoch': 0.09}
{'loss': 3.8134, 'grad_norm': 0.6497084720783126, 'learning_rate': 9.84465444204165e-05, 'epoch': 0.09}
{'loss': 3.6873, 'grad_norm': 0.6899163625419732, 'learning_rate': 9.844506324614154e-05, 'epoch': 0.09}
{'loss': 3.8811, 'grad_norm': 0.6333386625152796, 'learning_rate': 9.844358207186658e-05, 'epoch': 0.09}
{'loss': 3.7662, 'grad_norm': 0.7145520044599964, 'learning_rate': 9.844210089759161e-05, 'epoch': 0.09}
{'loss': 3.7858, 'grad_norm': 0.5997146600329928, 'learning_rate': 9.844061972331665e-05, 'epoch': 0.09}
{'loss': 3.7116, 'grad_norm': 0.6451236491171174, 'learning_rate': 9.843913854904169e-05, 'epoch': 0.09}
{'loss': 3.6846, 'grad_norm': 0.6472743564111253, 'learning_rate': 9.843765737476673e-05, 'epoch': 0.09}
{'loss': 3.6468, 'grad_norm': 0.746679148102099, 'learning_rate': 9.843617620049176e-05, 'epoch': 0.09}
{'loss': 3.9109, 'grad_norm': 0.6740001756680916, 'learning_rate': 9.84346950262168e-05, 'epoch': 0.09}
{'loss': 3.8931, 'grad_norm': 0.8961691438354382, 'learning_rate': 9.843321385194182e-05, 'epoch': 0.09}
{'loss': 3.8262, 'grad_norm': 0.7112946545172741, 'learning_rate': 9.843173267766685e-05, 'epoch': 0.09}
{'loss': 3.8796, 'grad_norm': 0.6747288288055789, 'learning_rate': 9.843025150339189e-05, 'epoch': 0.09}
[2024-04-12 06:34:57,096] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step10300 is about to be saved!
[2024-04-12 06:34:57,101] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-10300/global_step10300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 06:34:57,101] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10300/global_step10300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 06:34:57,107] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10300/global_step10300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 06:34:57,108] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10300/global_step10300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 06:35:00,198] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10300/global_step10300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 06:35:00,198] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-10300/global_step10300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 06:35:00,199] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10300 is ready now!
{'loss': 3.8803, 'grad_norm': 0.6366726692064396, 'learning_rate': 9.842877032911693e-05, 'epoch': 0.09}
{'loss': 3.8116, 'grad_norm': 0.6085195700405304, 'learning_rate': 9.842728915484197e-05, 'epoch': 0.09}
{'loss': 3.8852, 'grad_norm': 0.6056538808694547, 'learning_rate': 9.8425807980567e-05, 'epoch': 0.09}
{'loss': 3.8084, 'grad_norm': 0.6691755243030586, 'learning_rate': 9.842432680629203e-05, 'epoch': 0.09}
{'loss': 3.841, 'grad_norm': 0.6634845493115905, 'learning_rate': 9.842284563201707e-05, 'epoch': 0.09}
{'loss': 3.7782, 'grad_norm': 0.7109578518249632, 'learning_rate': 9.84213644577421e-05, 'epoch': 0.09}
{'loss': 3.7141, 'grad_norm': 0.6713615020182825, 'learning_rate': 9.841988328346714e-05, 'epoch': 0.09}
{'loss': 3.8113, 'grad_norm': 0.7861482557350877, 'learning_rate': 9.841840210919218e-05, 'epoch': 0.09}
{'loss': 3.7728, 'grad_norm': 0.659071841581487, 'learning_rate': 9.841692093491721e-05, 'epoch': 0.09}
{'loss': 3.7633, 'grad_norm': 0.5920511053128397, 'learning_rate': 9.841543976064225e-05, 'epoch': 0.09}
{'loss': 3.5658, 'grad_norm': 0.6136604232577973, 'learning_rate': 9.841395858636727e-05, 'epoch': 0.09}
{'loss': 3.7807, 'grad_norm': 0.635551345384217, 'learning_rate': 9.841247741209231e-05, 'epoch': 0.09}
{'loss': 3.7667, 'grad_norm': 0.6736312026002405, 'learning_rate': 9.841099623781734e-05, 'epoch': 0.09}
{'loss': 3.7932, 'grad_norm': 0.6706926031968969, 'learning_rate': 9.840951506354238e-05, 'epoch': 0.09}
{'loss': 3.7899, 'grad_norm': 0.671305137982583, 'learning_rate': 9.840803388926742e-05, 'epoch': 0.09}
{'loss': 3.6976, 'grad_norm': 0.6618560556899106, 'learning_rate': 9.840655271499245e-05, 'epoch': 0.09}
{'loss': 3.7476, 'grad_norm': 0.6223402562583895, 'learning_rate': 9.840507154071749e-05, 'epoch': 0.09}
{'loss': 3.8649, 'grad_norm': 0.6307565485016494, 'learning_rate': 9.840359036644253e-05, 'epoch': 0.09}
{'loss': 3.5481, 'grad_norm': 0.6307543190609648, 'learning_rate': 9.840210919216755e-05, 'epoch': 0.09}
{'loss': 3.7808, 'grad_norm': 0.6308227251909705, 'learning_rate': 9.840062801789259e-05, 'epoch': 0.09}
[2024-04-12 06:39:37,252] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step10400 is about to be saved!
[2024-04-12 06:39:37,258] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-10400/global_step10400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 06:39:37,258] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10400/global_step10400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 06:39:37,264] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10400/global_step10400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 06:39:37,264] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10400/global_step10400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 06:39:40,411] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10400/global_step10400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 06:39:40,411] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-10400/global_step10400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 06:39:40,412] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10400 is ready now!
{'loss': 3.6357, 'grad_norm': 0.6463881837627665, 'learning_rate': 9.839914684361763e-05, 'epoch': 0.09}
{'loss': 3.7728, 'grad_norm': 0.6118740324615718, 'learning_rate': 9.839766566934266e-05, 'epoch': 0.09}
{'loss': 3.6649, 'grad_norm': 0.6136495987097939, 'learning_rate': 9.839618449506769e-05, 'epoch': 0.09}
{'loss': 3.5925, 'grad_norm': 0.6518970345394666, 'learning_rate': 9.839470332079273e-05, 'epoch': 0.09}
{'loss': 3.8325, 'grad_norm': 0.6415473672106992, 'learning_rate': 9.839322214651777e-05, 'epoch': 0.09}
{'loss': 3.8117, 'grad_norm': 0.6094994336320634, 'learning_rate': 9.839174097224279e-05, 'epoch': 0.09}
{'loss': 3.6625, 'grad_norm': 0.6297692448151376, 'learning_rate': 9.839025979796783e-05, 'epoch': 0.09}
{'loss': 3.7126, 'grad_norm': 0.655519741364302, 'learning_rate': 9.838877862369287e-05, 'epoch': 0.09}
{'loss': 3.7835, 'grad_norm': 0.6004329215823206, 'learning_rate': 9.83872974494179e-05, 'epoch': 0.09}
{'loss': 3.6959, 'grad_norm': 0.6376550485257175, 'learning_rate': 9.838581627514294e-05, 'epoch': 0.09}
{'loss': 3.8218, 'grad_norm': 0.6304182907106591, 'learning_rate': 9.838433510086798e-05, 'epoch': 0.09}
{'loss': 3.6035, 'grad_norm': 0.6414077143375846, 'learning_rate': 9.8382853926593e-05, 'epoch': 0.09}
{'loss': 3.8375, 'grad_norm': 0.6626222367624853, 'learning_rate': 9.838137275231804e-05, 'epoch': 0.09}
{'loss': 3.7911, 'grad_norm': 0.6197339253551292, 'learning_rate': 9.837989157804308e-05, 'epoch': 0.09}
{'loss': 3.7986, 'grad_norm': 0.6850086901752899, 'learning_rate': 9.837841040376811e-05, 'epoch': 0.09}
{'loss': 3.6438, 'grad_norm': 0.6244528112300789, 'learning_rate': 9.837692922949314e-05, 'epoch': 0.09}
{'loss': 3.7235, 'grad_norm': 0.61208213784456, 'learning_rate': 9.837544805521818e-05, 'epoch': 0.09}
{'loss': 3.6548, 'grad_norm': 0.6244891907003739, 'learning_rate': 9.837396688094322e-05, 'epoch': 0.09}
{'loss': 3.6658, 'grad_norm': 0.6828444913420363, 'learning_rate': 9.837248570666824e-05, 'epoch': 0.09}
{'loss': 3.8467, 'grad_norm': 0.623136991413605, 'learning_rate': 9.837100453239328e-05, 'epoch': 0.09}
[2024-04-12 06:44:17,819] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step10500 is about to be saved!
[2024-04-12 06:44:17,824] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-10500/global_step10500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 06:44:17,824] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10500/global_step10500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 06:44:17,831] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10500/global_step10500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 06:44:17,832] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10500/global_step10500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 06:44:20,845] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10500/global_step10500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 06:44:20,845] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-10500/global_step10500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 06:44:20,846] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10500 is ready now!
{'loss': 3.8209, 'grad_norm': 0.5983525242692006, 'learning_rate': 9.836952335811832e-05, 'epoch': 0.09}
{'loss': 3.6781, 'grad_norm': 0.6158766078393416, 'learning_rate': 9.836804218384335e-05, 'epoch': 0.09}
{'loss': 3.7617, 'grad_norm': 0.6651320222757611, 'learning_rate': 9.836656100956839e-05, 'epoch': 0.09}
{'loss': 3.7965, 'grad_norm': 0.6267291152192962, 'learning_rate': 9.836507983529343e-05, 'epoch': 0.09}
{'loss': 3.8365, 'grad_norm': 0.6173118690814196, 'learning_rate': 9.836359866101847e-05, 'epoch': 0.09}
{'loss': 3.7588, 'grad_norm': 0.8273359787564539, 'learning_rate': 9.83621174867435e-05, 'epoch': 0.09}
{'loss': 3.7532, 'grad_norm': 0.621529750231537, 'learning_rate': 9.836063631246854e-05, 'epoch': 0.09}
{'loss': 3.7603, 'grad_norm': 0.5935165751691042, 'learning_rate': 9.835915513819356e-05, 'epoch': 0.09}
{'loss': 3.8103, 'grad_norm': 0.6220083845883927, 'learning_rate': 9.835767396391859e-05, 'epoch': 0.09}
{'loss': 3.7291, 'grad_norm': 0.621798689626374, 'learning_rate': 9.835619278964363e-05, 'epoch': 0.09}
{'loss': 3.7338, 'grad_norm': 0.6192299808387944, 'learning_rate': 9.835471161536867e-05, 'epoch': 0.09}
{'loss': 3.596, 'grad_norm': 0.6221770562066378, 'learning_rate': 9.83532304410937e-05, 'epoch': 0.09}
{'loss': 3.6674, 'grad_norm': 0.6381074745404446, 'learning_rate': 9.835174926681874e-05, 'epoch': 0.09}
{'loss': 3.6576, 'grad_norm': 0.6216059648932318, 'learning_rate': 9.835026809254378e-05, 'epoch': 0.09}
{'loss': 3.5756, 'grad_norm': 0.6521032715082631, 'learning_rate': 9.834878691826882e-05, 'epoch': 0.09}
{'loss': 3.6897, 'grad_norm': 0.612025277344967, 'learning_rate': 9.834730574399384e-05, 'epoch': 0.09}
{'loss': 3.8879, 'grad_norm': 0.615984352488563, 'learning_rate': 9.834582456971888e-05, 'epoch': 0.09}
{'loss': 3.6634, 'grad_norm': 0.7662193420653695, 'learning_rate': 9.834434339544392e-05, 'epoch': 0.09}
{'loss': 3.8722, 'grad_norm': 0.7037417418621539, 'learning_rate': 9.834286222116895e-05, 'epoch': 0.09}
{'loss': 3.6867, 'grad_norm': 0.6341002865133472, 'learning_rate': 9.834138104689399e-05, 'epoch': 0.09}
[2024-04-12 06:48:58,162] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step10600 is about to be saved!
[2024-04-12 06:48:58,167] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-10600/global_step10600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 06:48:58,167] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10600/global_step10600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 06:48:58,173] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10600/global_step10600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 06:48:58,173] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10600/global_step10600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 06:49:01,259] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10600/global_step10600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 06:49:01,259] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-10600/global_step10600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 06:49:01,260] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10600 is ready now!
{'loss': 3.8817, 'grad_norm': 0.6541592928776111, 'learning_rate': 9.833989987261901e-05, 'epoch': 0.09}
{'loss': 3.7075, 'grad_norm': 0.5724957577290795, 'learning_rate': 9.833841869834405e-05, 'epoch': 0.09}
{'loss': 3.6415, 'grad_norm': 0.7201283592987487, 'learning_rate': 9.833693752406908e-05, 'epoch': 0.09}
{'loss': 3.6073, 'grad_norm': 0.6205040010940835, 'learning_rate': 9.833545634979412e-05, 'epoch': 0.09}
{'loss': 3.7691, 'grad_norm': 0.6152968130406556, 'learning_rate': 9.833397517551916e-05, 'epoch': 0.09}
{'loss': 3.7706, 'grad_norm': 0.6486248646697315, 'learning_rate': 9.833249400124419e-05, 'epoch': 0.09}
{'loss': 3.7237, 'grad_norm': 0.6242107858085848, 'learning_rate': 9.833101282696923e-05, 'epoch': 0.09}
{'loss': 3.7458, 'grad_norm': 0.604773561422268, 'learning_rate': 9.832953165269427e-05, 'epoch': 0.09}
{'loss': 3.8967, 'grad_norm': 0.6043235049066904, 'learning_rate': 9.83280504784193e-05, 'epoch': 0.09}
{'loss': 3.7008, 'grad_norm': 0.8306910156306789, 'learning_rate': 9.832656930414433e-05, 'epoch': 0.09}
{'loss': 3.8477, 'grad_norm': 0.5966212643677429, 'learning_rate': 9.832508812986937e-05, 'epoch': 0.09}
{'loss': 3.6715, 'grad_norm': 0.6269446823744692, 'learning_rate': 9.83236069555944e-05, 'epoch': 0.09}
{'loss': 3.6859, 'grad_norm': 0.6718869058887281, 'learning_rate': 9.832212578131943e-05, 'epoch': 0.09}
{'loss': 3.7661, 'grad_norm': 0.6430769804010632, 'learning_rate': 9.832064460704447e-05, 'epoch': 0.09}
{'loss': 3.685, 'grad_norm': 0.6904779324990196, 'learning_rate': 9.83191634327695e-05, 'epoch': 0.09}
{'loss': 3.8076, 'grad_norm': 0.6107211066027801, 'learning_rate': 9.831768225849453e-05, 'epoch': 0.09}
{'loss': 3.7684, 'grad_norm': 0.638632011049219, 'learning_rate': 9.831620108421957e-05, 'epoch': 0.09}
{'loss': 3.7198, 'grad_norm': 0.6167362861945143, 'learning_rate': 9.831471990994461e-05, 'epoch': 0.09}
{'loss': 3.7061, 'grad_norm': 0.7040974427504856, 'learning_rate': 9.831323873566964e-05, 'epoch': 0.09}
{'loss': 3.6944, 'grad_norm': 0.7959008506995556, 'learning_rate': 9.831175756139468e-05, 'epoch': 0.09}
[2024-04-12 06:53:38,572] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step10700 is about to be saved!
[2024-04-12 06:53:38,579] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-10700/global_step10700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 06:53:38,579] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10700/global_step10700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 06:53:38,585] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10700/global_step10700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 06:53:38,585] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10700/global_step10700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 06:53:41,655] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10700/global_step10700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 06:53:41,655] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-10700/global_step10700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 06:53:41,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10700 is ready now!
{'loss': 3.6254, 'grad_norm': 0.6375756059749447, 'learning_rate': 9.831027638711972e-05, 'epoch': 0.09}
{'loss': 3.6585, 'grad_norm': 0.746966352303828, 'learning_rate': 9.830879521284475e-05, 'epoch': 0.09}
{'loss': 3.8333, 'grad_norm': 0.6413823697924931, 'learning_rate': 9.830731403856979e-05, 'epoch': 0.09}
{'loss': 3.6595, 'grad_norm': 0.5980680399807278, 'learning_rate': 9.830583286429483e-05, 'epoch': 0.09}
{'loss': 3.6097, 'grad_norm': 0.7071102887680688, 'learning_rate': 9.830435169001985e-05, 'epoch': 0.09}
{'loss': 3.7972, 'grad_norm': 0.6195341373117436, 'learning_rate': 9.830287051574488e-05, 'epoch': 0.09}
{'loss': 3.7294, 'grad_norm': 0.6045786597736843, 'learning_rate': 9.830138934146992e-05, 'epoch': 0.09}
{'loss': 3.6966, 'grad_norm': 0.6637912530185875, 'learning_rate': 9.829990816719496e-05, 'epoch': 0.09}
{'loss': 3.8277, 'grad_norm': 0.7576934243127496, 'learning_rate': 9.829842699291998e-05, 'epoch': 0.09}
{'loss': 3.6747, 'grad_norm': 0.6260988216492855, 'learning_rate': 9.829694581864502e-05, 'epoch': 0.09}
{'loss': 3.5592, 'grad_norm': 0.5903735038165098, 'learning_rate': 9.829546464437006e-05, 'epoch': 0.09}
{'loss': 3.7527, 'grad_norm': 0.5966055958103245, 'learning_rate': 9.829398347009509e-05, 'epoch': 0.09}
{'loss': 3.6817, 'grad_norm': 0.6219006429750611, 'learning_rate': 9.829250229582013e-05, 'epoch': 0.09}
{'loss': 3.6818, 'grad_norm': 0.6009081357365035, 'learning_rate': 9.829102112154517e-05, 'epoch': 0.09}
{'loss': 3.604, 'grad_norm': 0.6147100419040956, 'learning_rate': 9.82895399472702e-05, 'epoch': 0.09}
{'loss': 3.5086, 'grad_norm': 0.6336023414985491, 'learning_rate': 9.828805877299524e-05, 'epoch': 0.09}
{'loss': 3.8475, 'grad_norm': 0.6340228370086499, 'learning_rate': 9.828657759872028e-05, 'epoch': 0.09}
{'loss': 3.6811, 'grad_norm': 0.6982547885397039, 'learning_rate': 9.82850964244453e-05, 'epoch': 0.09}
{'loss': 3.7854, 'grad_norm': 0.6509506577191506, 'learning_rate': 9.828361525017033e-05, 'epoch': 0.09}
{'loss': 3.6831, 'grad_norm': 0.6680564292007986, 'learning_rate': 9.828213407589537e-05, 'epoch': 0.09}
[2024-04-12 06:58:18,987] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step10800 is about to be saved!
[2024-04-12 06:58:18,994] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-10800/global_step10800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 06:58:18,994] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10800/global_step10800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 06:58:19,000] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10800/global_step10800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 06:58:19,000] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10800/global_step10800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 06:58:22,186] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10800/global_step10800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 06:58:22,186] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-10800/global_step10800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 06:58:22,187] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10800 is ready now!
{'loss': 3.7373, 'grad_norm': 0.9198751957726073, 'learning_rate': 9.828065290162041e-05, 'epoch': 0.09}
{'loss': 3.5772, 'grad_norm': 0.7697863071775525, 'learning_rate': 9.827917172734544e-05, 'epoch': 0.09}
{'loss': 3.8104, 'grad_norm': 0.6190695233056228, 'learning_rate': 9.827769055307048e-05, 'epoch': 0.09}
{'loss': 3.7927, 'grad_norm': 0.6308401403372005, 'learning_rate': 9.827620937879552e-05, 'epoch': 0.09}
{'loss': 3.7054, 'grad_norm': 0.6793286406297684, 'learning_rate': 9.827472820452054e-05, 'epoch': 0.09}
{'loss': 3.7106, 'grad_norm': 0.623877868699227, 'learning_rate': 9.827324703024558e-05, 'epoch': 0.09}
{'loss': 3.7992, 'grad_norm': 0.6792823621172384, 'learning_rate': 9.827176585597062e-05, 'epoch': 0.09}
{'loss': 3.7772, 'grad_norm': 0.8500111394875328, 'learning_rate': 9.827028468169566e-05, 'epoch': 0.09}
{'loss': 3.6954, 'grad_norm': 0.6075617853053462, 'learning_rate': 9.826880350742069e-05, 'epoch': 0.09}
{'loss': 3.8208, 'grad_norm': 0.6393790175989802, 'learning_rate': 9.826732233314572e-05, 'epoch': 0.1}
{'loss': 3.5417, 'grad_norm': 0.6505187578031809, 'learning_rate': 9.826584115887076e-05, 'epoch': 0.1}
{'loss': 3.4936, 'grad_norm': 0.6628818104088592, 'learning_rate': 9.826435998459578e-05, 'epoch': 0.1}
{'loss': 3.6918, 'grad_norm': 0.6221091556396424, 'learning_rate': 9.826287881032082e-05, 'epoch': 0.1}
{'loss': 3.6579, 'grad_norm': 0.6302695864598102, 'learning_rate': 9.826139763604586e-05, 'epoch': 0.1}
{'loss': 3.6157, 'grad_norm': 0.7212566669419599, 'learning_rate': 9.82599164617709e-05, 'epoch': 0.1}
{'loss': 3.7901, 'grad_norm': 0.6458642504263551, 'learning_rate': 9.825843528749593e-05, 'epoch': 0.1}
{'loss': 3.5822, 'grad_norm': 0.6098655681573393, 'learning_rate': 9.825695411322097e-05, 'epoch': 0.1}
{'loss': 3.673, 'grad_norm': 0.6600027674627121, 'learning_rate': 9.825547293894601e-05, 'epoch': 0.1}
{'loss': 3.7623, 'grad_norm': 0.6424940103386033, 'learning_rate': 9.825399176467103e-05, 'epoch': 0.1}
{'loss': 3.7041, 'grad_norm': 0.7403114727434732, 'learning_rate': 9.825251059039607e-05, 'epoch': 0.1}
[2024-04-12 07:02:59,458] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step10900 is about to be saved!
[2024-04-12 07:02:59,464] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-10900/global_step10900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 07:02:59,464] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10900/global_step10900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 07:02:59,470] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10900/global_step10900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 07:02:59,470] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10900/global_step10900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 07:03:02,613] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10900/global_step10900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 07:03:02,613] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-10900/global_step10900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 07:03:02,613] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10900 is ready now!
{'loss': 3.7516, 'grad_norm': 0.6010647313782086, 'learning_rate': 9.825102941612111e-05, 'epoch': 0.1}
{'loss': 3.828, 'grad_norm': 0.6243973910642702, 'learning_rate': 9.824954824184614e-05, 'epoch': 0.1}
{'loss': 3.6661, 'grad_norm': 0.6076953042092239, 'learning_rate': 9.824806706757117e-05, 'epoch': 0.1}
{'loss': 3.6169, 'grad_norm': 0.6085504982465247, 'learning_rate': 9.824658589329621e-05, 'epoch': 0.1}
{'loss': 3.6563, 'grad_norm': 0.6105093536262506, 'learning_rate': 9.824510471902125e-05, 'epoch': 0.1}
{'loss': 3.7143, 'grad_norm': 0.6230675655608646, 'learning_rate': 9.824362354474627e-05, 'epoch': 0.1}
{'loss': 3.5775, 'grad_norm': 0.6809517780394416, 'learning_rate': 9.824214237047131e-05, 'epoch': 0.1}
{'loss': 3.4726, 'grad_norm': 0.7271504931365852, 'learning_rate': 9.824066119619635e-05, 'epoch': 0.1}
{'loss': 3.6166, 'grad_norm': 0.6208793127837774, 'learning_rate': 9.823918002192138e-05, 'epoch': 0.1}
{'loss': 3.5471, 'grad_norm': 0.5938806767214865, 'learning_rate': 9.823769884764642e-05, 'epoch': 0.1}
{'loss': 3.7099, 'grad_norm': 0.6322954575320839, 'learning_rate': 9.823621767337146e-05, 'epoch': 0.1}
{'loss': 3.7299, 'grad_norm': 0.6091062117505194, 'learning_rate': 9.823473649909649e-05, 'epoch': 0.1}
{'loss': 3.575, 'grad_norm': 0.6988273328051632, 'learning_rate': 9.823325532482153e-05, 'epoch': 0.1}
{'loss': 3.7852, 'grad_norm': 0.6811736531079983, 'learning_rate': 9.823177415054657e-05, 'epoch': 0.1}
{'loss': 3.732, 'grad_norm': 0.6341700714539201, 'learning_rate': 9.823029297627159e-05, 'epoch': 0.1}
{'loss': 3.6704, 'grad_norm': 0.6728018262176604, 'learning_rate': 9.822881180199662e-05, 'epoch': 0.1}
{'loss': 3.6866, 'grad_norm': 0.678094164417033, 'learning_rate': 9.822733062772166e-05, 'epoch': 0.1}
{'loss': 3.7591, 'grad_norm': 0.6862211055096906, 'learning_rate': 9.82258494534467e-05, 'epoch': 0.1}
{'loss': 3.6223, 'grad_norm': 0.6376335233016355, 'learning_rate': 9.822436827917173e-05, 'epoch': 0.1}
{'loss': 3.6756, 'grad_norm': 0.6496989939701137, 'learning_rate': 9.822288710489677e-05, 'epoch': 0.1}
[2024-04-12 07:07:39,834] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step11000 is about to be saved!
[2024-04-12 07:07:39,840] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-11000/global_step11000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 07:07:39,840] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11000/global_step11000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 07:07:39,846] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11000/global_step11000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 07:07:39,846] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11000/global_step11000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 07:07:42,883] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11000/global_step11000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 07:07:42,883] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-11000/global_step11000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 07:07:42,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
{'loss': 3.6303, 'grad_norm': 0.582838327461406, 'learning_rate': 9.82214059306218e-05, 'epoch': 0.1}
{'loss': 3.6093, 'grad_norm': 0.642124897489354, 'learning_rate': 9.821992475634683e-05, 'epoch': 0.1}
{'loss': 3.6712, 'grad_norm': 0.6153410477909036, 'learning_rate': 9.821844358207187e-05, 'epoch': 0.1}
{'loss': 3.6595, 'grad_norm': 0.6203957856778999, 'learning_rate': 9.821696240779691e-05, 'epoch': 0.1}
{'loss': 3.645, 'grad_norm': 0.5756099690299192, 'learning_rate': 9.821548123352194e-05, 'epoch': 0.1}
{'loss': 3.6074, 'grad_norm': 0.6681371681896882, 'learning_rate': 9.821400005924698e-05, 'epoch': 0.1}
{'loss': 3.5958, 'grad_norm': 0.6150465908609861, 'learning_rate': 9.821251888497202e-05, 'epoch': 0.1}
{'loss': 3.5376, 'grad_norm': 0.7555132539276751, 'learning_rate': 9.821103771069704e-05, 'epoch': 0.1}
{'loss': 3.5883, 'grad_norm': 0.5948533197284817, 'learning_rate': 9.820955653642207e-05, 'epoch': 0.1}
{'loss': 3.6579, 'grad_norm': 0.6662428166952122, 'learning_rate': 9.820807536214711e-05, 'epoch': 0.1}
{'loss': 3.671, 'grad_norm': 0.6129851390971282, 'learning_rate': 9.820659418787215e-05, 'epoch': 0.1}
{'loss': 3.6988, 'grad_norm': 0.5981245865146031, 'learning_rate': 9.820511301359718e-05, 'epoch': 0.1}
{'loss': 3.5604, 'grad_norm': 0.8431420100452304, 'learning_rate': 9.820363183932222e-05, 'epoch': 0.1}
{'loss': 3.856, 'grad_norm': 0.5894162374630314, 'learning_rate': 9.820215066504726e-05, 'epoch': 0.1}
{'loss': 3.6225, 'grad_norm': 0.5848275812992336, 'learning_rate': 9.820066949077228e-05, 'epoch': 0.1}
{'loss': 3.79, 'grad_norm': 0.6717859912645402, 'learning_rate': 9.819918831649732e-05, 'epoch': 0.1}
{'loss': 3.6283, 'grad_norm': 0.6347309976339701, 'learning_rate': 9.819770714222236e-05, 'epoch': 0.1}
{'loss': 3.5682, 'grad_norm': 0.7534509232109167, 'learning_rate': 9.81962259679474e-05, 'epoch': 0.1}
{'loss': 3.6728, 'grad_norm': 0.7143110585486013, 'learning_rate': 9.819474479367243e-05, 'epoch': 0.1}
{'loss': 3.4151, 'grad_norm': 0.7359394958547159, 'learning_rate': 9.819326361939746e-05, 'epoch': 0.1}
[2024-04-12 07:12:20,187] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step11100 is about to be saved!
[2024-04-12 07:12:20,192] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-11100/global_step11100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 07:12:20,192] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11100/global_step11100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 07:12:20,198] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11100/global_step11100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 07:12:20,198] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11100/global_step11100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 07:12:23,280] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11100/global_step11100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 07:12:23,280] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-11100/global_step11100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 07:12:23,281] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11100 is ready now!
{'loss': 3.7904, 'grad_norm': 0.6233237088086492, 'learning_rate': 9.81917824451225e-05, 'epoch': 0.1}
{'loss': 3.6213, 'grad_norm': 0.6293165238818322, 'learning_rate': 9.819030127084752e-05, 'epoch': 0.1}
{'loss': 3.6076, 'grad_norm': 0.6604151349452011, 'learning_rate': 9.818882009657256e-05, 'epoch': 0.1}
{'loss': 3.7983, 'grad_norm': 0.6651588608651581, 'learning_rate': 9.81873389222976e-05, 'epoch': 0.1}
{'loss': 3.7189, 'grad_norm': 0.6394371669538832, 'learning_rate': 9.818585774802263e-05, 'epoch': 0.1}
{'loss': 3.6837, 'grad_norm': 0.5916462811897373, 'learning_rate': 9.818437657374767e-05, 'epoch': 0.1}
{'loss': 3.6279, 'grad_norm': 0.6769438510725979, 'learning_rate': 9.818289539947271e-05, 'epoch': 0.1}
{'loss': 3.709, 'grad_norm': 0.59739811391669, 'learning_rate': 9.818141422519775e-05, 'epoch': 0.1}
{'loss': 3.6147, 'grad_norm': 0.7510354730257239, 'learning_rate': 9.817993305092278e-05, 'epoch': 0.1}
{'loss': 3.8009, 'grad_norm': 0.5964081772009052, 'learning_rate': 9.817845187664782e-05, 'epoch': 0.1}
{'loss': 3.76, 'grad_norm': 0.5817415614123485, 'learning_rate': 9.817697070237286e-05, 'epoch': 0.1}
{'loss': 3.6929, 'grad_norm': 0.6073951072621718, 'learning_rate': 9.817548952809788e-05, 'epoch': 0.1}
{'loss': 3.6687, 'grad_norm': 0.6503739615293758, 'learning_rate': 9.817400835382291e-05, 'epoch': 0.1}
{'loss': 3.6074, 'grad_norm': 0.6296197265601833, 'learning_rate': 9.817252717954795e-05, 'epoch': 0.1}
{'loss': 3.6899, 'grad_norm': 0.6635123849746548, 'learning_rate': 9.817104600527299e-05, 'epoch': 0.1}
{'loss': 3.5156, 'grad_norm': 0.6258076724960832, 'learning_rate': 9.816956483099801e-05, 'epoch': 0.1}
{'loss': 3.6601, 'grad_norm': 0.6271963542109938, 'learning_rate': 9.816808365672305e-05, 'epoch': 0.1}
{'loss': 3.5273, 'grad_norm': 0.7243289996192932, 'learning_rate': 9.81666024824481e-05, 'epoch': 0.1}
{'loss': 3.7198, 'grad_norm': 0.8816756770471954, 'learning_rate': 9.816512130817312e-05, 'epoch': 0.1}
{'loss': 3.7334, 'grad_norm': 0.6696167056644827, 'learning_rate': 9.816364013389816e-05, 'epoch': 0.1}
[2024-04-12 07:17:00,692] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step11200 is about to be saved!
[2024-04-12 07:17:00,697] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-11200/global_step11200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 07:17:00,698] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11200/global_step11200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 07:17:00,704] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11200/global_step11200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 07:17:00,704] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11200/global_step11200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 07:17:03,806] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11200/global_step11200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 07:17:03,806] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-11200/global_step11200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 07:17:03,807] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11200 is ready now!
{'loss': 3.7065, 'grad_norm': 0.6509284554946351, 'learning_rate': 9.81621589596232e-05, 'epoch': 0.1}
{'loss': 3.7212, 'grad_norm': 0.6851270830343209, 'learning_rate': 9.816067778534823e-05, 'epoch': 0.1}
{'loss': 3.7419, 'grad_norm': 0.6639130496032781, 'learning_rate': 9.815919661107327e-05, 'epoch': 0.1}
{'loss': 3.457, 'grad_norm': 0.6378585327436472, 'learning_rate': 9.815771543679831e-05, 'epoch': 0.1}
{'loss': 3.6797, 'grad_norm': 0.6053354730165773, 'learning_rate': 9.815623426252333e-05, 'epoch': 0.1}
{'loss': 3.7179, 'grad_norm': 0.6264948613314788, 'learning_rate': 9.815475308824836e-05, 'epoch': 0.1}
{'loss': 3.7664, 'grad_norm': 0.6330051808314887, 'learning_rate': 9.81532719139734e-05, 'epoch': 0.1}
{'loss': 3.6901, 'grad_norm': 0.7207061305527778, 'learning_rate': 9.815179073969844e-05, 'epoch': 0.1}
{'loss': 3.7262, 'grad_norm': 0.6312434427912775, 'learning_rate': 9.815030956542347e-05, 'epoch': 0.1}
{'loss': 3.6776, 'grad_norm': 0.6040219525086966, 'learning_rate': 9.81488283911485e-05, 'epoch': 0.1}
{'loss': 3.638, 'grad_norm': 0.7265624688989312, 'learning_rate': 9.814734721687355e-05, 'epoch': 0.1}
{'loss': 3.7679, 'grad_norm': 0.6278854699650573, 'learning_rate': 9.814586604259857e-05, 'epoch': 0.1}
{'loss': 3.5962, 'grad_norm': 0.8573395163279206, 'learning_rate': 9.814438486832361e-05, 'epoch': 0.1}
{'loss': 3.7742, 'grad_norm': 0.6503012737694827, 'learning_rate': 9.814290369404865e-05, 'epoch': 0.1}
{'loss': 3.6872, 'grad_norm': 0.6107006875678198, 'learning_rate': 9.814142251977368e-05, 'epoch': 0.1}
{'loss': 3.6364, 'grad_norm': 0.6086886153953266, 'learning_rate': 9.813994134549872e-05, 'epoch': 0.1}
{'loss': 3.6059, 'grad_norm': 0.7158759306902115, 'learning_rate': 9.813846017122376e-05, 'epoch': 0.1}
{'loss': 3.6457, 'grad_norm': 0.654778566536963, 'learning_rate': 9.813697899694879e-05, 'epoch': 0.1}
{'loss': 3.6225, 'grad_norm': 0.5891247604737021, 'learning_rate': 9.813549782267381e-05, 'epoch': 0.1}
{'loss': 3.7031, 'grad_norm': 0.5954430113016387, 'learning_rate': 9.813401664839885e-05, 'epoch': 0.1}
[2024-04-12 07:21:41,150] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step11300 is about to be saved!
[2024-04-12 07:21:41,155] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-11300/global_step11300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 07:21:41,155] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11300/global_step11300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 07:21:41,161] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11300/global_step11300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 07:21:41,162] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11300/global_step11300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 07:21:44,276] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11300/global_step11300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 07:21:44,276] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-11300/global_step11300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 07:21:44,277] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11300 is ready now!
{'loss': 3.732, 'grad_norm': 0.7223154069728664, 'learning_rate': 9.813253547412389e-05, 'epoch': 0.1}
{'loss': 3.7462, 'grad_norm': 0.6356304137403184, 'learning_rate': 9.813105429984892e-05, 'epoch': 0.1}
{'loss': 3.6809, 'grad_norm': 0.6525829400818608, 'learning_rate': 9.812957312557396e-05, 'epoch': 0.1}
{'loss': 3.5482, 'grad_norm': 0.6646871637695406, 'learning_rate': 9.8128091951299e-05, 'epoch': 0.1}
{'loss': 3.7233, 'grad_norm': 0.5959807843076667, 'learning_rate': 9.812661077702402e-05, 'epoch': 0.1}
{'loss': 3.6867, 'grad_norm': 1.1351445524421437, 'learning_rate': 9.812512960274906e-05, 'epoch': 0.1}
{'loss': 3.5502, 'grad_norm': 0.6149171958136067, 'learning_rate': 9.81236484284741e-05, 'epoch': 0.1}
{'loss': 3.7032, 'grad_norm': 0.6549754962464706, 'learning_rate': 9.812216725419913e-05, 'epoch': 0.1}
{'loss': 3.7387, 'grad_norm': 0.6250920362477722, 'learning_rate': 9.812068607992417e-05, 'epoch': 0.1}
{'loss': 3.6275, 'grad_norm': 0.7620776488186282, 'learning_rate': 9.81192049056492e-05, 'epoch': 0.1}
{'loss': 3.7546, 'grad_norm': 0.6427282783993965, 'learning_rate': 9.811772373137424e-05, 'epoch': 0.1}
{'loss': 3.5379, 'grad_norm': 0.5870292577806969, 'learning_rate': 9.811624255709926e-05, 'epoch': 0.1}
{'loss': 3.7443, 'grad_norm': 0.6503908913577655, 'learning_rate': 9.81147613828243e-05, 'epoch': 0.1}
{'loss': 3.5059, 'grad_norm': 0.7034854107936616, 'learning_rate': 9.811328020854934e-05, 'epoch': 0.1}
{'loss': 3.7066, 'grad_norm': 0.5731321025961982, 'learning_rate': 9.811179903427437e-05, 'epoch': 0.1}
{'loss': 3.6918, 'grad_norm': 0.5964391656607547, 'learning_rate': 9.811031785999941e-05, 'epoch': 0.1}
{'loss': 3.824, 'grad_norm': 0.6451403481558998, 'learning_rate': 9.810883668572445e-05, 'epoch': 0.1}
{'loss': 3.6377, 'grad_norm': 0.5942927017003627, 'learning_rate': 9.810735551144949e-05, 'epoch': 0.1}
{'loss': 3.5312, 'grad_norm': 0.5956042942084008, 'learning_rate': 9.810587433717452e-05, 'epoch': 0.1}
{'loss': 3.6635, 'grad_norm': 0.5976894957576896, 'learning_rate': 9.810439316289956e-05, 'epoch': 0.1}
[2024-04-12 07:26:21,582] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step11400 is about to be saved!
[2024-04-12 07:26:21,588] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-11400/global_step11400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 07:26:21,588] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11400/global_step11400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 07:26:21,594] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11400/global_step11400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 07:26:21,595] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11400/global_step11400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 07:26:24,685] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11400/global_step11400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 07:26:24,685] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-11400/global_step11400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 07:26:24,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11400 is ready now!
{'loss': 3.7644, 'grad_norm': 0.6795802530936377, 'learning_rate': 9.81029119886246e-05, 'epoch': 0.1}
{'loss': 3.5974, 'grad_norm': 0.6217357892839503, 'learning_rate': 9.810143081434962e-05, 'epoch': 0.1}
{'loss': 3.6148, 'grad_norm': 0.5751065352154843, 'learning_rate': 9.809994964007465e-05, 'epoch': 0.1}
{'loss': 3.6365, 'grad_norm': 0.5911753944213383, 'learning_rate': 9.809846846579969e-05, 'epoch': 0.1}
{'loss': 3.5466, 'grad_norm': 0.6262847412081269, 'learning_rate': 9.809698729152472e-05, 'epoch': 0.1}
{'loss': 3.5764, 'grad_norm': 0.6165163395620451, 'learning_rate': 9.809550611724976e-05, 'epoch': 0.1}
{'loss': 3.7076, 'grad_norm': 0.6719891081169855, 'learning_rate': 9.80940249429748e-05, 'epoch': 0.1}
{'loss': 3.6428, 'grad_norm': 0.6136742205732864, 'learning_rate': 9.809254376869984e-05, 'epoch': 0.1}
{'loss': 3.6035, 'grad_norm': 0.6113437050771358, 'learning_rate': 9.809106259442486e-05, 'epoch': 0.1}
{'loss': 3.7982, 'grad_norm': 0.5799410749696601, 'learning_rate': 9.80895814201499e-05, 'epoch': 0.1}
{'loss': 3.7435, 'grad_norm': 0.6086158376574233, 'learning_rate': 9.808810024587494e-05, 'epoch': 0.1}
{'loss': 3.6794, 'grad_norm': 0.8623781919837478, 'learning_rate': 9.808661907159997e-05, 'epoch': 0.1}
{'loss': 3.7979, 'grad_norm': 0.6279408122417404, 'learning_rate': 9.808513789732501e-05, 'epoch': 0.1}
{'loss': 3.5684, 'grad_norm': 0.5794012976528157, 'learning_rate': 9.808365672305005e-05, 'epoch': 0.1}
{'loss': 3.8961, 'grad_norm': 0.5959834239370101, 'learning_rate': 9.808217554877507e-05, 'epoch': 0.1}
{'loss': 3.6636, 'grad_norm': 0.6051097636937742, 'learning_rate': 9.80806943745001e-05, 'epoch': 0.1}
{'loss': 3.5913, 'grad_norm': 0.600566765295154, 'learning_rate': 9.807921320022514e-05, 'epoch': 0.1}
{'loss': 3.684, 'grad_norm': 0.6727903725542311, 'learning_rate': 9.807773202595018e-05, 'epoch': 0.1}
{'loss': 3.6386, 'grad_norm': 0.6262196706663693, 'learning_rate': 9.807625085167521e-05, 'epoch': 0.1}
{'loss': 3.6155, 'grad_norm': 0.6138331871433625, 'learning_rate': 9.807476967740025e-05, 'epoch': 0.1}
[2024-04-12 07:31:01,961] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step11500 is about to be saved!
[2024-04-12 07:31:01,966] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-11500/global_step11500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 07:31:01,966] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11500/global_step11500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 07:31:01,973] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11500/global_step11500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 07:31:01,973] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11500/global_step11500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 07:31:05,068] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11500/global_step11500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 07:31:05,068] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-11500/global_step11500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 07:31:05,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11500 is ready now!
{'loss': 3.5422, 'grad_norm': 0.6189719215337836, 'learning_rate': 9.807328850312529e-05, 'epoch': 0.1}
{'loss': 3.7195, 'grad_norm': 0.6251353462627509, 'learning_rate': 9.807180732885031e-05, 'epoch': 0.1}
{'loss': 3.8238, 'grad_norm': 0.6565008660710838, 'learning_rate': 9.807032615457535e-05, 'epoch': 0.1}
{'loss': 3.59, 'grad_norm': 0.6599700670350653, 'learning_rate': 9.80688449803004e-05, 'epoch': 0.1}
{'loss': 3.6856, 'grad_norm': 0.7345470073046741, 'learning_rate': 9.806736380602542e-05, 'epoch': 0.1}
{'loss': 3.6137, 'grad_norm': 0.7375086183864243, 'learning_rate': 9.806588263175046e-05, 'epoch': 0.1}
{'loss': 3.7266, 'grad_norm': 0.6860989735889373, 'learning_rate': 9.80644014574755e-05, 'epoch': 0.1}
{'loss': 3.5069, 'grad_norm': 0.6384066253040784, 'learning_rate': 9.806292028320053e-05, 'epoch': 0.1}
{'loss': 3.688, 'grad_norm': 0.6402756158697759, 'learning_rate': 9.806143910892555e-05, 'epoch': 0.1}
{'loss': 3.6954, 'grad_norm': 0.6121253786617922, 'learning_rate': 9.805995793465059e-05, 'epoch': 0.1}
{'loss': 3.7975, 'grad_norm': 0.6833857512862213, 'learning_rate': 9.805847676037563e-05, 'epoch': 0.1}
{'loss': 3.6208, 'grad_norm': 0.6579821869373367, 'learning_rate': 9.805699558610066e-05, 'epoch': 0.1}
{'loss': 3.8459, 'grad_norm': 0.6200961980118176, 'learning_rate': 9.80555144118257e-05, 'epoch': 0.1}
{'loss': 3.6609, 'grad_norm': 0.6456945340849539, 'learning_rate': 9.805403323755074e-05, 'epoch': 0.1}
{'loss': 3.7304, 'grad_norm': 0.7788281318890072, 'learning_rate': 9.805255206327577e-05, 'epoch': 0.1}
{'loss': 3.586, 'grad_norm': 0.6050627866531889, 'learning_rate': 9.80510708890008e-05, 'epoch': 0.1}
{'loss': 3.4771, 'grad_norm': 0.6179230090156612, 'learning_rate': 9.804958971472585e-05, 'epoch': 0.1}
{'loss': 3.5, 'grad_norm': 0.5814401233890694, 'learning_rate': 9.804810854045087e-05, 'epoch': 0.1}
{'loss': 3.6609, 'grad_norm': 0.6580299880168524, 'learning_rate': 9.804662736617591e-05, 'epoch': 0.1}
{'loss': 3.7393, 'grad_norm': 0.614087073696505, 'learning_rate': 9.804514619190094e-05, 'epoch': 0.1}
[2024-04-12 07:35:42,396] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step11600 is about to be saved!
[2024-04-12 07:35:42,401] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-11600/global_step11600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 07:35:42,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11600/global_step11600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 07:35:42,408] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11600/global_step11600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 07:35:42,408] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11600/global_step11600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 07:35:45,522] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11600/global_step11600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 07:35:45,522] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-11600/global_step11600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 07:35:45,522] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11600 is ready now!
{'loss': 3.688, 'grad_norm': 0.8231219855866254, 'learning_rate': 9.804366501762598e-05, 'epoch': 0.1}
{'loss': 3.6805, 'grad_norm': 0.6082115244058207, 'learning_rate': 9.8042183843351e-05, 'epoch': 0.1}
{'loss': 3.5934, 'grad_norm': 0.7920584812662075, 'learning_rate': 9.804070266907604e-05, 'epoch': 0.1}
{'loss': 3.7612, 'grad_norm': 0.6036058172515778, 'learning_rate': 9.803922149480108e-05, 'epoch': 0.1}
{'loss': 3.5384, 'grad_norm': 0.6330882079997796, 'learning_rate': 9.803774032052611e-05, 'epoch': 0.1}
{'loss': 3.5822, 'grad_norm': 0.5946042428821334, 'learning_rate': 9.803625914625115e-05, 'epoch': 0.1}
{'loss': 3.5523, 'grad_norm': 0.6633675635397954, 'learning_rate': 9.803477797197619e-05, 'epoch': 0.1}
{'loss': 3.4247, 'grad_norm': 0.6058505094041352, 'learning_rate': 9.803329679770122e-05, 'epoch': 0.1}
{'loss': 3.6174, 'grad_norm': 0.7086068746621899, 'learning_rate': 9.803181562342626e-05, 'epoch': 0.1}
{'loss': 3.6984, 'grad_norm': 0.719545469782157, 'learning_rate': 9.80303344491513e-05, 'epoch': 0.1}
{'loss': 3.7629, 'grad_norm': 0.6579229888197093, 'learning_rate': 9.802885327487634e-05, 'epoch': 0.1}
{'loss': 3.6356, 'grad_norm': 0.6646921884724528, 'learning_rate': 9.802737210060136e-05, 'epoch': 0.1}
{'loss': 3.6442, 'grad_norm': 0.6190007780392247, 'learning_rate': 9.802589092632639e-05, 'epoch': 0.1}
{'loss': 3.6409, 'grad_norm': 0.6490033384292503, 'learning_rate': 9.802440975205143e-05, 'epoch': 0.1}
{'loss': 3.4884, 'grad_norm': 0.6128521159848751, 'learning_rate': 9.802292857777646e-05, 'epoch': 0.1}
{'loss': 3.7397, 'grad_norm': 0.675690958871385, 'learning_rate': 9.80214474035015e-05, 'epoch': 0.1}
{'loss': 3.6175, 'grad_norm': 0.6028854725622289, 'learning_rate': 9.801996622922654e-05, 'epoch': 0.1}
{'loss': 3.5117, 'grad_norm': 0.6565190926353371, 'learning_rate': 9.801848505495158e-05, 'epoch': 0.1}
{'loss': 3.5336, 'grad_norm': 0.6943874161946872, 'learning_rate': 9.80170038806766e-05, 'epoch': 0.1}
{'loss': 3.5735, 'grad_norm': 0.6292455534130336, 'learning_rate': 9.801552270640164e-05, 'epoch': 0.1}
[2024-04-12 07:40:22,802] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step11700 is about to be saved!
[2024-04-12 07:40:22,807] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-11700/global_step11700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 07:40:22,807] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11700/global_step11700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 07:40:22,813] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11700/global_step11700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 07:40:22,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11700/global_step11700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 07:40:25,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11700/global_step11700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 07:40:25,936] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-11700/global_step11700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 07:40:25,937] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11700 is ready now!
{'loss': 3.5196, 'grad_norm': 0.5804109307758871, 'learning_rate': 9.801404153212668e-05, 'epoch': 0.1}
{'loss': 3.7505, 'grad_norm': 0.6353122295294023, 'learning_rate': 9.801256035785171e-05, 'epoch': 0.1}
{'loss': 3.5901, 'grad_norm': 0.6100677590333213, 'learning_rate': 9.801107918357675e-05, 'epoch': 0.1}
{'loss': 3.7223, 'grad_norm': 0.5926480868784325, 'learning_rate': 9.800959800930179e-05, 'epoch': 0.1}
{'loss': 3.5037, 'grad_norm': 0.6991226435339304, 'learning_rate': 9.80081168350268e-05, 'epoch': 0.1}
{'loss': 3.7363, 'grad_norm': 0.6103729318637643, 'learning_rate': 9.800663566075184e-05, 'epoch': 0.1}
{'loss': 3.6738, 'grad_norm': 0.5913446354910815, 'learning_rate': 9.800515448647688e-05, 'epoch': 0.1}
{'loss': 3.726, 'grad_norm': 0.6728895869746918, 'learning_rate': 9.800367331220192e-05, 'epoch': 0.1}
{'loss': 3.6374, 'grad_norm': 0.5928087815373695, 'learning_rate': 9.800219213792695e-05, 'epoch': 0.1}
{'loss': 3.6425, 'grad_norm': 0.6074631698799196, 'learning_rate': 9.800071096365199e-05, 'epoch': 0.1}
{'loss': 3.6018, 'grad_norm': 0.605747894069679, 'learning_rate': 9.799922978937703e-05, 'epoch': 0.1}
{'loss': 3.6538, 'grad_norm': 0.6003628572615443, 'learning_rate': 9.799774861510205e-05, 'epoch': 0.1}
{'loss': 3.6396, 'grad_norm': 0.5794616456202667, 'learning_rate': 9.79962674408271e-05, 'epoch': 0.1}
{'loss': 3.6176, 'grad_norm': 0.6115966625994836, 'learning_rate': 9.799478626655213e-05, 'epoch': 0.1}
{'loss': 3.6359, 'grad_norm': 0.6059690577913455, 'learning_rate': 9.799330509227716e-05, 'epoch': 0.1}
{'loss': 3.7039, 'grad_norm': 0.6424819814832429, 'learning_rate': 9.79918239180022e-05, 'epoch': 0.1}
{'loss': 3.6411, 'grad_norm': 0.604383375836881, 'learning_rate': 9.799034274372724e-05, 'epoch': 0.1}
{'loss': 3.6547, 'grad_norm': 0.581272275471325, 'learning_rate': 9.798886156945227e-05, 'epoch': 0.1}
{'loss': 3.5489, 'grad_norm': 0.6092269472116145, 'learning_rate': 9.79873803951773e-05, 'epoch': 0.1}
{'loss': 3.6994, 'grad_norm': 0.6430784259557842, 'learning_rate': 9.798589922090233e-05, 'epoch': 0.1}
[2024-04-12 07:45:03,266] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step11800 is about to be saved!
[2024-04-12 07:45:03,271] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-11800/global_step11800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 07:45:03,271] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11800/global_step11800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 07:45:03,277] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11800/global_step11800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 07:45:03,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11800/global_step11800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 07:45:06,433] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11800/global_step11800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 07:45:06,433] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-11800/global_step11800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 07:45:06,434] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11800 is ready now!
{'loss': 3.5735, 'grad_norm': 0.6018270188342013, 'learning_rate': 9.798441804662737e-05, 'epoch': 0.1}
{'loss': 3.5526, 'grad_norm': 0.6119136746482081, 'learning_rate': 9.79829368723524e-05, 'epoch': 0.1}
{'loss': 3.6899, 'grad_norm': 0.61177299305909, 'learning_rate': 9.798145569807744e-05, 'epoch': 0.1}
{'loss': 3.6037, 'grad_norm': 0.6026425606324877, 'learning_rate': 9.797997452380248e-05, 'epoch': 0.1}
{'loss': 3.604, 'grad_norm': 0.6702273190959879, 'learning_rate': 9.79784933495275e-05, 'epoch': 0.1}
{'loss': 3.6344, 'grad_norm': 0.6829488289920147, 'learning_rate': 9.797701217525255e-05, 'epoch': 0.1}
{'loss': 3.6755, 'grad_norm': 0.589394440942454, 'learning_rate': 9.797553100097759e-05, 'epoch': 0.1}
{'loss': 3.774, 'grad_norm': 0.6240220338567894, 'learning_rate': 9.797404982670261e-05, 'epoch': 0.1}
{'loss': 3.726, 'grad_norm': 0.5931706781618064, 'learning_rate': 9.797256865242765e-05, 'epoch': 0.1}
{'loss': 3.7077, 'grad_norm': 0.6416866416670179, 'learning_rate': 9.797108747815268e-05, 'epoch': 0.1}
{'loss': 3.6186, 'grad_norm': 0.6401978914100823, 'learning_rate': 9.796960630387772e-05, 'epoch': 0.1}
{'loss': 3.7629, 'grad_norm': 0.596393436540122, 'learning_rate': 9.796812512960275e-05, 'epoch': 0.1}
{'loss': 3.7234, 'grad_norm': 0.6091931359447895, 'learning_rate': 9.796664395532779e-05, 'epoch': 0.1}
{'loss': 3.4631, 'grad_norm': 0.5941323040923517, 'learning_rate': 9.796516278105283e-05, 'epoch': 0.1}
{'loss': 3.5217, 'grad_norm': 0.6040423468425526, 'learning_rate': 9.796368160677785e-05, 'epoch': 0.1}
{'loss': 3.6028, 'grad_norm': 0.6562371186868686, 'learning_rate': 9.796220043250289e-05, 'epoch': 0.1}
{'loss': 3.5603, 'grad_norm': 0.6674129900375896, 'learning_rate': 9.796071925822793e-05, 'epoch': 0.1}
{'loss': 3.6073, 'grad_norm': 0.6185059118573718, 'learning_rate': 9.795923808395296e-05, 'epoch': 0.1}
{'loss': 3.8364, 'grad_norm': 0.6863633924708475, 'learning_rate': 9.7957756909678e-05, 'epoch': 0.1}
{'loss': 3.7191, 'grad_norm': 0.6651193022164557, 'learning_rate': 9.795627573540304e-05, 'epoch': 0.1}
[2024-04-12 07:49:43,582] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step11900 is about to be saved!
[2024-04-12 07:49:43,587] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-11900/global_step11900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 07:49:43,587] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11900/global_step11900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 07:49:43,593] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11900/global_step11900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 07:49:43,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-11900/global_step11900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 07:49:46,691] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-11900/global_step11900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 07:49:46,691] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-11900/global_step11900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 07:49:46,692] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11900 is ready now!
{'loss': 3.5725, 'grad_norm': 0.6141767781134051, 'learning_rate': 9.795479456112806e-05, 'epoch': 0.1}
{'loss': 3.6483, 'grad_norm': 0.6085930241409225, 'learning_rate': 9.79533133868531e-05, 'epoch': 0.1}
{'loss': 3.7504, 'grad_norm': 0.5792836661618618, 'learning_rate': 9.795183221257813e-05, 'epoch': 0.1}
{'loss': 3.5855, 'grad_norm': 0.5857356340578971, 'learning_rate': 9.795035103830317e-05, 'epoch': 0.1}
{'loss': 3.7986, 'grad_norm': 0.7414748790705623, 'learning_rate': 9.79488698640282e-05, 'epoch': 0.1}
{'loss': 3.5153, 'grad_norm': 0.6252898284626939, 'learning_rate': 9.794738868975324e-05, 'epoch': 0.1}
{'loss': 3.6663, 'grad_norm': 0.5708848625579533, 'learning_rate': 9.794590751547828e-05, 'epoch': 0.1}
{'loss': 3.624, 'grad_norm': 0.6663239260048319, 'learning_rate': 9.79444263412033e-05, 'epoch': 0.1}
{'loss': 3.5935, 'grad_norm': 0.5975079130725394, 'learning_rate': 9.794294516692834e-05, 'epoch': 0.1}
{'loss': 3.7673, 'grad_norm': 0.6603366061470446, 'learning_rate': 9.794146399265338e-05, 'epoch': 0.1}
{'loss': 3.5239, 'grad_norm': 0.6564622341092502, 'learning_rate': 9.793998281837842e-05, 'epoch': 0.1}
{'loss': 3.5531, 'grad_norm': 0.633945172743689, 'learning_rate': 9.793850164410345e-05, 'epoch': 0.1}
{'loss': 3.6221, 'grad_norm': 0.5787285561525713, 'learning_rate': 9.793702046982849e-05, 'epoch': 0.1}
{'loss': 3.6961, 'grad_norm': 0.6236569914500911, 'learning_rate': 9.793553929555353e-05, 'epoch': 0.1}
{'loss': 3.6041, 'grad_norm': 0.6772525620110434, 'learning_rate': 9.793405812127854e-05, 'epoch': 0.1}
{'loss': 3.6509, 'grad_norm': 0.6208935252887487, 'learning_rate': 9.793257694700358e-05, 'epoch': 0.1}
{'loss': 3.3536, 'grad_norm': 0.6183495259073223, 'learning_rate': 9.793109577272862e-05, 'epoch': 0.1}
{'loss': 3.6888, 'grad_norm': 0.6559408015810402, 'learning_rate': 9.792961459845365e-05, 'epoch': 0.1}
{'loss': 3.6211, 'grad_norm': 0.5742144722815324, 'learning_rate': 9.792813342417869e-05, 'epoch': 0.11}
{'loss': 3.5249, 'grad_norm': 0.6143056659527328, 'learning_rate': 9.792665224990373e-05, 'epoch': 0.11}
[2024-04-12 07:54:23,838] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step12000 is about to be saved!
[2024-04-12 07:54:23,844] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-12000/global_step12000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 07:54:23,844] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12000/global_step12000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 07:54:23,850] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12000/global_step12000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 07:54:23,850] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12000/global_step12000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 07:54:26,929] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12000/global_step12000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 07:54:26,930] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-12000/global_step12000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 07:54:26,930] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
{'loss': 3.6706, 'grad_norm': 0.6284558270113678, 'learning_rate': 9.792517107562877e-05, 'epoch': 0.11}
{'loss': 3.6597, 'grad_norm': 0.5875063568750126, 'learning_rate': 9.79236899013538e-05, 'epoch': 0.11}
{'loss': 3.6579, 'grad_norm': 0.6696799562663411, 'learning_rate': 9.792220872707884e-05, 'epoch': 0.11}
{'loss': 3.7437, 'grad_norm': 0.650301865551242, 'learning_rate': 9.792072755280388e-05, 'epoch': 0.11}
{'loss': 3.6894, 'grad_norm': 0.6771540522945921, 'learning_rate': 9.79192463785289e-05, 'epoch': 0.11}
{'loss': 3.6674, 'grad_norm': 0.618817093860327, 'learning_rate': 9.791776520425394e-05, 'epoch': 0.11}
{'loss': 3.7554, 'grad_norm': 0.6799692046041713, 'learning_rate': 9.791628402997898e-05, 'epoch': 0.11}
{'loss': 3.6702, 'grad_norm': 0.5760188305590427, 'learning_rate': 9.791480285570401e-05, 'epoch': 0.11}
{'loss': 3.5999, 'grad_norm': 0.6072008143604836, 'learning_rate': 9.791332168142903e-05, 'epoch': 0.11}
{'loss': 3.4283, 'grad_norm': 0.7701852892757801, 'learning_rate': 9.791184050715407e-05, 'epoch': 0.11}
{'loss': 3.6426, 'grad_norm': 0.6027241607151007, 'learning_rate': 9.791035933287911e-05, 'epoch': 0.11}
{'loss': 3.5758, 'grad_norm': 0.5935490349112623, 'learning_rate': 9.790887815860414e-05, 'epoch': 0.11}
{'loss': 3.6777, 'grad_norm': 0.6233758426592501, 'learning_rate': 9.790739698432918e-05, 'epoch': 0.11}
{'loss': 3.4649, 'grad_norm': 0.6388613882892429, 'learning_rate': 9.790591581005422e-05, 'epoch': 0.11}
{'loss': 3.6578, 'grad_norm': 0.590183084822463, 'learning_rate': 9.790443463577925e-05, 'epoch': 0.11}
{'loss': 3.5832, 'grad_norm': 0.6266414108667383, 'learning_rate': 9.790295346150429e-05, 'epoch': 0.11}
{'loss': 3.6188, 'grad_norm': 0.7066120728969494, 'learning_rate': 9.790147228722933e-05, 'epoch': 0.11}
{'loss': 3.5092, 'grad_norm': 0.7822407504413534, 'learning_rate': 9.789999111295435e-05, 'epoch': 0.11}
{'loss': 3.6605, 'grad_norm': 0.6258398730051354, 'learning_rate': 9.78985099386794e-05, 'epoch': 0.11}
{'loss': 3.6915, 'grad_norm': 0.6571981335077838, 'learning_rate': 9.789702876440442e-05, 'epoch': 0.11}
[2024-04-12 07:59:03,745] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step12100 is about to be saved!
[2024-04-12 07:59:03,750] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-12100/global_step12100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 07:59:03,750] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12100/global_step12100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 07:59:03,756] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12100/global_step12100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 07:59:03,757] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12100/global_step12100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 07:59:06,856] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12100/global_step12100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 07:59:06,856] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-12100/global_step12100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 07:59:06,857] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12100 is ready now!
{'loss': 3.6197, 'grad_norm': 0.6360107121918689, 'learning_rate': 9.789554759012946e-05, 'epoch': 0.11}
{'loss': 3.5115, 'grad_norm': 0.6111478577938818, 'learning_rate': 9.789406641585449e-05, 'epoch': 0.11}
{'loss': 3.7084, 'grad_norm': 0.6665559645556055, 'learning_rate': 9.789258524157953e-05, 'epoch': 0.11}
{'loss': 3.6519, 'grad_norm': 0.6329511072569292, 'learning_rate': 9.789110406730457e-05, 'epoch': 0.11}
{'loss': 3.5971, 'grad_norm': 1.4858376323079052, 'learning_rate': 9.788962289302959e-05, 'epoch': 0.11}
{'loss': 3.6076, 'grad_norm': 0.5870739283926406, 'learning_rate': 9.788814171875463e-05, 'epoch': 0.11}
{'loss': 3.6209, 'grad_norm': 0.594771601559405, 'learning_rate': 9.788666054447967e-05, 'epoch': 0.11}
{'loss': 3.5821, 'grad_norm': 0.592315171007178, 'learning_rate': 9.78851793702047e-05, 'epoch': 0.11}
{'loss': 3.716, 'grad_norm': 0.6173935730864278, 'learning_rate': 9.788369819592974e-05, 'epoch': 0.11}
{'loss': 3.6722, 'grad_norm': 0.6219194791859314, 'learning_rate': 9.788221702165478e-05, 'epoch': 0.11}
{'loss': 3.545, 'grad_norm': 0.682276040377563, 'learning_rate': 9.78807358473798e-05, 'epoch': 0.11}
{'loss': 3.7768, 'grad_norm': 0.6433006275881019, 'learning_rate': 9.787925467310485e-05, 'epoch': 0.11}
{'loss': 3.5764, 'grad_norm': 0.6779198860741793, 'learning_rate': 9.787777349882987e-05, 'epoch': 0.11}
{'loss': 3.7119, 'grad_norm': 0.5914125721960659, 'learning_rate': 9.787629232455491e-05, 'epoch': 0.11}
{'loss': 3.7006, 'grad_norm': 0.6460757485991464, 'learning_rate': 9.787481115027994e-05, 'epoch': 0.11}
{'loss': 3.6454, 'grad_norm': 0.6784282792891455, 'learning_rate': 9.787332997600498e-05, 'epoch': 0.11}
{'loss': 3.6008, 'grad_norm': 0.6296836408057935, 'learning_rate': 9.787184880173002e-05, 'epoch': 0.11}
{'loss': 3.5299, 'grad_norm': 0.6280909348880236, 'learning_rate': 9.787036762745504e-05, 'epoch': 0.11}
{'loss': 3.485, 'grad_norm': 0.6352296064305524, 'learning_rate': 9.786888645318008e-05, 'epoch': 0.11}
{'loss': 3.6237, 'grad_norm': 0.6159516977674753, 'learning_rate': 9.786740527890512e-05, 'epoch': 0.11}
[2024-04-12 08:03:43,687] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step12200 is about to be saved!
[2024-04-12 08:03:43,693] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-12200/global_step12200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 08:03:43,693] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12200/global_step12200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 08:03:43,699] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12200/global_step12200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 08:03:43,699] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12200/global_step12200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 08:03:46,727] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12200/global_step12200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 08:03:46,727] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-12200/global_step12200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 08:03:46,728] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12200 is ready now!
{'loss': 3.7225, 'grad_norm': 0.6368602011236418, 'learning_rate': 9.786592410463015e-05, 'epoch': 0.11}
{'loss': 3.4822, 'grad_norm': 0.6128756765252705, 'learning_rate': 9.786444293035519e-05, 'epoch': 0.11}
{'loss': 3.5609, 'grad_norm': 0.6151725578261197, 'learning_rate': 9.786296175608023e-05, 'epoch': 0.11}
{'loss': 3.5788, 'grad_norm': 0.6367232657574953, 'learning_rate': 9.786148058180527e-05, 'epoch': 0.11}
{'loss': 3.6295, 'grad_norm': 0.652516386111054, 'learning_rate': 9.785999940753028e-05, 'epoch': 0.11}
{'loss': 3.652, 'grad_norm': 0.6408377045865533, 'learning_rate': 9.785851823325532e-05, 'epoch': 0.11}
{'loss': 3.5649, 'grad_norm': 1.170426537602674, 'learning_rate': 9.785703705898036e-05, 'epoch': 0.11}
{'loss': 3.5965, 'grad_norm': 0.6083012154863552, 'learning_rate': 9.785555588470539e-05, 'epoch': 0.11}
{'loss': 3.5795, 'grad_norm': 0.7194485587683479, 'learning_rate': 9.785407471043043e-05, 'epoch': 0.11}
{'loss': 3.6086, 'grad_norm': 0.6041476959626543, 'learning_rate': 9.785259353615547e-05, 'epoch': 0.11}
{'loss': 3.5638, 'grad_norm': 0.6114157360884257, 'learning_rate': 9.785111236188051e-05, 'epoch': 0.11}
{'loss': 3.555, 'grad_norm': 0.6720965948528654, 'learning_rate': 9.784963118760554e-05, 'epoch': 0.11}
{'loss': 3.625, 'grad_norm': 0.6171376686861654, 'learning_rate': 9.784815001333058e-05, 'epoch': 0.11}
{'loss': 3.6148, 'grad_norm': 0.6247199321788802, 'learning_rate': 9.784666883905562e-05, 'epoch': 0.11}
{'loss': 3.4312, 'grad_norm': 0.5830668110936125, 'learning_rate': 9.784518766478064e-05, 'epoch': 0.11}
{'loss': 3.6665, 'grad_norm': 0.6138420479034127, 'learning_rate': 9.784370649050568e-05, 'epoch': 0.11}
{'loss': 3.6427, 'grad_norm': 0.6327497847000862, 'learning_rate': 9.784222531623072e-05, 'epoch': 0.11}
{'loss': 3.6635, 'grad_norm': 0.5909703151901007, 'learning_rate': 9.784074414195574e-05, 'epoch': 0.11}
{'loss': 3.7164, 'grad_norm': 0.6245996957589963, 'learning_rate': 9.783926296768078e-05, 'epoch': 0.11}
{'loss': 3.4815, 'grad_norm': 0.6373051767072954, 'learning_rate': 9.783778179340582e-05, 'epoch': 0.11}
[2024-04-12 08:08:23,471] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step12300 is about to be saved!
[2024-04-12 08:08:23,477] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-12300/global_step12300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 08:08:23,477] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12300/global_step12300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 08:08:23,483] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12300/global_step12300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 08:08:23,483] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12300/global_step12300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 08:08:26,574] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12300/global_step12300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 08:08:26,574] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-12300/global_step12300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 08:08:26,574] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12300 is ready now!
{'loss': 3.5801, 'grad_norm': 0.6178714162432806, 'learning_rate': 9.783630061913086e-05, 'epoch': 0.11}
{'loss': 3.4591, 'grad_norm': 0.6098453886202126, 'learning_rate': 9.783481944485588e-05, 'epoch': 0.11}
{'loss': 3.5725, 'grad_norm': 0.6092506709418857, 'learning_rate': 9.783333827058092e-05, 'epoch': 0.11}
{'loss': 3.5779, 'grad_norm': 0.6662292495114551, 'learning_rate': 9.783185709630596e-05, 'epoch': 0.11}
{'loss': 3.6629, 'grad_norm': 0.6121670370153341, 'learning_rate': 9.783037592203099e-05, 'epoch': 0.11}
{'loss': 3.6363, 'grad_norm': 0.5827828275021013, 'learning_rate': 9.782889474775603e-05, 'epoch': 0.11}
{'loss': 3.5898, 'grad_norm': 0.7180877780548193, 'learning_rate': 9.782741357348107e-05, 'epoch': 0.11}
{'loss': 3.4777, 'grad_norm': 0.6012612531971361, 'learning_rate': 9.78259323992061e-05, 'epoch': 0.11}
{'loss': 3.5073, 'grad_norm': 1.0117821544563161, 'learning_rate': 9.782445122493113e-05, 'epoch': 0.11}
{'loss': 3.6423, 'grad_norm': 0.7290193950253433, 'learning_rate': 9.782297005065616e-05, 'epoch': 0.11}
{'loss': 3.6305, 'grad_norm': 0.6489622773378457, 'learning_rate': 9.78214888763812e-05, 'epoch': 0.11}
{'loss': 3.6482, 'grad_norm': 0.5685999112877136, 'learning_rate': 9.782000770210623e-05, 'epoch': 0.11}
{'loss': 3.7746, 'grad_norm': 0.6177105241026304, 'learning_rate': 9.781852652783127e-05, 'epoch': 0.11}
{'loss': 3.6559, 'grad_norm': 0.6609998646884504, 'learning_rate': 9.781704535355631e-05, 'epoch': 0.11}
{'loss': 3.6271, 'grad_norm': 0.6278674574285618, 'learning_rate': 9.781556417928133e-05, 'epoch': 0.11}
{'loss': 3.6466, 'grad_norm': 0.6630282852311897, 'learning_rate': 9.781408300500637e-05, 'epoch': 0.11}
{'loss': 3.6274, 'grad_norm': 0.6058314923346328, 'learning_rate': 9.781260183073141e-05, 'epoch': 0.11}
{'loss': 3.5167, 'grad_norm': 0.7005715006168057, 'learning_rate': 9.781112065645644e-05, 'epoch': 0.11}
{'loss': 3.3906, 'grad_norm': 0.6090903203348311, 'learning_rate': 9.780963948218148e-05, 'epoch': 0.11}
{'loss': 3.6554, 'grad_norm': 0.5971775538486885, 'learning_rate': 9.780815830790652e-05, 'epoch': 0.11}
[2024-04-12 08:13:03,306] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step12400 is about to be saved!
[2024-04-12 08:13:03,311] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-12400/global_step12400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 08:13:03,311] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12400/global_step12400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 08:13:03,317] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12400/global_step12400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 08:13:03,318] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12400/global_step12400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 08:13:06,394] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12400/global_step12400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 08:13:06,394] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-12400/global_step12400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 08:13:06,394] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12400 is ready now!
{'loss': 3.617, 'grad_norm': 0.8690754587817968, 'learning_rate': 9.780667713363155e-05, 'epoch': 0.11}
{'loss': 3.5111, 'grad_norm': 0.6070780051881982, 'learning_rate': 9.780519595935659e-05, 'epoch': 0.11}
{'loss': 3.5421, 'grad_norm': 0.6337212117982631, 'learning_rate': 9.780371478508161e-05, 'epoch': 0.11}
{'loss': 3.6442, 'grad_norm': 0.6260944193407153, 'learning_rate': 9.780223361080665e-05, 'epoch': 0.11}
{'loss': 3.7075, 'grad_norm': 0.7955131770472066, 'learning_rate': 9.780075243653168e-05, 'epoch': 0.11}
{'loss': 3.5976, 'grad_norm': 0.5688154828659746, 'learning_rate': 9.779927126225672e-05, 'epoch': 0.11}
{'loss': 3.5962, 'grad_norm': 0.681198585227657, 'learning_rate': 9.779779008798176e-05, 'epoch': 0.11}
{'loss': 3.5241, 'grad_norm': 0.6178635276723055, 'learning_rate': 9.779630891370679e-05, 'epoch': 0.11}
{'loss': 3.6491, 'grad_norm': 0.6293863879950377, 'learning_rate': 9.779482773943183e-05, 'epoch': 0.11}
{'loss': 3.7318, 'grad_norm': 0.6352343019860801, 'learning_rate': 9.779334656515687e-05, 'epoch': 0.11}
{'loss': 3.6682, 'grad_norm': 0.6477634795383913, 'learning_rate': 9.779186539088189e-05, 'epoch': 0.11}
{'loss': 3.6115, 'grad_norm': 0.61779210247261, 'learning_rate': 9.779038421660693e-05, 'epoch': 0.11}
{'loss': 3.6854, 'grad_norm': 0.6537554117333846, 'learning_rate': 9.778890304233197e-05, 'epoch': 0.11}
{'loss': 3.6187, 'grad_norm': 0.6045555380226292, 'learning_rate': 9.7787421868057e-05, 'epoch': 0.11}
{'loss': 3.5687, 'grad_norm': 0.5918478813182141, 'learning_rate': 9.778594069378203e-05, 'epoch': 0.11}
{'loss': 3.7491, 'grad_norm': 0.6107248546855323, 'learning_rate': 9.778445951950706e-05, 'epoch': 0.11}
{'loss': 3.3839, 'grad_norm': 0.6284179400644545, 'learning_rate': 9.77829783452321e-05, 'epoch': 0.11}
{'loss': 3.5011, 'grad_norm': 0.6994430284466282, 'learning_rate': 9.778149717095713e-05, 'epoch': 0.11}
{'loss': 3.6192, 'grad_norm': 0.6443697666212401, 'learning_rate': 9.778001599668217e-05, 'epoch': 0.11}
{'loss': 3.5529, 'grad_norm': 0.6414583288746506, 'learning_rate': 9.777853482240721e-05, 'epoch': 0.11}
[2024-04-12 08:17:42,806] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step12500 is about to be saved!
[2024-04-12 08:17:42,811] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-12500/global_step12500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 08:17:42,811] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12500/global_step12500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 08:17:42,817] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12500/global_step12500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 08:17:42,818] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12500/global_step12500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 08:17:45,879] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12500/global_step12500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 08:17:45,879] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-12500/global_step12500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 08:17:45,880] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12500 is ready now!
{'loss': 3.6707, 'grad_norm': 0.7207889275066273, 'learning_rate': 9.777705364813224e-05, 'epoch': 0.11}
{'loss': 3.6004, 'grad_norm': 0.6508498226714947, 'learning_rate': 9.777557247385728e-05, 'epoch': 0.11}
{'loss': 3.6203, 'grad_norm': 0.6498671510824745, 'learning_rate': 9.777409129958232e-05, 'epoch': 0.11}
{'loss': 3.4372, 'grad_norm': 0.6049136315130271, 'learning_rate': 9.777261012530736e-05, 'epoch': 0.11}
{'loss': 3.5246, 'grad_norm': 0.6623832048762468, 'learning_rate': 9.777112895103238e-05, 'epoch': 0.11}
{'loss': 3.5723, 'grad_norm': 0.6164988528680593, 'learning_rate': 9.776964777675742e-05, 'epoch': 0.11}
{'loss': 3.5351, 'grad_norm': 0.6646999375843026, 'learning_rate': 9.776816660248246e-05, 'epoch': 0.11}
{'loss': 3.6321, 'grad_norm': 0.6236291687725661, 'learning_rate': 9.776668542820748e-05, 'epoch': 0.11}
{'loss': 3.6659, 'grad_norm': 0.7176565065970375, 'learning_rate': 9.776520425393252e-05, 'epoch': 0.11}
{'loss': 3.7461, 'grad_norm': 1.025419968305792, 'learning_rate': 9.776372307965756e-05, 'epoch': 0.11}
{'loss': 3.4456, 'grad_norm': 0.6018704067102416, 'learning_rate': 9.77622419053826e-05, 'epoch': 0.11}
{'loss': 3.6388, 'grad_norm': 0.6251458744086099, 'learning_rate': 9.776076073110762e-05, 'epoch': 0.11}
{'loss': 3.5103, 'grad_norm': 0.76758326588267, 'learning_rate': 9.775927955683266e-05, 'epoch': 0.11}
{'loss': 3.6213, 'grad_norm': 0.6007722490854436, 'learning_rate': 9.77577983825577e-05, 'epoch': 0.11}
{'loss': 3.6468, 'grad_norm': 0.6504605867891432, 'learning_rate': 9.775631720828273e-05, 'epoch': 0.11}
{'loss': 3.4178, 'grad_norm': 0.6440559538084029, 'learning_rate': 9.775483603400777e-05, 'epoch': 0.11}
{'loss': 3.6179, 'grad_norm': 0.5908402413674096, 'learning_rate': 9.775335485973281e-05, 'epoch': 0.11}
{'loss': 3.6812, 'grad_norm': 0.6574866563093528, 'learning_rate': 9.775187368545784e-05, 'epoch': 0.11}
{'loss': 3.4667, 'grad_norm': 0.7537831426472904, 'learning_rate': 9.775039251118288e-05, 'epoch': 0.11}
{'loss': 3.4257, 'grad_norm': 0.6818503817225644, 'learning_rate': 9.77489113369079e-05, 'epoch': 0.11}
[2024-04-12 08:22:22,213] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step12600 is about to be saved!
[2024-04-12 08:22:22,219] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-12600/global_step12600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 08:22:22,219] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12600/global_step12600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 08:22:22,225] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12600/global_step12600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 08:22:22,225] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12600/global_step12600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 08:22:25,444] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12600/global_step12600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 08:22:25,444] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-12600/global_step12600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 08:22:25,445] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12600 is ready now!
{'loss': 3.6804, 'grad_norm': 0.6432549436405177, 'learning_rate': 9.774743016263294e-05, 'epoch': 0.11}
{'loss': 3.6165, 'grad_norm': 0.5751171517339552, 'learning_rate': 9.774594898835797e-05, 'epoch': 0.11}
{'loss': 3.62, 'grad_norm': 0.5812740228363648, 'learning_rate': 9.774446781408301e-05, 'epoch': 0.11}
{'loss': 3.5294, 'grad_norm': 0.6641551951012392, 'learning_rate': 9.774298663980805e-05, 'epoch': 0.11}
{'loss': 3.4032, 'grad_norm': 0.5813750494102968, 'learning_rate': 9.774150546553307e-05, 'epoch': 0.11}
{'loss': 3.5352, 'grad_norm': 0.6050066360442029, 'learning_rate': 9.774002429125811e-05, 'epoch': 0.11}
{'loss': 3.6387, 'grad_norm': 0.6642152417597196, 'learning_rate': 9.773854311698315e-05, 'epoch': 0.11}
{'loss': 3.7017, 'grad_norm': 0.5780910236882504, 'learning_rate': 9.773706194270818e-05, 'epoch': 0.11}
{'loss': 3.5197, 'grad_norm': 0.6430648648432369, 'learning_rate': 9.773558076843322e-05, 'epoch': 0.11}
{'loss': 3.6409, 'grad_norm': 0.6257131988760628, 'learning_rate': 9.773409959415826e-05, 'epoch': 0.11}
{'loss': 3.5637, 'grad_norm': 0.6157850086523686, 'learning_rate': 9.773261841988329e-05, 'epoch': 0.11}
{'loss': 3.5209, 'grad_norm': 0.5844782509547981, 'learning_rate': 9.773113724560831e-05, 'epoch': 0.11}
{'loss': 3.6232, 'grad_norm': 0.6314432370421864, 'learning_rate': 9.772965607133335e-05, 'epoch': 0.11}
{'loss': 3.4994, 'grad_norm': 0.5766017578920173, 'learning_rate': 9.77281748970584e-05, 'epoch': 0.11}
{'loss': 3.3137, 'grad_norm': 0.5966236396790543, 'learning_rate': 9.772669372278342e-05, 'epoch': 0.11}
{'loss': 3.69, 'grad_norm': 0.6438392294929046, 'learning_rate': 9.772521254850846e-05, 'epoch': 0.11}
{'loss': 3.7447, 'grad_norm': 0.6519131787023008, 'learning_rate': 9.77237313742335e-05, 'epoch': 0.11}
{'loss': 3.4412, 'grad_norm': 0.6922887501655787, 'learning_rate': 9.772225019995853e-05, 'epoch': 0.11}
{'loss': 3.6786, 'grad_norm': 0.6727865918697888, 'learning_rate': 9.772076902568357e-05, 'epoch': 0.11}
{'loss': 3.5663, 'grad_norm': 0.6264518730819196, 'learning_rate': 9.77192878514086e-05, 'epoch': 0.11}
[2024-04-12 08:27:01,630] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step12700 is about to be saved!
[2024-04-12 08:27:01,636] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-12700/global_step12700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 08:27:01,636] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12700/global_step12700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 08:27:01,642] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12700/global_step12700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 08:27:01,643] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12700/global_step12700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 08:27:04,695] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12700/global_step12700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 08:27:04,695] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-12700/global_step12700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 08:27:04,696] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12700 is ready now!
{'loss': 3.4263, 'grad_norm': 0.6694105105792538, 'learning_rate': 9.771780667713363e-05, 'epoch': 0.11}
{'loss': 3.6511, 'grad_norm': 0.6220668355324662, 'learning_rate': 9.771632550285867e-05, 'epoch': 0.11}
{'loss': 3.5062, 'grad_norm': 0.6986824227743254, 'learning_rate': 9.771484432858371e-05, 'epoch': 0.11}
{'loss': 3.5297, 'grad_norm': 0.6662266010016822, 'learning_rate': 9.771336315430874e-05, 'epoch': 0.11}
{'loss': 3.554, 'grad_norm': 0.6209613278323666, 'learning_rate': 9.771188198003377e-05, 'epoch': 0.11}
{'loss': 3.5678, 'grad_norm': 0.6115789403810913, 'learning_rate': 9.77104008057588e-05, 'epoch': 0.11}
{'loss': 3.5798, 'grad_norm': 0.6472857317461596, 'learning_rate': 9.770891963148385e-05, 'epoch': 0.11}
{'loss': 3.621, 'grad_norm': 0.6101041484639366, 'learning_rate': 9.770743845720887e-05, 'epoch': 0.11}
{'loss': 3.7487, 'grad_norm': 0.6357438024148079, 'learning_rate': 9.770595728293391e-05, 'epoch': 0.11}
{'loss': 3.5536, 'grad_norm': 0.7352416716423679, 'learning_rate': 9.770447610865895e-05, 'epoch': 0.11}
{'loss': 3.5169, 'grad_norm': 0.6467684190668135, 'learning_rate': 9.770299493438398e-05, 'epoch': 0.11}
{'loss': 3.5773, 'grad_norm': 0.6064122219903686, 'learning_rate': 9.770151376010902e-05, 'epoch': 0.11}
{'loss': 3.6654, 'grad_norm': 0.6125745637402259, 'learning_rate': 9.770003258583406e-05, 'epoch': 0.11}
{'loss': 3.5693, 'grad_norm': 0.672104417795751, 'learning_rate': 9.769855141155909e-05, 'epoch': 0.11}
{'loss': 3.6806, 'grad_norm': 0.589742177710047, 'learning_rate': 9.769707023728412e-05, 'epoch': 0.11}
{'loss': 3.4563, 'grad_norm': 0.6145524310050131, 'learning_rate': 9.769558906300916e-05, 'epoch': 0.11}
{'loss': 3.6609, 'grad_norm': 0.6120934707596302, 'learning_rate': 9.769410788873419e-05, 'epoch': 0.11}
{'loss': 3.632, 'grad_norm': 0.60197660760077, 'learning_rate': 9.769262671445922e-05, 'epoch': 0.11}
{'loss': 3.5691, 'grad_norm': 0.5995680653479117, 'learning_rate': 9.769114554018426e-05, 'epoch': 0.11}
{'loss': 3.6145, 'grad_norm': 0.6557055783094271, 'learning_rate': 9.76896643659093e-05, 'epoch': 0.11}
[2024-04-12 08:31:41,228] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step12800 is about to be saved!
[2024-04-12 08:31:41,234] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-12800/global_step12800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 08:31:41,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12800/global_step12800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 08:31:41,240] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12800/global_step12800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 08:31:41,240] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12800/global_step12800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 08:31:44,286] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12800/global_step12800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 08:31:44,286] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-12800/global_step12800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 08:31:44,286] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12800 is ready now!
{'loss': 3.7431, 'grad_norm': 0.5977737890483846, 'learning_rate': 9.768818319163432e-05, 'epoch': 0.11}
{'loss': 3.5656, 'grad_norm': 0.624297541640813, 'learning_rate': 9.768670201735936e-05, 'epoch': 0.11}
{'loss': 3.3952, 'grad_norm': 0.6335898307687983, 'learning_rate': 9.76852208430844e-05, 'epoch': 0.11}
{'loss': 3.6067, 'grad_norm': 0.6811904725052059, 'learning_rate': 9.768373966880944e-05, 'epoch': 0.11}
{'loss': 3.6367, 'grad_norm': 0.6327926731249237, 'learning_rate': 9.768225849453447e-05, 'epoch': 0.11}
{'loss': 3.5734, 'grad_norm': 0.6173908689838802, 'learning_rate': 9.768077732025951e-05, 'epoch': 0.11}
{'loss': 3.507, 'grad_norm': 0.6301904329853435, 'learning_rate': 9.767929614598455e-05, 'epoch': 0.11}
{'loss': 3.6148, 'grad_norm': 0.6476284971003958, 'learning_rate': 9.767781497170958e-05, 'epoch': 0.11}
{'loss': 3.5912, 'grad_norm': 0.6039846835046484, 'learning_rate': 9.767633379743462e-05, 'epoch': 0.11}
{'loss': 3.437, 'grad_norm': 0.6650259050627245, 'learning_rate': 9.767485262315964e-05, 'epoch': 0.11}
{'loss': 3.5312, 'grad_norm': 0.7671555758965845, 'learning_rate': 9.767337144888467e-05, 'epoch': 0.11}
{'loss': 3.7127, 'grad_norm': 0.6188930665866446, 'learning_rate': 9.767189027460971e-05, 'epoch': 0.11}
{'loss': 3.4757, 'grad_norm': 0.6926540799724852, 'learning_rate': 9.767040910033475e-05, 'epoch': 0.11}
{'loss': 3.6787, 'grad_norm': 0.6576316627555048, 'learning_rate': 9.766892792605979e-05, 'epoch': 0.11}
{'loss': 3.6083, 'grad_norm': 0.5959562155042868, 'learning_rate': 9.766744675178482e-05, 'epoch': 0.11}
{'loss': 3.553, 'grad_norm': 0.614664570097823, 'learning_rate': 9.766596557750986e-05, 'epoch': 0.11}
{'loss': 3.6268, 'grad_norm': 0.6036724839322138, 'learning_rate': 9.76644844032349e-05, 'epoch': 0.11}
{'loss': 3.56, 'grad_norm': 0.6241715185106852, 'learning_rate': 9.766300322895992e-05, 'epoch': 0.11}
{'loss': 3.4292, 'grad_norm': 0.5797573032618929, 'learning_rate': 9.766152205468496e-05, 'epoch': 0.11}
{'loss': 3.667, 'grad_norm': 0.6264785887613041, 'learning_rate': 9.766004088041e-05, 'epoch': 0.11}
[2024-04-12 08:36:20,995] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step12900 is about to be saved!
[2024-04-12 08:36:21,000] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-12900/global_step12900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 08:36:21,000] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12900/global_step12900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 08:36:21,006] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12900/global_step12900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 08:36:21,006] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-12900/global_step12900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 08:36:24,003] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-12900/global_step12900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 08:36:24,004] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-12900/global_step12900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 08:36:24,004] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12900 is ready now!
{'loss': 3.6383, 'grad_norm': 0.6183416206902925, 'learning_rate': 9.765855970613503e-05, 'epoch': 0.11}
{'loss': 3.4366, 'grad_norm': 0.6088839887539713, 'learning_rate': 9.765707853186006e-05, 'epoch': 0.11}
{'loss': 3.7537, 'grad_norm': 0.604992545165291, 'learning_rate': 9.76555973575851e-05, 'epoch': 0.11}
{'loss': 3.6092, 'grad_norm': 0.6425358680385738, 'learning_rate': 9.765411618331013e-05, 'epoch': 0.11}
{'loss': 3.6255, 'grad_norm': 0.5735087948073151, 'learning_rate': 9.765263500903516e-05, 'epoch': 0.11}
{'loss': 3.4921, 'grad_norm': 0.5946187579987074, 'learning_rate': 9.76511538347602e-05, 'epoch': 0.11}
{'loss': 3.5515, 'grad_norm': 0.638637279568809, 'learning_rate': 9.764967266048524e-05, 'epoch': 0.11}
{'loss': 3.5465, 'grad_norm': 0.6040894582323256, 'learning_rate': 9.764819148621027e-05, 'epoch': 0.11}
{'loss': 3.3979, 'grad_norm': 0.5530424380682644, 'learning_rate': 9.764671031193531e-05, 'epoch': 0.11}
{'loss': 3.3959, 'grad_norm': 0.6373448333301317, 'learning_rate': 9.764522913766035e-05, 'epoch': 0.11}
{'loss': 3.6516, 'grad_norm': 0.6100812803146044, 'learning_rate': 9.764374796338537e-05, 'epoch': 0.11}
{'loss': 3.588, 'grad_norm': 0.6512405638561157, 'learning_rate': 9.764226678911041e-05, 'epoch': 0.11}
{'loss': 3.5951, 'grad_norm': 0.6604281311996445, 'learning_rate': 9.764078561483545e-05, 'epoch': 0.11}
{'loss': 3.5583, 'grad_norm': 0.5656503918640378, 'learning_rate': 9.763930444056048e-05, 'epoch': 0.11}
{'loss': 3.6475, 'grad_norm': 0.6124533030970611, 'learning_rate': 9.763782326628551e-05, 'epoch': 0.11}
{'loss': 3.3887, 'grad_norm': 0.5990048864621096, 'learning_rate': 9.763634209201055e-05, 'epoch': 0.11}
{'loss': 3.5704, 'grad_norm': 0.6088352017190818, 'learning_rate': 9.763486091773559e-05, 'epoch': 0.11}
{'loss': 3.5879, 'grad_norm': 0.5887910291056798, 'learning_rate': 9.763337974346061e-05, 'epoch': 0.11}
{'loss': 3.6378, 'grad_norm': 0.5907057329697538, 'learning_rate': 9.763189856918565e-05, 'epoch': 0.11}
{'loss': 3.4464, 'grad_norm': 0.6050259385239711, 'learning_rate': 9.763041739491069e-05, 'epoch': 0.11}
[2024-04-12 08:41:00,643] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step13000 is about to be saved!
[2024-04-12 08:41:00,648] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-13000/global_step13000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 08:41:00,648] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13000/global_step13000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 08:41:00,654] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13000/global_step13000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 08:41:00,654] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13000/global_step13000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 08:41:03,735] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13000/global_step13000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 08:41:03,735] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-13000/global_step13000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 08:41:03,735] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
{'loss': 3.6353, 'grad_norm': 0.6211887220950897, 'learning_rate': 9.762893622063572e-05, 'epoch': 0.11}
{'loss': 3.7376, 'grad_norm': 0.6266058357915798, 'learning_rate': 9.762745504636076e-05, 'epoch': 0.11}
{'loss': 3.5358, 'grad_norm': 0.6083220592505199, 'learning_rate': 9.76259738720858e-05, 'epoch': 0.11}
{'loss': 3.5351, 'grad_norm': 0.5898011894423469, 'learning_rate': 9.762449269781083e-05, 'epoch': 0.11}
{'loss': 3.577, 'grad_norm': 0.7495331997014988, 'learning_rate': 9.762301152353587e-05, 'epoch': 0.11}
{'loss': 3.3944, 'grad_norm': 0.6830812944952152, 'learning_rate': 9.76215303492609e-05, 'epoch': 0.11}
{'loss': 3.628, 'grad_norm': 0.6481393377492516, 'learning_rate': 9.762004917498593e-05, 'epoch': 0.11}
{'loss': 3.5973, 'grad_norm': 0.6697871761192874, 'learning_rate': 9.761856800071096e-05, 'epoch': 0.11}
{'loss': 3.5975, 'grad_norm': 0.6308836494343741, 'learning_rate': 9.7617086826436e-05, 'epoch': 0.11}
{'loss': 3.6317, 'grad_norm': 0.6264189292611204, 'learning_rate': 9.761560565216104e-05, 'epoch': 0.11}
{'loss': 3.6894, 'grad_norm': 0.6164186201245102, 'learning_rate': 9.761412447788607e-05, 'epoch': 0.11}
{'loss': 3.5419, 'grad_norm': 0.6038309570050691, 'learning_rate': 9.76126433036111e-05, 'epoch': 0.11}
{'loss': 3.485, 'grad_norm': 0.723395821552358, 'learning_rate': 9.761116212933614e-05, 'epoch': 0.11}
{'loss': 3.6609, 'grad_norm': 0.6697592158265449, 'learning_rate': 9.760968095506117e-05, 'epoch': 0.11}
{'loss': 3.5078, 'grad_norm': 0.7926852534547228, 'learning_rate': 9.760819978078621e-05, 'epoch': 0.11}
{'loss': 3.5939, 'grad_norm': 0.6355199232198907, 'learning_rate': 9.760671860651125e-05, 'epoch': 0.11}
{'loss': 3.4013, 'grad_norm': 0.5885821205695878, 'learning_rate': 9.760523743223629e-05, 'epoch': 0.11}
{'loss': 3.4602, 'grad_norm': 0.5857974959104421, 'learning_rate': 9.760375625796132e-05, 'epoch': 0.11}
{'loss': 3.5978, 'grad_norm': 0.6696857079105338, 'learning_rate': 9.760227508368636e-05, 'epoch': 0.11}
{'loss': 3.469, 'grad_norm': 0.6244111846700229, 'learning_rate': 9.760079390941138e-05, 'epoch': 0.11}
[2024-04-12 08:45:40,373] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step13100 is about to be saved!
[2024-04-12 08:45:40,380] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-13100/global_step13100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 08:45:40,380] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13100/global_step13100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 08:45:40,386] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13100/global_step13100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 08:45:40,387] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13100/global_step13100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 08:45:43,466] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13100/global_step13100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 08:45:43,466] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-13100/global_step13100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 08:45:43,467] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13100 is ready now!
{'loss': 3.5229, 'grad_norm': 0.6697035450113139, 'learning_rate': 9.759931273513641e-05, 'epoch': 0.11}
{'loss': 3.577, 'grad_norm': 0.5964250239649962, 'learning_rate': 9.759783156086145e-05, 'epoch': 0.11}
{'loss': 3.4784, 'grad_norm': 0.669892227417956, 'learning_rate': 9.759635038658649e-05, 'epoch': 0.11}
{'loss': 3.5769, 'grad_norm': 0.7016235654773261, 'learning_rate': 9.759486921231153e-05, 'epoch': 0.11}
{'loss': 3.5013, 'grad_norm': 0.6020588254352482, 'learning_rate': 9.759338803803656e-05, 'epoch': 0.11}
{'loss': 3.5561, 'grad_norm': 0.6080972257698837, 'learning_rate': 9.75919068637616e-05, 'epoch': 0.11}
{'loss': 3.6396, 'grad_norm': 0.5976753766245428, 'learning_rate': 9.759042568948664e-05, 'epoch': 0.12}
{'loss': 3.4957, 'grad_norm': 0.7255159853404, 'learning_rate': 9.758894451521166e-05, 'epoch': 0.12}
{'loss': 3.4762, 'grad_norm': 0.5738436706120338, 'learning_rate': 9.75874633409367e-05, 'epoch': 0.12}
{'loss': 3.5187, 'grad_norm': 0.625360009954666, 'learning_rate': 9.758598216666174e-05, 'epoch': 0.12}
{'loss': 3.492, 'grad_norm': 0.612035912584659, 'learning_rate': 9.758450099238677e-05, 'epoch': 0.12}
{'loss': 3.5461, 'grad_norm': 0.6157635864297377, 'learning_rate': 9.75830198181118e-05, 'epoch': 0.12}
{'loss': 3.5144, 'grad_norm': 0.5741381762262376, 'learning_rate': 9.758153864383684e-05, 'epoch': 0.12}
{'loss': 3.4847, 'grad_norm': 0.6319955114686913, 'learning_rate': 9.758005746956188e-05, 'epoch': 0.12}
{'loss': 3.7225, 'grad_norm': 0.6231736781998203, 'learning_rate': 9.75785762952869e-05, 'epoch': 0.12}
{'loss': 3.6315, 'grad_norm': 0.761584377445136, 'learning_rate': 9.757709512101194e-05, 'epoch': 0.12}
{'loss': 3.3629, 'grad_norm': 0.6851233868465696, 'learning_rate': 9.757561394673698e-05, 'epoch': 0.12}
{'loss': 3.5385, 'grad_norm': 0.630748193019953, 'learning_rate': 9.757413277246201e-05, 'epoch': 0.12}
{'loss': 3.474, 'grad_norm': 0.5890554318177296, 'learning_rate': 9.757265159818705e-05, 'epoch': 0.12}
{'loss': 3.5647, 'grad_norm': 0.6488914490986871, 'learning_rate': 9.757117042391209e-05, 'epoch': 0.12}
[2024-04-12 08:50:20,176] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step13200 is about to be saved!
[2024-04-12 08:50:20,182] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-13200/global_step13200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 08:50:20,182] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13200/global_step13200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 08:50:20,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13200/global_step13200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 08:50:20,188] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13200/global_step13200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 08:50:23,248] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13200/global_step13200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 08:50:23,248] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-13200/global_step13200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 08:50:23,248] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13200 is ready now!
{'loss': 3.7359, 'grad_norm': 0.6445678194022699, 'learning_rate': 9.756968924963712e-05, 'epoch': 0.12}
{'loss': 3.6237, 'grad_norm': 0.6759876590474154, 'learning_rate': 9.756820807536216e-05, 'epoch': 0.12}
{'loss': 3.6359, 'grad_norm': 0.6491516660049031, 'learning_rate': 9.75667269010872e-05, 'epoch': 0.12}
{'loss': 3.4794, 'grad_norm': 0.6645747802687691, 'learning_rate': 9.756524572681222e-05, 'epoch': 0.12}
{'loss': 3.5902, 'grad_norm': 0.6571593353064972, 'learning_rate': 9.756376455253725e-05, 'epoch': 0.12}
{'loss': 3.5795, 'grad_norm': 0.6263078968847003, 'learning_rate': 9.756228337826229e-05, 'epoch': 0.12}
{'loss': 3.5911, 'grad_norm': 0.5953829377802287, 'learning_rate': 9.756080220398733e-05, 'epoch': 0.12}
{'loss': 3.6644, 'grad_norm': 0.6290701964700117, 'learning_rate': 9.755932102971235e-05, 'epoch': 0.12}
{'loss': 3.544, 'grad_norm': 0.6326679941986093, 'learning_rate': 9.75578398554374e-05, 'epoch': 0.12}
{'loss': 3.6632, 'grad_norm': 0.6248631841539353, 'learning_rate': 9.755635868116243e-05, 'epoch': 0.12}
{'loss': 3.4909, 'grad_norm': 0.6659957004033813, 'learning_rate': 9.755487750688746e-05, 'epoch': 0.12}
{'loss': 3.621, 'grad_norm': 0.6483312726248056, 'learning_rate': 9.75533963326125e-05, 'epoch': 0.12}
{'loss': 3.2982, 'grad_norm': 0.6203801316559563, 'learning_rate': 9.755191515833754e-05, 'epoch': 0.12}
{'loss': 3.5377, 'grad_norm': 0.618024368717807, 'learning_rate': 9.755043398406257e-05, 'epoch': 0.12}
{'loss': 3.5528, 'grad_norm': 0.6219291987744543, 'learning_rate': 9.754895280978761e-05, 'epoch': 0.12}
{'loss': 3.5159, 'grad_norm': 0.647383124197738, 'learning_rate': 9.754747163551265e-05, 'epoch': 0.12}
{'loss': 3.5985, 'grad_norm': 0.6616895861931261, 'learning_rate': 9.754599046123767e-05, 'epoch': 0.12}
{'loss': 3.5022, 'grad_norm': 0.592613799985627, 'learning_rate': 9.75445092869627e-05, 'epoch': 0.12}
{'loss': 3.6961, 'grad_norm': 0.5755135141514311, 'learning_rate': 9.754302811268774e-05, 'epoch': 0.12}
{'loss': 3.7518, 'grad_norm': 0.5890794660862082, 'learning_rate': 9.754154693841278e-05, 'epoch': 0.12}
[2024-04-12 08:54:59,991] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step13300 is about to be saved!
[2024-04-12 08:54:59,996] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-13300/global_step13300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 08:54:59,996] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13300/global_step13300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 08:55:00,002] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13300/global_step13300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 08:55:00,002] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13300/global_step13300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 08:55:03,113] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13300/global_step13300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 08:55:03,113] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-13300/global_step13300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 08:55:03,114] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13300 is ready now!
{'loss': 3.6554, 'grad_norm': 0.6402841994158158, 'learning_rate': 9.75400657641378e-05, 'epoch': 0.12}
{'loss': 3.5979, 'grad_norm': 0.6260082078188884, 'learning_rate': 9.753858458986285e-05, 'epoch': 0.12}
{'loss': 3.394, 'grad_norm': 1.1802748817019009, 'learning_rate': 9.753710341558789e-05, 'epoch': 0.12}
{'loss': 3.5914, 'grad_norm': 0.6738907072982776, 'learning_rate': 9.753562224131291e-05, 'epoch': 0.12}
{'loss': 3.651, 'grad_norm': 0.5922454855173028, 'learning_rate': 9.753414106703795e-05, 'epoch': 0.12}
{'loss': 3.5816, 'grad_norm': 0.6028384292415483, 'learning_rate': 9.753265989276299e-05, 'epoch': 0.12}
{'loss': 3.5776, 'grad_norm': 0.5880873369254362, 'learning_rate': 9.753117871848802e-05, 'epoch': 0.12}
{'loss': 3.588, 'grad_norm': 0.6026201238815453, 'learning_rate': 9.752969754421306e-05, 'epoch': 0.12}
{'loss': 3.5983, 'grad_norm': 0.6486078577696561, 'learning_rate': 9.75282163699381e-05, 'epoch': 0.12}
{'loss': 3.5126, 'grad_norm': 0.6276699411933806, 'learning_rate': 9.752673519566313e-05, 'epoch': 0.12}
{'loss': 3.624, 'grad_norm': 0.6581685391750126, 'learning_rate': 9.752525402138815e-05, 'epoch': 0.12}
{'loss': 3.5539, 'grad_norm': 0.5991813556943557, 'learning_rate': 9.752377284711319e-05, 'epoch': 0.12}
{'loss': 3.6717, 'grad_norm': 0.6014161471768754, 'learning_rate': 9.752229167283823e-05, 'epoch': 0.12}
{'loss': 3.545, 'grad_norm': 0.6318156979582209, 'learning_rate': 9.752081049856326e-05, 'epoch': 0.12}
{'loss': 3.5001, 'grad_norm': 0.6146165042111703, 'learning_rate': 9.75193293242883e-05, 'epoch': 0.12}
{'loss': 3.5342, 'grad_norm': 0.6231468282088485, 'learning_rate': 9.751784815001334e-05, 'epoch': 0.12}
{'loss': 3.6067, 'grad_norm': 0.6029988227446973, 'learning_rate': 9.751636697573838e-05, 'epoch': 0.12}
{'loss': 3.5661, 'grad_norm': 0.6261169956988583, 'learning_rate': 9.75148858014634e-05, 'epoch': 0.12}
{'loss': 3.4486, 'grad_norm': 1.0549539440092148, 'learning_rate': 9.751340462718844e-05, 'epoch': 0.12}
{'loss': 3.5642, 'grad_norm': 0.7029145572862326, 'learning_rate': 9.751192345291348e-05, 'epoch': 0.12}
[2024-04-12 08:59:39,965] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step13400 is about to be saved!
[2024-04-12 08:59:39,971] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-13400/global_step13400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 08:59:39,971] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13400/global_step13400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 08:59:39,977] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13400/global_step13400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 08:59:39,977] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13400/global_step13400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 08:59:43,029] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13400/global_step13400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 08:59:43,029] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-13400/global_step13400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 08:59:43,030] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13400 is ready now!
{'loss': 3.5674, 'grad_norm': 0.6151703744528634, 'learning_rate': 9.751044227863851e-05, 'epoch': 0.12}
{'loss': 3.5235, 'grad_norm': 0.5964207448148673, 'learning_rate': 9.750896110436354e-05, 'epoch': 0.12}
{'loss': 3.4667, 'grad_norm': 0.597953227108991, 'learning_rate': 9.750747993008858e-05, 'epoch': 0.12}
{'loss': 3.5661, 'grad_norm': 0.6609178133470607, 'learning_rate': 9.750599875581362e-05, 'epoch': 0.12}
{'loss': 3.6606, 'grad_norm': 0.5905262261852273, 'learning_rate': 9.750451758153864e-05, 'epoch': 0.12}
{'loss': 3.6454, 'grad_norm': 0.599129061343389, 'learning_rate': 9.750303640726368e-05, 'epoch': 0.12}
{'loss': 3.552, 'grad_norm': 0.6228668032662315, 'learning_rate': 9.750155523298872e-05, 'epoch': 0.12}
{'loss': 3.2623, 'grad_norm': 0.7261537205620751, 'learning_rate': 9.750007405871375e-05, 'epoch': 0.12}
{'loss': 3.5767, 'grad_norm': 0.603502138615608, 'learning_rate': 9.749859288443879e-05, 'epoch': 0.12}
{'loss': 3.5505, 'grad_norm': 0.5863405613672732, 'learning_rate': 9.749711171016383e-05, 'epoch': 0.12}
{'loss': 3.4609, 'grad_norm': 0.6420804289318618, 'learning_rate': 9.749563053588886e-05, 'epoch': 0.12}
{'loss': 3.5926, 'grad_norm': 0.5848515667790581, 'learning_rate': 9.74941493616139e-05, 'epoch': 0.12}
{'loss': 3.5125, 'grad_norm': 0.5871126245594818, 'learning_rate': 9.749266818733894e-05, 'epoch': 0.12}
{'loss': 3.4639, 'grad_norm': 0.6012250359168746, 'learning_rate': 9.749118701306396e-05, 'epoch': 0.12}
{'loss': 3.599, 'grad_norm': 0.6972449423370157, 'learning_rate': 9.748970583878899e-05, 'epoch': 0.12}
{'loss': 3.5595, 'grad_norm': 0.6124317094659135, 'learning_rate': 9.748822466451403e-05, 'epoch': 0.12}
{'loss': 3.5286, 'grad_norm': 0.7259725820024692, 'learning_rate': 9.748674349023907e-05, 'epoch': 0.12}
{'loss': 3.5572, 'grad_norm': 0.6029796794700899, 'learning_rate': 9.74852623159641e-05, 'epoch': 0.12}
{'loss': 3.5466, 'grad_norm': 0.6498209981441686, 'learning_rate': 9.748378114168914e-05, 'epoch': 0.12}
{'loss': 3.5686, 'grad_norm': 0.6287843572631514, 'learning_rate': 9.748229996741418e-05, 'epoch': 0.12}
[2024-04-12 09:04:19,688] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step13500 is about to be saved!
[2024-04-12 09:04:19,694] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-13500/global_step13500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 09:04:19,694] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13500/global_step13500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 09:04:19,700] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13500/global_step13500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 09:04:19,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13500/global_step13500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 09:04:22,644] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13500/global_step13500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 09:04:22,644] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-13500/global_step13500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 09:04:22,644] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13500 is ready now!
{'loss': 3.4605, 'grad_norm': 0.6076690257731591, 'learning_rate': 9.74808187931392e-05, 'epoch': 0.12}
{'loss': 3.5927, 'grad_norm': 0.5850558398749481, 'learning_rate': 9.747933761886424e-05, 'epoch': 0.12}
{'loss': 3.7722, 'grad_norm': 0.581574527312358, 'learning_rate': 9.747785644458928e-05, 'epoch': 0.12}
{'loss': 3.4204, 'grad_norm': 0.7400354050526249, 'learning_rate': 9.747637527031431e-05, 'epoch': 0.12}
{'loss': 3.2555, 'grad_norm': 0.6317064032398175, 'learning_rate': 9.747489409603935e-05, 'epoch': 0.12}
{'loss': 3.4935, 'grad_norm': 0.5727835673753036, 'learning_rate': 9.747341292176439e-05, 'epoch': 0.12}
{'loss': 3.6145, 'grad_norm': 0.6262208839691495, 'learning_rate': 9.747193174748941e-05, 'epoch': 0.12}
{'loss': 3.6067, 'grad_norm': 0.6422607532172985, 'learning_rate': 9.747045057321444e-05, 'epoch': 0.12}
{'loss': 3.5732, 'grad_norm': 0.6624743849244725, 'learning_rate': 9.746896939893948e-05, 'epoch': 0.12}
{'loss': 3.493, 'grad_norm': 0.6267560433643076, 'learning_rate': 9.746748822466452e-05, 'epoch': 0.12}
{'loss': 3.4338, 'grad_norm': 0.580359087536935, 'learning_rate': 9.746600705038955e-05, 'epoch': 0.12}
{'loss': 3.5751, 'grad_norm': 0.6919706872111963, 'learning_rate': 9.746452587611459e-05, 'epoch': 0.12}
{'loss': 3.4422, 'grad_norm': 0.6335581325350065, 'learning_rate': 9.746304470183963e-05, 'epoch': 0.12}
{'loss': 3.6856, 'grad_norm': 0.6053113923826676, 'learning_rate': 9.746156352756465e-05, 'epoch': 0.12}
{'loss': 3.6285, 'grad_norm': 0.5781321396688318, 'learning_rate': 9.74600823532897e-05, 'epoch': 0.12}
{'loss': 3.4047, 'grad_norm': 0.6526420403141141, 'learning_rate': 9.745860117901473e-05, 'epoch': 0.12}
{'loss': 3.4063, 'grad_norm': 0.5793544407351645, 'learning_rate': 9.745712000473976e-05, 'epoch': 0.12}
{'loss': 3.3805, 'grad_norm': 0.6792599220753888, 'learning_rate': 9.74556388304648e-05, 'epoch': 0.12}
{'loss': 3.5026, 'grad_norm': 0.5901775008512216, 'learning_rate': 9.745415765618984e-05, 'epoch': 0.12}
{'loss': 3.6216, 'grad_norm': 0.6333010166051825, 'learning_rate': 9.745267648191487e-05, 'epoch': 0.12}
[2024-04-12 09:08:59,320] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step13600 is about to be saved!
[2024-04-12 09:08:59,326] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-13600/global_step13600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 09:08:59,326] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13600/global_step13600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 09:08:59,332] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13600/global_step13600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 09:08:59,333] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13600/global_step13600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 09:09:02,343] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13600/global_step13600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 09:09:02,343] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-13600/global_step13600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 09:09:02,344] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13600 is ready now!
{'loss': 3.593, 'grad_norm': 0.6016004633772849, 'learning_rate': 9.745119530763989e-05, 'epoch': 0.12}
{'loss': 3.4244, 'grad_norm': 0.6172577524256069, 'learning_rate': 9.744971413336493e-05, 'epoch': 0.12}
{'loss': 3.5579, 'grad_norm': 0.5861502049032247, 'learning_rate': 9.744823295908997e-05, 'epoch': 0.12}
{'loss': 3.4572, 'grad_norm': 0.6565933290366103, 'learning_rate': 9.7446751784815e-05, 'epoch': 0.12}
{'loss': 3.5794, 'grad_norm': 0.8187616272654302, 'learning_rate': 9.744527061054004e-05, 'epoch': 0.12}
{'loss': 3.4978, 'grad_norm': 0.5922483664918984, 'learning_rate': 9.744378943626508e-05, 'epoch': 0.12}
{'loss': 3.4067, 'grad_norm': 0.7493852730164753, 'learning_rate': 9.74423082619901e-05, 'epoch': 0.12}
{'loss': 3.5759, 'grad_norm': 0.6448852828050456, 'learning_rate': 9.744082708771515e-05, 'epoch': 0.12}
{'loss': 3.5703, 'grad_norm': 0.6336848102356161, 'learning_rate': 9.743934591344019e-05, 'epoch': 0.12}
{'loss': 3.4712, 'grad_norm': 0.6466192270169765, 'learning_rate': 9.743786473916523e-05, 'epoch': 0.12}
{'loss': 3.5154, 'grad_norm': 0.5897294860523408, 'learning_rate': 9.743638356489025e-05, 'epoch': 0.12}
{'loss': 3.4628, 'grad_norm': 0.6431979344481142, 'learning_rate': 9.743490239061528e-05, 'epoch': 0.12}
{'loss': 3.5805, 'grad_norm': 0.6427178993384651, 'learning_rate': 9.743342121634032e-05, 'epoch': 0.12}
{'loss': 3.4734, 'grad_norm': 0.55451660125729, 'learning_rate': 9.743194004206534e-05, 'epoch': 0.12}
{'loss': 3.5122, 'grad_norm': 0.58747149573797, 'learning_rate': 9.743045886779038e-05, 'epoch': 0.12}
{'loss': 3.4595, 'grad_norm': 0.6563457312017396, 'learning_rate': 9.742897769351542e-05, 'epoch': 0.12}
{'loss': 3.484, 'grad_norm': 0.6307265277940535, 'learning_rate': 9.742749651924046e-05, 'epoch': 0.12}
{'loss': 3.4544, 'grad_norm': 0.6426152914457942, 'learning_rate': 9.742601534496549e-05, 'epoch': 0.12}
{'loss': 3.5877, 'grad_norm': 0.6360090640837509, 'learning_rate': 9.742453417069053e-05, 'epoch': 0.12}
{'loss': 3.2377, 'grad_norm': 0.5899607287990419, 'learning_rate': 9.742305299641557e-05, 'epoch': 0.12}
[2024-04-12 09:13:39,006] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step13700 is about to be saved!
[2024-04-12 09:13:39,012] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-13700/global_step13700/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 09:13:39,012] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13700/global_step13700/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 09:13:39,018] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13700/global_step13700/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 09:13:39,019] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13700/global_step13700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 09:13:41,987] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13700/global_step13700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 09:13:41,987] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-13700/global_step13700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 09:13:41,988] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13700 is ready now!
{'loss': 3.2817, 'grad_norm': 0.7438422952856532, 'learning_rate': 9.74215718221406e-05, 'epoch': 0.12}
{'loss': 3.482, 'grad_norm': 0.6149542090688237, 'learning_rate': 9.742009064786564e-05, 'epoch': 0.12}
{'loss': 3.4942, 'grad_norm': 0.6241859050555509, 'learning_rate': 9.741860947359068e-05, 'epoch': 0.12}
{'loss': 3.5815, 'grad_norm': 0.6185226699733658, 'learning_rate': 9.74171282993157e-05, 'epoch': 0.12}
{'loss': 3.4604, 'grad_norm': 0.6394344704154222, 'learning_rate': 9.741564712504073e-05, 'epoch': 0.12}
{'loss': 3.3675, 'grad_norm': 0.5929051397144507, 'learning_rate': 9.741416595076577e-05, 'epoch': 0.12}
{'loss': 3.3902, 'grad_norm': 0.6302709099923338, 'learning_rate': 9.741268477649081e-05, 'epoch': 0.12}
{'loss': 3.6115, 'grad_norm': 0.6064333808172625, 'learning_rate': 9.741120360221584e-05, 'epoch': 0.12}
{'loss': 3.4025, 'grad_norm': 0.5563191675242911, 'learning_rate': 9.740972242794088e-05, 'epoch': 0.12}
{'loss': 3.4913, 'grad_norm': 0.5690455447048935, 'learning_rate': 9.740824125366592e-05, 'epoch': 0.12}
{'loss': 3.5038, 'grad_norm': 0.5880486197828714, 'learning_rate': 9.740676007939094e-05, 'epoch': 0.12}
{'loss': 3.3795, 'grad_norm': 0.6458862464568779, 'learning_rate': 9.740527890511598e-05, 'epoch': 0.12}
{'loss': 3.4491, 'grad_norm': 0.5838205067696288, 'learning_rate': 9.740379773084102e-05, 'epoch': 0.12}
{'loss': 3.541, 'grad_norm': 0.5549361005684078, 'learning_rate': 9.740231655656605e-05, 'epoch': 0.12}
{'loss': 3.3652, 'grad_norm': 0.61210185083441, 'learning_rate': 9.740083538229109e-05, 'epoch': 0.12}
{'loss': 3.4082, 'grad_norm': 0.6432903422540018, 'learning_rate': 9.739935420801613e-05, 'epoch': 0.12}
{'loss': 3.5508, 'grad_norm': 0.6125407917611967, 'learning_rate': 9.739787303374116e-05, 'epoch': 0.12}
{'loss': 3.5097, 'grad_norm': 0.6204568947482406, 'learning_rate': 9.739639185946618e-05, 'epoch': 0.12}
{'loss': 3.609, 'grad_norm': 0.6088845947915128, 'learning_rate': 9.739491068519122e-05, 'epoch': 0.12}
{'loss': 3.5775, 'grad_norm': 0.6080644900569411, 'learning_rate': 9.739342951091626e-05, 'epoch': 0.12}
[2024-04-12 09:18:18,742] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step13800 is about to be saved!
[2024-04-12 09:18:18,747] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-13800/global_step13800/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 09:18:18,747] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13800/global_step13800/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 09:18:18,753] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13800/global_step13800/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 09:18:18,753] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13800/global_step13800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 09:18:21,699] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13800/global_step13800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 09:18:21,699] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-13800/global_step13800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 09:18:21,699] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13800 is ready now!
{'loss': 3.6754, 'grad_norm': 0.6011006482061564, 'learning_rate': 9.739194833664129e-05, 'epoch': 0.12}
{'loss': 3.4867, 'grad_norm': 0.5882371453927839, 'learning_rate': 9.739046716236633e-05, 'epoch': 0.12}
{'loss': 3.4547, 'grad_norm': 0.5896975914343062, 'learning_rate': 9.738898598809137e-05, 'epoch': 0.12}
{'loss': 3.7153, 'grad_norm': 0.6090271672076724, 'learning_rate': 9.73875048138164e-05, 'epoch': 0.12}
{'loss': 3.5542, 'grad_norm': 0.6167846004935397, 'learning_rate': 9.738602363954143e-05, 'epoch': 0.12}
{'loss': 3.4838, 'grad_norm': 0.6358793975123873, 'learning_rate': 9.738454246526647e-05, 'epoch': 0.12}
{'loss': 3.5018, 'grad_norm': 0.5742575603008152, 'learning_rate': 9.73830612909915e-05, 'epoch': 0.12}
{'loss': 3.5199, 'grad_norm': 0.65039228618159, 'learning_rate': 9.738158011671654e-05, 'epoch': 0.12}
{'loss': 3.5931, 'grad_norm': 0.6051284093473376, 'learning_rate': 9.738009894244158e-05, 'epoch': 0.12}
{'loss': 3.5148, 'grad_norm': 0.5881374300020604, 'learning_rate': 9.737861776816661e-05, 'epoch': 0.12}
{'loss': 3.3561, 'grad_norm': 0.6353154627416233, 'learning_rate': 9.737713659389163e-05, 'epoch': 0.12}
{'loss': 3.489, 'grad_norm': 0.6263880705876814, 'learning_rate': 9.737565541961667e-05, 'epoch': 0.12}
{'loss': 3.556, 'grad_norm': 0.6539842856008287, 'learning_rate': 9.737417424534171e-05, 'epoch': 0.12}
{'loss': 3.537, 'grad_norm': 0.623043401017073, 'learning_rate': 9.737269307106674e-05, 'epoch': 0.12}
{'loss': 3.5832, 'grad_norm': 0.8955107710986073, 'learning_rate': 9.737121189679178e-05, 'epoch': 0.12}
{'loss': 3.4884, 'grad_norm': 0.6277724839065387, 'learning_rate': 9.736973072251682e-05, 'epoch': 0.12}
{'loss': 3.4306, 'grad_norm': 0.6349161507168165, 'learning_rate': 9.736824954824185e-05, 'epoch': 0.12}
{'loss': 3.5507, 'grad_norm': 0.597242774966959, 'learning_rate': 9.736676837396689e-05, 'epoch': 0.12}
{'loss': 3.5978, 'grad_norm': 0.6305492365640304, 'learning_rate': 9.736528719969193e-05, 'epoch': 0.12}
{'loss': 3.4174, 'grad_norm': 0.5737235144627909, 'learning_rate': 9.736380602541697e-05, 'epoch': 0.12}
[2024-04-12 09:23:01,581] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step13900 is about to be saved!
[2024-04-12 09:23:01,588] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-13900/global_step13900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 09:23:01,588] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13900/global_step13900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 09:23:01,597] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13900/global_step13900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 09:23:01,597] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-13900/global_step13900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 09:23:04,424] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-13900/global_step13900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 09:23:04,424] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-13900/global_step13900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 09:23:04,424] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13900 is ready now!
{'loss': 3.43, 'grad_norm': 0.6166115846621777, 'learning_rate': 9.736232485114199e-05, 'epoch': 0.12}
{'loss': 3.4218, 'grad_norm': 0.6337253831456875, 'learning_rate': 9.736084367686702e-05, 'epoch': 0.12}
{'loss': 3.3617, 'grad_norm': 0.6696730445292005, 'learning_rate': 9.735936250259206e-05, 'epoch': 0.12}
{'loss': 3.5709, 'grad_norm': 0.5981511708802606, 'learning_rate': 9.735788132831709e-05, 'epoch': 0.12}
{'loss': 3.7318, 'grad_norm': 0.6494680096975023, 'learning_rate': 9.735640015404213e-05, 'epoch': 0.12}
{'loss': 3.4273, 'grad_norm': 0.7420434387842245, 'learning_rate': 9.735491897976717e-05, 'epoch': 0.12}
{'loss': 3.4436, 'grad_norm': 0.6574042427560395, 'learning_rate': 9.735343780549219e-05, 'epoch': 0.12}
{'loss': 3.6647, 'grad_norm': 0.5683089590962203, 'learning_rate': 9.735195663121723e-05, 'epoch': 0.12}
{'loss': 3.5337, 'grad_norm': 0.5625889369801123, 'learning_rate': 9.735047545694227e-05, 'epoch': 0.12}
{'loss': 3.5416, 'grad_norm': 0.8275946784696359, 'learning_rate': 9.734899428266731e-05, 'epoch': 0.12}
{'loss': 3.4708, 'grad_norm': 0.6204320481481697, 'learning_rate': 9.734751310839234e-05, 'epoch': 0.12}
{'loss': 3.4815, 'grad_norm': 0.5738628157728377, 'learning_rate': 9.734603193411738e-05, 'epoch': 0.12}
{'loss': 3.5665, 'grad_norm': 0.5945509988768625, 'learning_rate': 9.734455075984242e-05, 'epoch': 0.12}
{'loss': 3.5069, 'grad_norm': 0.6524713221969652, 'learning_rate': 9.734306958556744e-05, 'epoch': 0.12}
{'loss': 3.4536, 'grad_norm': 0.6143563351041033, 'learning_rate': 9.734158841129247e-05, 'epoch': 0.12}
{'loss': 3.5474, 'grad_norm': 0.6188540073276259, 'learning_rate': 9.734010723701751e-05, 'epoch': 0.12}
{'loss': 3.4282, 'grad_norm': 0.958877613945582, 'learning_rate': 9.733862606274255e-05, 'epoch': 0.12}
{'loss': 3.5955, 'grad_norm': 0.6230313273645663, 'learning_rate': 9.733714488846758e-05, 'epoch': 0.12}
{'loss': 3.4822, 'grad_norm': 0.6222425496405264, 'learning_rate': 9.733566371419262e-05, 'epoch': 0.12}
{'loss': 3.4465, 'grad_norm': 0.6575370629619507, 'learning_rate': 9.733418253991766e-05, 'epoch': 0.12}
[2024-04-12 09:27:45,057] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step14000 is about to be saved!
[2024-04-12 09:27:45,062] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-14000/global_step14000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 09:27:45,063] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-14000/global_step14000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 09:27:45,069] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-14000/global_step14000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 09:27:45,069] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-14000/global_step14000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 09:27:47,988] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-14000/global_step14000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 09:27:47,988] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-14000/global_step14000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 09:27:47,989] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
{'loss': 3.3639, 'grad_norm': 0.6286353967507412, 'learning_rate': 9.733270136564268e-05, 'epoch': 0.12}
{'loss': 3.5508, 'grad_norm': 0.7433895629429751, 'learning_rate': 9.733122019136772e-05, 'epoch': 0.12}
{'loss': 3.6188, 'grad_norm': 0.5853072250212392, 'learning_rate': 9.732973901709276e-05, 'epoch': 0.12}
{'loss': 3.3258, 'grad_norm': 0.6243635309949508, 'learning_rate': 9.732825784281779e-05, 'epoch': 0.12}
{'loss': 3.573, 'grad_norm': 0.6092498769250269, 'learning_rate': 9.732677666854283e-05, 'epoch': 0.12}
{'loss': 3.6161, 'grad_norm': 0.6026569493581203, 'learning_rate': 9.732529549426787e-05, 'epoch': 0.12}
{'loss': 3.4595, 'grad_norm': 0.5730988883650342, 'learning_rate': 9.73238143199929e-05, 'epoch': 0.12}
{'loss': 3.4789, 'grad_norm': 0.6493951422622611, 'learning_rate': 9.732233314571792e-05, 'epoch': 0.12}
{'loss': 3.6039, 'grad_norm': 0.6997848897483371, 'learning_rate': 9.732085197144296e-05, 'epoch': 0.12}
{'loss': 3.6004, 'grad_norm': 0.642855206384793, 'learning_rate': 9.7319370797168e-05, 'epoch': 0.12}
{'loss': 3.4162, 'grad_norm': 0.5500336189524302, 'learning_rate': 9.731788962289303e-05, 'epoch': 0.12}
{'loss': 3.407, 'grad_norm': 0.6157859038806084, 'learning_rate': 9.731640844861807e-05, 'epoch': 0.12}
{'loss': 3.4479, 'grad_norm': 0.6558911594035367, 'learning_rate': 9.731492727434311e-05, 'epoch': 0.12}
{'loss': 3.4936, 'grad_norm': 0.5892237059427197, 'learning_rate': 9.731344610006814e-05, 'epoch': 0.12}
{'loss': 3.438, 'grad_norm': 0.6159755888965278, 'learning_rate': 9.731196492579318e-05, 'epoch': 0.12}
{'loss': 3.5149, 'grad_norm': 0.6175340529891088, 'learning_rate': 9.731048375151822e-05, 'epoch': 0.12}
{'loss': 3.4859, 'grad_norm': 0.5759115237510363, 'learning_rate': 9.730900257724324e-05, 'epoch': 0.12}
{'loss': 3.4616, 'grad_norm': 0.6219333788345315, 'learning_rate': 9.730752140296828e-05, 'epoch': 0.12}
{'loss': 3.4578, 'grad_norm': 0.6441853255801371, 'learning_rate': 9.730604022869332e-05, 'epoch': 0.12}
{'loss': 3.2844, 'grad_norm': 0.6002099962178865, 'learning_rate': 9.730455905441835e-05, 'epoch': 0.12}
[2024-04-12 09:32:27,574] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step14100 is about to be saved!
[2024-04-12 09:32:27,580] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-14100/global_step14100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 09:32:27,581] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-14100/global_step14100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 09:32:27,589] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-14100/global_step14100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 09:32:27,590] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-14100/global_step14100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 09:32:31,484] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-14100/global_step14100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 09:32:31,484] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-14100/global_step14100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 09:32:31,484] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14100 is ready now!
{'loss': 3.3725, 'grad_norm': 0.6055249246206291, 'learning_rate': 9.730307788014337e-05, 'epoch': 0.12}
{'loss': 3.4297, 'grad_norm': 0.5743974540749325, 'learning_rate': 9.730159670586841e-05, 'epoch': 0.12}
{'loss': 3.5784, 'grad_norm': 0.6844873098038349, 'learning_rate': 9.730011553159345e-05, 'epoch': 0.12}
{'loss': 3.6884, 'grad_norm': 0.6740659504826714, 'learning_rate': 9.729863435731848e-05, 'epoch': 0.12}
{'loss': 3.4559, 'grad_norm': 0.620652660569425, 'learning_rate': 9.729715318304352e-05, 'epoch': 0.12}
{'loss': 3.661, 'grad_norm': 0.6134103604809119, 'learning_rate': 9.729567200876856e-05, 'epoch': 0.12}
{'loss': 3.4167, 'grad_norm': 0.5999643860198782, 'learning_rate': 9.729419083449359e-05, 'epoch': 0.12}
{'loss': 3.433, 'grad_norm': 0.6259323427363126, 'learning_rate': 9.729270966021863e-05, 'epoch': 0.12}
{'loss': 3.3332, 'grad_norm': 0.6804129760893096, 'learning_rate': 9.729122848594367e-05, 'epoch': 0.12}
{'loss': 3.5157, 'grad_norm': 0.6233098344734147, 'learning_rate': 9.72897473116687e-05, 'epoch': 0.12}
{'loss': 3.5599, 'grad_norm': 0.6588499316202832, 'learning_rate': 9.728826613739373e-05, 'epoch': 0.12}
{'loss': 3.4888, 'grad_norm': 0.6187791887003821, 'learning_rate': 9.728678496311876e-05, 'epoch': 0.12}
{'loss': 3.546, 'grad_norm': 0.5946216399381067, 'learning_rate': 9.72853037888438e-05, 'epoch': 0.12}
{'loss': 3.3695, 'grad_norm': 0.6001796451926652, 'learning_rate': 9.728382261456883e-05, 'epoch': 0.12}
{'loss': 3.4216, 'grad_norm': 0.6122956224655147, 'learning_rate': 9.728234144029387e-05, 'epoch': 0.12}
{'loss': 3.5167, 'grad_norm': 0.5865519224910588, 'learning_rate': 9.72808602660189e-05, 'epoch': 0.12}
{'loss': 3.5623, 'grad_norm': 0.5913289923568443, 'learning_rate': 9.727937909174393e-05, 'epoch': 0.12}
{'loss': 3.4077, 'grad_norm': 0.6504211678434211, 'learning_rate': 9.727789791746897e-05, 'epoch': 0.12}
{'loss': 3.3365, 'grad_norm': 0.6809644357896939, 'learning_rate': 9.727641674319401e-05, 'epoch': 0.12}
{'loss': 3.6108, 'grad_norm': 0.5905230107736534, 'learning_rate': 9.727493556891904e-05, 'epoch': 0.12}
[2024-04-12 09:37:11,415] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step14200 is about to be saved!
[2024-04-12 09:37:11,420] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-14200/global_step14200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 09:37:11,420] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-14200/global_step14200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 09:37:11,426] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-14200/global_step14200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 09:37:11,427] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-14200/global_step14200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 09:37:14,543] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-14200/global_step14200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 09:37:14,544] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-14200/global_step14200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 09:37:14,544] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14200 is ready now!
{'loss': 3.6532, 'grad_norm': 0.6965703986052482, 'learning_rate': 9.727345439464408e-05, 'epoch': 0.12}
{'loss': 3.3701, 'grad_norm': 0.640702861931858, 'learning_rate': 9.727197322036912e-05, 'epoch': 0.12}
{'loss': 3.4704, 'grad_norm': 0.6231556627475302, 'learning_rate': 9.727049204609416e-05, 'epoch': 0.12}
{'loss': 3.6007, 'grad_norm': 0.5952233643407454, 'learning_rate': 9.726901087181919e-05, 'epoch': 0.12}
{'loss': 3.5916, 'grad_norm': 0.6266678599534039, 'learning_rate': 9.726752969754421e-05, 'epoch': 0.12}
{'loss': 3.5701, 'grad_norm': 0.6063342666477107, 'learning_rate': 9.726604852326925e-05, 'epoch': 0.12}
{'loss': 3.3137, 'grad_norm': 0.6187086322078006, 'learning_rate': 9.726456734899428e-05, 'epoch': 0.12}
{'loss': 3.5947, 'grad_norm': 0.56034871748476, 'learning_rate': 9.726308617471932e-05, 'epoch': 0.12}
{'loss': 3.5121, 'grad_norm': 0.6303280923340353, 'learning_rate': 9.726160500044436e-05, 'epoch': 0.12}
{'loss': 3.521, 'grad_norm': 0.6045990663626655, 'learning_rate': 9.72601238261694e-05, 'epoch': 0.12}
{'loss': 3.4937, 'grad_norm': 0.643397655913898, 'learning_rate': 9.725864265189442e-05, 'epoch': 0.12}
{'loss': 3.5286, 'grad_norm': 0.6268744633312354, 'learning_rate': 9.725716147761946e-05, 'epoch': 0.12}
{'loss': 3.557, 'grad_norm': 0.5754857266504768, 'learning_rate': 9.72556803033445e-05, 'epoch': 0.12}
{'loss': 3.4183, 'grad_norm': 0.5867709883902232, 'learning_rate': 9.725419912906953e-05, 'epoch': 0.12}
{'loss': 3.2932, 'grad_norm': 0.6147436549903952, 'learning_rate': 9.725271795479457e-05, 'epoch': 0.13}
{'loss': 3.5626, 'grad_norm': 0.6404100670622545, 'learning_rate': 9.725123678051961e-05, 'epoch': 0.13}
{'loss': 3.556, 'grad_norm': 0.5915814776925044, 'learning_rate': 9.724975560624464e-05, 'epoch': 0.13}
{'loss': 3.5891, 'grad_norm': 0.6279138890504837, 'learning_rate': 9.724827443196966e-05, 'epoch': 0.13}
{'loss': 3.4045, 'grad_norm': 0.5907549189480592, 'learning_rate': 9.72467932576947e-05, 'epoch': 0.13}
{'loss': 3.4883, 'grad_norm': 0.610678485427058, 'learning_rate': 9.724531208341974e-05, 'epoch': 0.13}
[2024-04-12 09:42:59,641] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step14300 is about to be saved!
[2024-04-12 09:42:59,648] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-14300/global_step14300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 09:42:59,648] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-14300/global_step14300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 09:42:59,654] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-14300/global_step14300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 09:42:59,654] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-14300/global_step14300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 09:43:02,939] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-14300/global_step14300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 09:43:02,939] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-14300/global_step14300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 09:43:02,940] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14300 is ready now!
{'loss': 3.5332, 'grad_norm': 0.6893973205492457, 'learning_rate': 9.724383090914477e-05, 'epoch': 0.13}
{'loss': 3.5848, 'grad_norm': 0.607696028646761, 'learning_rate': 9.724234973486981e-05, 'epoch': 0.13}
[2024-04-12 09:46:14,395] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 09:46:14,712] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-12 09:46:14,712] [INFO] [runner.py:568:main] cmd = /home/cc/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --model_name_or_path output_model/checkpoint-14200 --tokenizer_name YeungNLP/firefly-llama2-7b-base --train_files data/训练数据/train_all_data_random.csv data/训练数据/train_news_chunk15.csv data/训练数据/train_news_chunk4.csv data/训练数据/train_my_baike_qa.csv data/训练数据/train_news_chunk16.csv data/训练数据/train_news_chunk5.csv data/训练数据/train_my_belll_3M_cn.csv data/训练数据/train_news_chunk17.csv data/训练数据/train_news_chunk6.csv data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv data/训练数据/train_news_chunk18.csv data/训练数据/train_news_chunk7.csv data/训练数据/train_my_web_text_zh.csv data/训练数据/train_news_chunk19.csv data/训练数据/train_news_chunk8.csv data/训练数据/train_news_chunk1.csv data/训练数据/train_news_chunk2.csv data/训练数据/train_news_chunk9.csv data/训练数据/train_news_chunk10.csv data/训练数据/train_news_chunk20.csv data/训练数据/train_sft_train.csv data/训练数据/train_news_chunk11.csv data/训练数据/train_news_chunk21.csv data/训练数据/train_web_poetry.csv data/训练数据/train_news_chunk12.csv data/训练数据/train_news_chunk22.csv data/训练数据/train_zhihu_kol.csv data/训练数据/train_news_chunk13.csv data/训练数据/train_news_chunk23.csv data/训练数据/train_news_chunk14.csv data/训练数据/train_news_chunk3.csv --validation_files data/dev_sft.csv --per_device_train_batch_size 64 --per_device_eval_batch_size 64 --do_train --output_dir output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 2000 --eval_steps 5000000 --save_total_limit 5 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2024-04-12 09:46:15,999] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 09:46:16,214] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2024-04-12 09:46:16,214] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-04-12 09:46:16,214] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-12 09:46:16,215] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-12 09:46:16,215] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-12 09:46:16,215] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-04-12 09:46:16,215] [INFO] [launch.py:253:main] process 177133 spawned with command: ['/home/cc/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--model_name_or_path', 'output_model/checkpoint-14200', '--tokenizer_name', 'YeungNLP/firefly-llama2-7b-base', '--train_files', 'data/训练数据/train_all_data_random.csv', 'data/训练数据/train_news_chunk15.csv', 'data/训练数据/train_news_chunk4.csv', 'data/训练数据/train_my_baike_qa.csv', 'data/训练数据/train_news_chunk16.csv', 'data/训练数据/train_news_chunk5.csv', 'data/训练数据/train_my_belll_3M_cn.csv', 'data/训练数据/train_news_chunk17.csv', 'data/训练数据/train_news_chunk6.csv', 'data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv', 'data/训练数据/train_news_chunk18.csv', 'data/训练数据/train_news_chunk7.csv', 'data/训练数据/train_my_web_text_zh.csv', 'data/训练数据/train_news_chunk19.csv', 'data/训练数据/train_news_chunk8.csv', 'data/训练数据/train_news_chunk1.csv', 'data/训练数据/train_news_chunk2.csv', 'data/训练数据/train_news_chunk9.csv', 'data/训练数据/train_news_chunk10.csv', 'data/训练数据/train_news_chunk20.csv', 'data/训练数据/train_sft_train.csv', 'data/训练数据/train_news_chunk11.csv', 'data/训练数据/train_news_chunk21.csv', 'data/训练数据/train_web_poetry.csv', 'data/训练数据/train_news_chunk12.csv', 'data/训练数据/train_news_chunk22.csv', 'data/训练数据/train_zhihu_kol.csv', 'data/训练数据/train_news_chunk13.csv', 'data/训练数据/train_news_chunk23.csv', 'data/训练数据/train_news_chunk14.csv', 'data/训练数据/train_news_chunk3.csv', '--validation_files', 'data/dev_sft.csv', '--per_device_train_batch_size', '64', '--per_device_eval_batch_size', '64', '--do_train', '--output_dir', 'output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', 'output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '2000', '--eval_steps', '5000000', '--save_total_limit', '5', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', 'output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2024-04-12 09:46:17,908] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 09:46:18,636] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-12 09:46:18,636] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
04/12/2024 09:46:18 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
04/12/2024 09:46:18 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=18000000,
debug=[],
deepspeed=./ds_config_zero3.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=5000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=True,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_model/logs,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=output_model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=64,
per_device_train_batch_size=64,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=output_model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=2000,
save_strategy=steps,
save_total_limit=5,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=5000,
weight_decay=0.0,
)
['data/训练数据/train_all_data_random.csv', 'data/训练数据/train_news_chunk15.csv', 'data/训练数据/train_news_chunk4.csv', 'data/训练数据/train_my_baike_qa.csv', 'data/训练数据/train_news_chunk16.csv', 'data/训练数据/train_news_chunk5.csv', 'data/训练数据/train_my_belll_3M_cn.csv', 'data/训练数据/train_news_chunk17.csv', 'data/训练数据/train_news_chunk6.csv', 'data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv', 'data/训练数据/train_news_chunk18.csv', 'data/训练数据/train_news_chunk7.csv', 'data/训练数据/train_my_web_text_zh.csv', 'data/训练数据/train_news_chunk19.csv', 'data/训练数据/train_news_chunk8.csv', 'data/训练数据/train_news_chunk1.csv', 'data/训练数据/train_news_chunk2.csv', 'data/训练数据/train_news_chunk9.csv', 'data/训练数据/train_news_chunk10.csv', 'data/训练数据/train_news_chunk20.csv', 'data/训练数据/train_sft_train.csv', 'data/训练数据/train_news_chunk11.csv', 'data/训练数据/train_news_chunk21.csv', 'data/训练数据/train_web_poetry.csv', 'data/训练数据/train_news_chunk12.csv', 'data/训练数据/train_news_chunk22.csv', 'data/训练数据/train_zhihu_kol.csv', 'data/训练数据/train_news_chunk13.csv', 'data/训练数据/train_news_chunk23.csv', 'data/训练数据/train_news_chunk14.csv', 'data/训练数据/train_news_chunk3.csv']
训练文件总个数 31
04/12/2024 09:46:19 - INFO - datasets.builder - Using custom data configuration default-1187581384cc2f62
04/12/2024 09:46:19 - INFO - datasets.info - Loading Dataset Infos from /home/cc/anaconda3/envs/llama-chinese/lib/python3.10/site-packages/datasets/packaged_modules/csv
04/12/2024 09:46:19 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/12/2024 09:46:19 - INFO - datasets.info - Loading Dataset info from output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
04/12/2024 09:46:19 - INFO - datasets.builder - Found cached dataset csv (/home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6)
04/12/2024 09:46:19 - INFO - datasets.info - Loading Dataset info from /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
0 start load tokenizer
0 end load tokenizer
0 start load model
[2024-04-12 09:46:23,890] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 291, num_elems = 0.39B
0 end load model
['text']
04/12/2024 09:46:24 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00000_of_00010.arrow
04/12/2024 09:46:24 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00001_of_00010.arrow
04/12/2024 09:46:24 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00002_of_00010.arrow
04/12/2024 09:46:24 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00003_of_00010.arrow
04/12/2024 09:46:24 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00004_of_00010.arrow
04/12/2024 09:46:24 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00005_of_00010.arrow
04/12/2024 09:46:24 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00006_of_00010.arrow
04/12/2024 09:46:24 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00007_of_00010.arrow
04/12/2024 09:46:24 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00008_of_00010.arrow
04/12/2024 09:46:24 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00009_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_*_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00000_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00001_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00002_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00003_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00004_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00005_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00006_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00007_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00008_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00009_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_*_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00000_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00001_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00002_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00003_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00004_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00005_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00006_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00007_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00008_of_00010.arrow
04/12/2024 09:46:30 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00009_of_00010.arrow
04/12/2024 09:46:32 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_*_of_00010.arrow
04/12/2024 09:46:32 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/12/2024 09:46:32 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00000_of_00010.arrow
04/12/2024 09:46:32 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00001_of_00010.arrow
04/12/2024 09:46:32 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00002_of_00010.arrow
04/12/2024 09:46:32 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00003_of_00010.arrow
04/12/2024 09:46:32 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00004_of_00010.arrow
04/12/2024 09:46:32 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00005_of_00010.arrow
04/12/2024 09:46:32 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00006_of_00010.arrow
04/12/2024 09:46:32 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00007_of_00010.arrow
04/12/2024 09:46:32 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00008_of_00010.arrow
04/12/2024 09:46:32 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00009_of_00010.arrow
04/12/2024 09:46:32 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_*_of_00010.arrow
04/12/2024 09:46:32 - INFO - datasets.arrow_dataset - Concatenating 10 shards
0 start select train_dataset
0 end select train_dataset
0 start select eval_dataset
0 end select eval_dataset
0 start load metric
0 end load metric
0 Initialize our Trainer
0 start train
[2024-04-12 09:46:32,922] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-04-12 09:46:32,929] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
ninja: no work to do.
Time to load fused_adam op: 0.039614200592041016 seconds
[2024-04-12 09:46:33,219] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2024-04-12 09:46:33,220] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-04-12 09:46:33,228] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-04-12 09:46:33,228] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2024-04-12 09:46:33,228] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-04-12 09:46:33,228] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-04-12 09:46:33,294] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
[2024-04-12 09:46:33,295] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 1.07 GB         CA 0.92 GB         Max_CA 1 GB 
[2024-04-12 09:46:33,295] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.83 GB, percent = 20.6%
[2024-04-12 09:46:33,296] [INFO] [stage3.py:130:__init__] Reduce bucket size 589824
[2024-04-12 09:46:33,296] [INFO] [stage3.py:131:__init__] Prefetch bucket size 530841
[2024-04-12 09:46:33,363] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-04-12 09:46:33,364] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.74 GB         CA 0.92 GB         Max_CA 1 GB 
[2024-04-12 09:46:33,364] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.83 GB, percent = 20.6%
Parameter Offload: Total persistent parameters: 49920 in 65 params
[2024-04-12 09:46:33,445] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-04-12 09:46:33,445] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.82 GB         CA 0.92 GB         Max_CA 1 GB 
[2024-04-12 09:46:33,446] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.83 GB, percent = 20.6%
[2024-04-12 09:46:33,514] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
[2024-04-12 09:46:33,514] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.74 GB         CA 0.92 GB         Max_CA 1 GB 
[2024-04-12 09:46:33,514] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.83 GB, percent = 20.6%
[2024-04-12 09:46:34,205] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 1
[2024-04-12 09:46:34,206] [INFO] [utils.py:801:see_memory_usage] MA 0.72 GB         Max_MA 0.74 GB         CA 0.73 GB         Max_CA 1 GB 
[2024-04-12 09:46:34,206] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.84 GB, percent = 20.6%
[2024-04-12 09:46:34,271] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
[2024-04-12 09:46:34,272] [INFO] [utils.py:801:see_memory_usage] MA 0.72 GB         Max_MA 0.72 GB         CA 0.73 GB         Max_CA 1 GB 
[2024-04-12 09:46:34,272] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.84 GB, percent = 20.6%
[2024-04-12 09:46:34,349] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
[2024-04-12 09:46:34,349] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 2.89 GB         CA 2.9 GB         Max_CA 3 GB 
[2024-04-12 09:46:34,349] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.84 GB, percent = 20.6%
[2024-04-12 09:46:34,502] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-04-12 09:46:34,502] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 2.17 GB         CA 2.9 GB         Max_CA 3 GB 
[2024-04-12 09:46:34,502] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.84 GB, percent = 20.6%
[2024-04-12 09:46:34,580] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-04-12 09:46:34,580] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 3.61 GB         CA 4.34 GB         Max_CA 4 GB 
[2024-04-12 09:46:34,580] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.85 GB, percent = 20.7%
[2024-04-12 09:46:34,581] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-04-12 09:46:34,688] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-04-12 09:46:34,689] [INFO] [utils.py:801:see_memory_usage] MA 2.89 GB         Max_MA 3.05 GB         CA 4.34 GB         Max_CA 4 GB 
[2024-04-12 09:46:34,689] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.84 GB, percent = 20.6%
[2024-04-12 09:46:34,689] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2024-04-12 09:46:34,689] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-04-12 09:46:34,689] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f7080b73af0>
[2024-04-12 09:46:34,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2024-04-12 09:46:34,690] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-04-12 09:46:34,690] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-04-12 09:46:34,690] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-04-12 09:46:34,690] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-04-12 09:46:34,690] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-04-12 09:46:34,690] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-04-12 09:46:34,690] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-04-12 09:46:34,690] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-04-12 09:46:34,690] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-04-12 09:46:34,690] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-04-12 09:46:34,690] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-04-12 09:46:34,690] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f71ac677340>
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 2
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-04-12 09:46:34,691] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupDecayLR
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   scheduler_params ............. {'last_batch_iteration': -1, 'total_num_steps': 85644, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 5000}
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  64
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=589824 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=530841 param_persistence_threshold=7680 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-04-12 09:46:34,692] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-04-12 09:46:34,693] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-04-12 09:46:34,693] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-04-12 09:46:34,693] [INFO] [config.py:986:print_user_config]   json = {
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "fp16_opt_level": "O2"
    }, 
    "bf16": {
        "enabled": true
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0001, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0.0
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "last_batch_iteration": -1, 
            "total_num_steps": 8.564400e+04, 
            "warmup_min_lr": 0, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 5.000000e+03
        }
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 5.898240e+05, 
        "stage3_prefetch_bucket_size": 5.308416e+05, 
        "stage3_param_persistence_threshold": 7.680000e+03, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 2, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 64, 
    "wall_clock_breakdown": false
}
[2024-04-12 09:48:17,019] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-04-12 09:48:27,355] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 177133
[2024-04-12 09:48:27,355] [ERROR] [launch.py:322:sigkill_handler] ['/home/cc/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--model_name_or_path', 'output_model/checkpoint-14200', '--tokenizer_name', 'YeungNLP/firefly-llama2-7b-base', '--train_files', 'data/训练数据/train_all_data_random.csv', 'data/训练数据/train_news_chunk15.csv', 'data/训练数据/train_news_chunk4.csv', 'data/训练数据/train_my_baike_qa.csv', 'data/训练数据/train_news_chunk16.csv', 'data/训练数据/train_news_chunk5.csv', 'data/训练数据/train_my_belll_3M_cn.csv', 'data/训练数据/train_news_chunk17.csv', 'data/训练数据/train_news_chunk6.csv', 'data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv', 'data/训练数据/train_news_chunk18.csv', 'data/训练数据/train_news_chunk7.csv', 'data/训练数据/train_my_web_text_zh.csv', 'data/训练数据/train_news_chunk19.csv', 'data/训练数据/train_news_chunk8.csv', 'data/训练数据/train_news_chunk1.csv', 'data/训练数据/train_news_chunk2.csv', 'data/训练数据/train_news_chunk9.csv', 'data/训练数据/train_news_chunk10.csv', 'data/训练数据/train_news_chunk20.csv', 'data/训练数据/train_sft_train.csv', 'data/训练数据/train_news_chunk11.csv', 'data/训练数据/train_news_chunk21.csv', 'data/训练数据/train_web_poetry.csv', 'data/训练数据/train_news_chunk12.csv', 'data/训练数据/train_news_chunk22.csv', 'data/训练数据/train_zhihu_kol.csv', 'data/训练数据/train_news_chunk13.csv', 'data/训练数据/train_news_chunk23.csv', 'data/训练数据/train_news_chunk14.csv', 'data/训练数据/train_news_chunk3.csv', '--validation_files', 'data/dev_sft.csv', '--per_device_train_batch_size', '64', '--per_device_eval_batch_size', '64', '--do_train', '--output_dir', 'output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', 'output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '2000', '--eval_steps', '5000000', '--save_total_limit', '5', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', 'output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000'] exits with return code = 1
[2024-04-12 09:49:46,577] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 09:49:46,933] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-12 09:49:46,933] [INFO] [runner.py:568:main] cmd = /home/cc/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --model_name_or_path output_model/checkpoint-14200 --tokenizer_name YeungNLP/firefly-llama2-7b-base --train_files data/训练数据/train_all_data_random.csv data/训练数据/train_news_chunk15.csv data/训练数据/train_news_chunk4.csv data/训练数据/train_my_baike_qa.csv data/训练数据/train_news_chunk16.csv data/训练数据/train_news_chunk5.csv data/训练数据/train_my_belll_3M_cn.csv data/训练数据/train_news_chunk17.csv data/训练数据/train_news_chunk6.csv data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv data/训练数据/train_news_chunk18.csv data/训练数据/train_news_chunk7.csv data/训练数据/train_my_web_text_zh.csv data/训练数据/train_news_chunk19.csv data/训练数据/train_news_chunk8.csv data/训练数据/train_news_chunk1.csv data/训练数据/train_news_chunk2.csv data/训练数据/train_news_chunk9.csv data/训练数据/train_news_chunk10.csv data/训练数据/train_news_chunk20.csv data/训练数据/train_sft_train.csv data/训练数据/train_news_chunk11.csv data/训练数据/train_news_chunk21.csv data/训练数据/train_web_poetry.csv data/训练数据/train_news_chunk12.csv data/训练数据/train_news_chunk22.csv data/训练数据/train_zhihu_kol.csv data/训练数据/train_news_chunk13.csv data/训练数据/train_news_chunk23.csv data/训练数据/train_news_chunk14.csv data/训练数据/train_news_chunk3.csv --validation_files data/dev_sft.csv --per_device_train_batch_size 64 --per_device_eval_batch_size 64 --do_train --output_dir output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 2000 --eval_steps 5000000 --save_total_limit 5 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2024-04-12 09:49:48,215] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 09:49:48,428] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2024-04-12 09:49:48,428] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-04-12 09:49:48,428] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-12 09:49:48,428] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-12 09:49:48,428] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-12 09:49:48,428] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-04-12 09:49:48,428] [INFO] [launch.py:253:main] process 179362 spawned with command: ['/home/cc/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--model_name_or_path', 'output_model/checkpoint-14200', '--tokenizer_name', 'YeungNLP/firefly-llama2-7b-base', '--train_files', 'data/训练数据/train_all_data_random.csv', 'data/训练数据/train_news_chunk15.csv', 'data/训练数据/train_news_chunk4.csv', 'data/训练数据/train_my_baike_qa.csv', 'data/训练数据/train_news_chunk16.csv', 'data/训练数据/train_news_chunk5.csv', 'data/训练数据/train_my_belll_3M_cn.csv', 'data/训练数据/train_news_chunk17.csv', 'data/训练数据/train_news_chunk6.csv', 'data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv', 'data/训练数据/train_news_chunk18.csv', 'data/训练数据/train_news_chunk7.csv', 'data/训练数据/train_my_web_text_zh.csv', 'data/训练数据/train_news_chunk19.csv', 'data/训练数据/train_news_chunk8.csv', 'data/训练数据/train_news_chunk1.csv', 'data/训练数据/train_news_chunk2.csv', 'data/训练数据/train_news_chunk9.csv', 'data/训练数据/train_news_chunk10.csv', 'data/训练数据/train_news_chunk20.csv', 'data/训练数据/train_sft_train.csv', 'data/训练数据/train_news_chunk11.csv', 'data/训练数据/train_news_chunk21.csv', 'data/训练数据/train_web_poetry.csv', 'data/训练数据/train_news_chunk12.csv', 'data/训练数据/train_news_chunk22.csv', 'data/训练数据/train_zhihu_kol.csv', 'data/训练数据/train_news_chunk13.csv', 'data/训练数据/train_news_chunk23.csv', 'data/训练数据/train_news_chunk14.csv', 'data/训练数据/train_news_chunk3.csv', '--validation_files', 'data/dev_sft.csv', '--per_device_train_batch_size', '64', '--per_device_eval_batch_size', '64', '--do_train', '--output_dir', 'output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', 'output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '2000', '--eval_steps', '5000000', '--save_total_limit', '5', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', 'output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2024-04-12 09:49:50,060] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 09:49:50,759] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-12 09:49:50,759] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
04/12/2024 09:49:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
04/12/2024 09:49:50 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=18000000,
debug=[],
deepspeed=./ds_config_zero3.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=5000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=True,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_model/logs,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=output_model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=64,
per_device_train_batch_size=64,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=output_model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=2000,
save_strategy=steps,
save_total_limit=5,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=5000,
weight_decay=0.0,
)
['data/训练数据/train_all_data_random.csv', 'data/训练数据/train_news_chunk15.csv', 'data/训练数据/train_news_chunk4.csv', 'data/训练数据/train_my_baike_qa.csv', 'data/训练数据/train_news_chunk16.csv', 'data/训练数据/train_news_chunk5.csv', 'data/训练数据/train_my_belll_3M_cn.csv', 'data/训练数据/train_news_chunk17.csv', 'data/训练数据/train_news_chunk6.csv', 'data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv', 'data/训练数据/train_news_chunk18.csv', 'data/训练数据/train_news_chunk7.csv', 'data/训练数据/train_my_web_text_zh.csv', 'data/训练数据/train_news_chunk19.csv', 'data/训练数据/train_news_chunk8.csv', 'data/训练数据/train_news_chunk1.csv', 'data/训练数据/train_news_chunk2.csv', 'data/训练数据/train_news_chunk9.csv', 'data/训练数据/train_news_chunk10.csv', 'data/训练数据/train_news_chunk20.csv', 'data/训练数据/train_sft_train.csv', 'data/训练数据/train_news_chunk11.csv', 'data/训练数据/train_news_chunk21.csv', 'data/训练数据/train_web_poetry.csv', 'data/训练数据/train_news_chunk12.csv', 'data/训练数据/train_news_chunk22.csv', 'data/训练数据/train_zhihu_kol.csv', 'data/训练数据/train_news_chunk13.csv', 'data/训练数据/train_news_chunk23.csv', 'data/训练数据/train_news_chunk14.csv', 'data/训练数据/train_news_chunk3.csv']
训练文件总个数 31
04/12/2024 09:49:50 - INFO - datasets.builder - Using custom data configuration default-1187581384cc2f62
04/12/2024 09:49:50 - INFO - datasets.info - Loading Dataset Infos from /home/cc/anaconda3/envs/llama-chinese/lib/python3.10/site-packages/datasets/packaged_modules/csv
04/12/2024 09:49:50 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/12/2024 09:49:50 - INFO - datasets.info - Loading Dataset info from output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
04/12/2024 09:49:50 - INFO - datasets.builder - Found cached dataset csv (/home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6)
04/12/2024 09:49:50 - INFO - datasets.info - Loading Dataset info from /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
0 start load tokenizer
0 end load tokenizer
0 start load model
[2024-04-12 09:49:54,330] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 291, num_elems = 0.39B
0 end load model
['text']
04/12/2024 09:49:54 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00000_of_00010.arrow
04/12/2024 09:49:54 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00001_of_00010.arrow
04/12/2024 09:49:54 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00002_of_00010.arrow
04/12/2024 09:49:54 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00003_of_00010.arrow
04/12/2024 09:49:54 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00004_of_00010.arrow
04/12/2024 09:49:54 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00005_of_00010.arrow
04/12/2024 09:49:54 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00006_of_00010.arrow
04/12/2024 09:49:54 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00007_of_00010.arrow
04/12/2024 09:49:54 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00008_of_00010.arrow
04/12/2024 09:49:54 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00009_of_00010.arrow
04/12/2024 09:50:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_*_of_00010.arrow
04/12/2024 09:50:00 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/12/2024 09:50:00 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00000_of_00010.arrow
04/12/2024 09:50:00 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00001_of_00010.arrow
04/12/2024 09:50:00 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00002_of_00010.arrow
04/12/2024 09:50:00 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00003_of_00010.arrow
04/12/2024 09:50:00 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00004_of_00010.arrow
04/12/2024 09:50:00 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00005_of_00010.arrow
04/12/2024 09:50:00 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00006_of_00010.arrow
04/12/2024 09:50:00 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00007_of_00010.arrow
04/12/2024 09:50:00 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00008_of_00010.arrow
04/12/2024 09:50:00 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00009_of_00010.arrow
04/12/2024 09:50:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_*_of_00010.arrow
04/12/2024 09:50:00 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/12/2024 09:50:01 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00000_of_00010.arrow
04/12/2024 09:50:01 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00001_of_00010.arrow
04/12/2024 09:50:01 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00002_of_00010.arrow
04/12/2024 09:50:01 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00003_of_00010.arrow
04/12/2024 09:50:01 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00004_of_00010.arrow
04/12/2024 09:50:01 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00005_of_00010.arrow
04/12/2024 09:50:01 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00006_of_00010.arrow
04/12/2024 09:50:01 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00007_of_00010.arrow
04/12/2024 09:50:01 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00008_of_00010.arrow
04/12/2024 09:50:01 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00009_of_00010.arrow
04/12/2024 09:50:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_*_of_00010.arrow
04/12/2024 09:50:03 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/12/2024 09:50:03 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00000_of_00010.arrow
04/12/2024 09:50:03 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00001_of_00010.arrow
04/12/2024 09:50:03 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00002_of_00010.arrow
04/12/2024 09:50:03 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00003_of_00010.arrow
04/12/2024 09:50:03 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00004_of_00010.arrow
04/12/2024 09:50:03 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00005_of_00010.arrow
04/12/2024 09:50:03 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00006_of_00010.arrow
04/12/2024 09:50:03 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00007_of_00010.arrow
04/12/2024 09:50:03 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00008_of_00010.arrow
04/12/2024 09:50:03 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00009_of_00010.arrow
04/12/2024 09:50:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_*_of_00010.arrow
04/12/2024 09:50:03 - INFO - datasets.arrow_dataset - Concatenating 10 shards
0 start select train_dataset
0 end select train_dataset
0 start select eval_dataset
0 end select eval_dataset
0 start load metric
0 end load metric
0 Initialize our Trainer
0 start train
[2024-04-12 09:50:03,541] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-04-12 09:50:03,548] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
ninja: no work to do.
Time to load fused_adam op: 0.04706525802612305 seconds
[2024-04-12 09:50:03,879] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2024-04-12 09:50:03,879] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-04-12 09:50:03,887] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-04-12 09:50:03,887] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2024-04-12 09:50:03,887] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-04-12 09:50:03,887] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-04-12 09:50:03,957] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
[2024-04-12 09:50:03,957] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 1.07 GB         CA 0.92 GB         Max_CA 1 GB 
[2024-04-12 09:50:03,957] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.95 GB, percent = 21.1%
[2024-04-12 09:50:03,958] [INFO] [stage3.py:130:__init__] Reduce bucket size 589824
[2024-04-12 09:50:03,958] [INFO] [stage3.py:131:__init__] Prefetch bucket size 530841
[2024-04-12 09:50:04,022] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-04-12 09:50:04,023] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.74 GB         CA 0.92 GB         Max_CA 1 GB 
[2024-04-12 09:50:04,023] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.95 GB, percent = 21.1%
Parameter Offload: Total persistent parameters: 49920 in 65 params
[2024-04-12 09:50:04,102] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-04-12 09:50:04,102] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.82 GB         CA 0.92 GB         Max_CA 1 GB 
[2024-04-12 09:50:04,102] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.95 GB, percent = 21.1%
[2024-04-12 09:50:04,168] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
[2024-04-12 09:50:04,169] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.74 GB         CA 0.92 GB         Max_CA 1 GB 
[2024-04-12 09:50:04,169] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.96 GB, percent = 21.1%
[2024-04-12 09:50:04,926] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 1
[2024-04-12 09:50:04,927] [INFO] [utils.py:801:see_memory_usage] MA 0.72 GB         Max_MA 0.74 GB         CA 0.73 GB         Max_CA 1 GB 
[2024-04-12 09:50:04,927] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.96 GB, percent = 21.1%
[2024-04-12 09:50:04,992] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
[2024-04-12 09:50:04,993] [INFO] [utils.py:801:see_memory_usage] MA 0.72 GB         Max_MA 0.72 GB         CA 0.73 GB         Max_CA 1 GB 
[2024-04-12 09:50:04,993] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.96 GB, percent = 21.1%
[2024-04-12 09:50:05,068] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
[2024-04-12 09:50:05,069] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 2.89 GB         CA 2.9 GB         Max_CA 3 GB 
[2024-04-12 09:50:05,069] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.96 GB, percent = 21.1%
[2024-04-12 09:50:05,137] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-04-12 09:50:05,138] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 2.17 GB         CA 2.9 GB         Max_CA 3 GB 
[2024-04-12 09:50:05,138] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.96 GB, percent = 21.1%
[2024-04-12 09:50:05,210] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-04-12 09:50:05,211] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 3.61 GB         CA 4.34 GB         Max_CA 4 GB 
[2024-04-12 09:50:05,211] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.96 GB, percent = 21.1%
[2024-04-12 09:50:05,211] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-04-12 09:50:05,312] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-04-12 09:50:05,313] [INFO] [utils.py:801:see_memory_usage] MA 2.89 GB         Max_MA 3.05 GB         CA 4.34 GB         Max_CA 4 GB 
[2024-04-12 09:50:05,313] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.96 GB, percent = 21.1%
[2024-04-12 09:50:05,313] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2024-04-12 09:50:05,313] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-04-12 09:50:05,313] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f2aa8d97ac0>
[2024-04-12 09:50:05,313] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2024-04-12 09:50:05,314] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2bb0ab2590>
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-04-12 09:50:05,314] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 2
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupDecayLR
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   scheduler_params ............. {'last_batch_iteration': -1, 'total_num_steps': 85644, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 5000}
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  64
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=589824 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=530841 param_persistence_threshold=7680 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-04-12 09:50:05,315] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-04-12 09:50:05,315] [INFO] [config.py:986:print_user_config]   json = {
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "fp16_opt_level": "O2"
    }, 
    "bf16": {
        "enabled": true
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0001, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0.0
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "last_batch_iteration": -1, 
            "total_num_steps": 8.564400e+04, 
            "warmup_min_lr": 0, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 5.000000e+03
        }
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 5.898240e+05, 
        "stage3_prefetch_bucket_size": 5.308416e+05, 
        "stage3_param_persistence_threshold": 7.680000e+03, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 2, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 64, 
    "wall_clock_breakdown": false
}
[2024-04-12 09:55:57,462] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 09:55:58,104] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-12 09:55:58,104] [INFO] [runner.py:568:main] cmd = /home/cc/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --model_name_or_path output_model/checkpoint-14200 --tokenizer_name YeungNLP/firefly-llama2-7b-base --train_files data/训练数据/train_all_data_random.csv data/训练数据/train_news_chunk15.csv data/训练数据/train_news_chunk4.csv data/训练数据/train_my_baike_qa.csv data/训练数据/train_news_chunk16.csv data/训练数据/train_news_chunk5.csv data/训练数据/train_my_belll_3M_cn.csv data/训练数据/train_news_chunk17.csv data/训练数据/train_news_chunk6.csv data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv data/训练数据/train_news_chunk18.csv data/训练数据/train_news_chunk7.csv data/训练数据/train_my_web_text_zh.csv data/训练数据/train_news_chunk19.csv data/训练数据/train_news_chunk8.csv data/训练数据/train_news_chunk1.csv data/训练数据/train_news_chunk2.csv data/训练数据/train_news_chunk9.csv data/训练数据/train_news_chunk10.csv data/训练数据/train_news_chunk20.csv data/训练数据/train_sft_train.csv data/训练数据/train_news_chunk11.csv data/训练数据/train_news_chunk21.csv data/训练数据/train_web_poetry.csv data/训练数据/train_news_chunk12.csv data/训练数据/train_news_chunk22.csv data/训练数据/train_zhihu_kol.csv data/训练数据/train_news_chunk13.csv data/训练数据/train_news_chunk23.csv data/训练数据/train_news_chunk14.csv data/训练数据/train_news_chunk3.csv --validation_files data/dev_sft.csv --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --output_dir output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 2000 --eval_steps 5000000 --save_total_limit 5 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_d                                     [2024-04-12 10:07:20,455] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 10:07:21,047] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-12 10:07:21,047] [INFO] [runner.py:568:main] cmd = /home/cc/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --model_name_or_path output_model/checkpoint-14200 --tokenizer_name YeungNLP/firefly-llama2-7b-base --train_files data/训练数据/train_all_data_random.csv data/训练数据/train_news_chunk15.csv data/训练数据/train_news_chunk4.csv data/训练数据/train_my_baike_qa.csv data/训练数据/train_news_chunk16.csv data/训练数据/train_news_chunk5.csv data/训练数据/train_my_belll_3M_cn.csv data/训练数据/train_news_chunk17.csv data/训练数据/train_news_chunk6.csv data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv data/训练数据/train_news_chunk18.csv data/训练数据/train_news_chunk7.csv data/训练数据/train_my_web_text_zh.csv data/训练数据/train_news_chunk19.csv data/训练数据/train_news_chunk8.csv data/训练数据/train_news_chunk1.csv data/训练数据/train_news_chunk2.csv data/训练数据/train_news_chunk9.csv data/训练数据/train_news_chunk10.csv data/训练数据/train_news_chunk20.csv data/训练数据/train_sft_train.csv data/训练数据/train_news_chunk11.csv data/训练数据/train_news_chunk21.csv data/训练数据/train_web_poetry.csv data/训练数据/train_news_chunk12.csv data/训练数据/train_news_chunk22.csv data/训练数据/train_zhihu_kol.csv data/训练数据/train_news_chunk13.csv data/训练数据/train_news_chunk23.csv data/训练数据/train_news_chunk14.csv data/训练数据/train_news_chunk3.csv --validation_files data/dev_sft.csv --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --output_dir output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 2000 --eval_steps 5000000 --save_total_limit 5 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2024-04-12 10:07:22,360] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 10:07:22,576] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2024-04-12 10:07:22,576] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-04-12 10:07:22,576] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-12 10:07:22,576] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-12 10:07:22,576] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-12 10:07:22,576] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-04-12 10:07:22,576] [INFO] [launch.py:253:main] process 792 spawned with command: ['/home/cc/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--model_name_or_path', 'output_model/checkpoint-14200', '--tokenizer_name', 'YeungNLP/firefly-llama2-7b-base', '--train_files', 'data/训练数据/train_all_data_random.csv', 'data/训练数据/train_news_chunk15.csv', 'data/训练数据/train_news_chunk4.csv', 'data/训练数据/train_my_baike_qa.csv', 'data/训练数据/train_news_chunk16.csv', 'data/训练数据/train_news_chunk5.csv', 'data/训练数据/train_my_belll_3M_cn.csv', 'data/训练数据/train_news_chunk17.csv', 'data/训练数据/train_news_chunk6.csv', 'data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv', 'data/训练数据/train_news_chunk18.csv', 'data/训练数据/train_news_chunk7.csv', 'data/训练数据/train_my_web_text_zh.csv', 'data/训练数据/train_news_chunk19.csv', 'data/训练数据/train_news_chunk8.csv', 'data/训练数据/train_news_chunk1.csv', 'data/训练数据/train_news_chunk2.csv', 'data/训练数据/train_news_chunk9.csv', 'data/训练数据/train_news_chunk10.csv', 'data/训练数据/train_news_chunk20.csv', 'data/训练数据/train_sft_train.csv', 'data/训练数据/train_news_chunk11.csv', 'data/训练数据/train_news_chunk21.csv', 'data/训练数据/train_web_poetry.csv', 'data/训练数据/train_news_chunk12.csv', 'data/训练数据/train_news_chunk22.csv', 'data/训练数据/train_zhihu_kol.csv', 'data/训练数据/train_news_chunk13.csv', 'data/训练数据/train_news_chunk23.csv', 'data/训练数据/train_news_chunk14.csv', 'data/训练数据/train_news_chunk3.csv', '--validation_files', 'data/dev_sft.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', 'output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', 'output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '2000', '--eval_steps', '5000000', '--save_total_limit', '5', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', 'output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2024-04-12 10:07:24,322] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 10:07:25,114] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-12 10:07:25,114] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
04/12/2024 10:07:25 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
04/12/2024 10:07:25 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=18000000,
debug=[],
deepspeed=./ds_config_zero3.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=5000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=True,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_model/logs,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=output_model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=32,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=output_model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=2000,
save_strategy=steps,
save_total_limit=5,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=5000,
weight_decay=0.0,
)
['data/训练数据/train_all_data_random.csv', 'data/训练数据/train_news_chunk15.csv', 'data/训练数据/train_news_chunk4.csv', 'data/训练数据/train_my_baike_qa.csv', 'data/训练数据/train_news_chunk16.csv', 'data/训练数据/train_news_chunk5.csv', 'data/训练数据/train_my_belll_3M_cn.csv', 'data/训练数据/train_news_chunk17.csv', 'data/训练数据/train_news_chunk6.csv', 'data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv', 'data/训练数据/train_news_chunk18.csv', 'data/训练数据/train_news_chunk7.csv', 'data/训练数据/train_my_web_text_zh.csv', 'data/训练数据/train_news_chunk19.csv', 'data/训练数据/train_news_chunk8.csv', 'data/训练数据/train_news_chunk1.csv', 'data/训练数据/train_news_chunk2.csv', 'data/训练数据/train_news_chunk9.csv', 'data/训练数据/train_news_chunk10.csv', 'data/训练数据/train_news_chunk20.csv', 'data/训练数据/train_sft_train.csv', 'data/训练数据/train_news_chunk11.csv', 'data/训练数据/train_news_chunk21.csv', 'data/训练数据/train_web_poetry.csv', 'data/训练数据/train_news_chunk12.csv', 'data/训练数据/train_news_chunk22.csv', 'data/训练数据/train_zhihu_kol.csv', 'data/训练数据/train_news_chunk13.csv', 'data/训练数据/train_news_chunk23.csv', 'data/训练数据/train_news_chunk14.csv', 'data/训练数据/train_news_chunk3.csv']
训练文件总个数 31
04/12/2024 10:07:26 - INFO - datasets.builder - Using custom data configuration default-1187581384cc2f62
04/12/2024 10:07:26 - INFO - datasets.info - Loading Dataset Infos from /home/cc/anaconda3/envs/llama-chinese/lib/python3.10/site-packages/datasets/packaged_modules/csv
04/12/2024 10:07:26 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/12/2024 10:07:26 - INFO - datasets.info - Loading Dataset info from output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
04/12/2024 10:07:26 - INFO - datasets.builder - Found cached dataset csv (/home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6)
04/12/2024 10:07:26 - INFO - datasets.info - Loading Dataset info from /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
0 start load tokenizer
0 end load tokenizer
0 start load model
[2024-04-12 10:07:29,604] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 291, num_elems = 0.39B
0 end load model
['text']
04/12/2024 10:07:30 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00000_of_00010.arrow
04/12/2024 10:07:30 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00001_of_00010.arrow
04/12/2024 10:07:30 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00002_of_00010.arrow
04/12/2024 10:07:30 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00003_of_00010.arrow
04/12/2024 10:07:30 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00004_of_00010.arrow
04/12/2024 10:07:30 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00005_of_00010.arrow
04/12/2024 10:07:30 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00006_of_00010.arrow
04/12/2024 10:07:30 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00007_of_00010.arrow
04/12/2024 10:07:30 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00008_of_00010.arrow
04/12/2024 10:07:30 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00009_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_*_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00000_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00001_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00002_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00003_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00004_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00005_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00006_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00007_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00008_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00009_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_*_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00000_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00001_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00002_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00003_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00004_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00005_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00006_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00007_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00008_of_00010.arrow
04/12/2024 10:07:35 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00009_of_00010.arrow
04/12/2024 10:07:38 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_*_of_00010.arrow
04/12/2024 10:07:38 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/12/2024 10:07:38 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00000_of_00010.arrow
04/12/2024 10:07:38 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00001_of_00010.arrow
04/12/2024 10:07:38 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00002_of_00010.arrow
04/12/2024 10:07:38 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00003_of_00010.arrow
04/12/2024 10:07:38 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00004_of_00010.arrow
04/12/2024 10:07:38 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00005_of_00010.arrow
04/12/2024 10:07:38 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00006_of_00010.arrow
04/12/2024 10:07:38 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00007_of_00010.arrow
04/12/2024 10:07:38 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00008_of_00010.arrow
04/12/2024 10:07:38 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00009_of_00010.arrow
04/12/2024 10:07:38 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_*_of_00010.arrow
04/12/2024 10:07:38 - INFO - datasets.arrow_dataset - Concatenating 10 shards
0 start select train_dataset
0 end select train_dataset
0 start select eval_dataset
0 end select eval_dataset
0 start load metric
0 end load metric
0 Initialize our Trainer
0 start train
[2024-04-12 10:07:38,212] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-04-12 10:07:38,219] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
ninja: no work to do.
Time to load fused_adam op: 0.039960384368896484 seconds
[2024-04-12 10:07:38,511] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2024-04-12 10:07:38,511] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-04-12 10:07:38,520] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-04-12 10:07:38,520] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2024-04-12 10:07:38,520] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-04-12 10:07:38,520] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-04-12 10:07:38,587] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
[2024-04-12 10:07:38,588] [INFO] [utils.py:801:see_memory_usage] MA 0.75 GB         Max_MA 1.07 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-12 10:07:38,588] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.64 GB, percent = 11.3%
[2024-04-12 10:07:38,589] [INFO] [stage3.py:130:__init__] Reduce bucket size 589824
[2024-04-12 10:07:38,589] [INFO] [stage3.py:131:__init__] Prefetch bucket size 530841
[2024-04-12 10:07:38,655] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-04-12 10:07:38,655] [INFO] [utils.py:801:see_memory_usage] MA 0.75 GB         Max_MA 0.75 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-12 10:07:38,655] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.64 GB, percent = 11.3%
Parameter Offload: Total persistent parameters: 49920 in 65 params
[2024-04-12 10:07:38,738] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-04-12 10:07:38,738] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.82 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-12 10:07:38,739] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.65 GB, percent = 11.3%
[2024-04-12 10:07:38,807] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
[2024-04-12 10:07:38,807] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.74 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-12 10:07:38,807] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.65 GB, percent = 11.3%
[2024-04-12 10:07:39,457] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 1
[2024-04-12 10:07:39,458] [INFO] [utils.py:801:see_memory_usage] MA 0.72 GB         Max_MA 0.74 GB         CA 0.73 GB         Max_CA 1 GB 
[2024-04-12 10:07:39,458] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.65 GB, percent = 11.3%
[2024-04-12 10:07:39,528] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
[2024-04-12 10:07:39,528] [INFO] [utils.py:801:see_memory_usage] MA 0.72 GB         Max_MA 0.72 GB         CA 0.73 GB         Max_CA 1 GB 
[2024-04-12 10:07:39,529] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.65 GB, percent = 11.3%
[2024-04-12 10:07:39,610] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
[2024-04-12 10:07:39,611] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 2.89 GB         CA 2.9 GB         Max_CA 3 GB 
[2024-04-12 10:07:39,611] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.65 GB, percent = 11.3%
[2024-04-12 10:07:39,736] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-04-12 10:07:39,736] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 2.17 GB         CA 2.9 GB         Max_CA 3 GB 
[2024-04-12 10:07:39,736] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.65 GB, percent = 11.3%
[2024-04-12 10:07:39,806] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-04-12 10:07:39,806] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 3.61 GB         CA 4.34 GB         Max_CA 4 GB 
[2024-04-12 10:07:39,806] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.65 GB, percent = 11.3%
[2024-04-12 10:07:39,806] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-04-12 10:07:39,921] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-04-12 10:07:39,922] [INFO] [utils.py:801:see_memory_usage] MA 2.89 GB         Max_MA 3.05 GB         CA 4.34 GB         Max_CA 4 GB 
[2024-04-12 10:07:39,922] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 2.65 GB, percent = 11.3%
[2024-04-12 10:07:39,922] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2024-04-12 10:07:39,922] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-04-12 10:07:39,922] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f27f3177c70>
[2024-04-12 10:07:39,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2024-04-12 10:07:39,922] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f281f02e170>
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-04-12 10:07:39,923] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 2
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-04-12 10:07:39,924] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupDecayLR
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   scheduler_params ............. {'last_batch_iteration': -1, 'total_num_steps': 171285, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 5000}
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   train_batch_size ............. 64
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=589824 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=530841 param_persistence_threshold=7680 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-04-12 10:07:39,925] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-04-12 10:07:39,925] [INFO] [config.py:986:print_user_config]   json = {
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "fp16_opt_level": "O2"
    }, 
    "bf16": {
        "enabled": true
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0001, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0.0
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "last_batch_iteration": -1, 
            "total_num_steps": 1.712850e+05, 
            "warmup_min_lr": 0, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 5.000000e+03
        }
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 5.898240e+05, 
        "stage3_prefetch_bucket_size": 5.308416e+05, 
        "stage3_param_persistence_threshold": 7.680000e+03, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 2, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 32, 
    "wall_clock_breakdown": false
}
{'loss': 5.6703, 'grad_norm': 2.6998767490922813, 'learning_rate': 1.889634150903278e-05, 'epoch': 0.0}
{'loss': 4.8107, 'grad_norm': 1.1393375007837818, 'learning_rate': 2.7034552830322406e-05, 'epoch': 0.0}
{'loss': 4.4579, 'grad_norm': 0.8107371941175295, 'learning_rate': 3.1795101276221204e-05, 'epoch': 0.0}
[2024-04-12 10:12:55,090] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 10:12:55,405] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-12 10:12:55,405] [INFO] [runner.py:568:main] cmd = /home/cc/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --model_name_or_path output_model/checkpoint-14200 --tokenizer_name YeungNLP/firefly-llama2-7b-base --train_files data/训练数据/train_all_data_random.csv data/训练数据/train_news_chunk15.csv data/训练数据/train_news_chunk4.csv data/训练数据/train_my_baike_qa.csv data/训练数据/train_news_chunk16.csv data/训练数据/train_news_chunk5.csv data/训练数据/train_my_belll_3M_cn.csv data/训练数据/train_news_chunk17.csv data/训练数据/train_news_chunk6.csv data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv data/训练数据/train_news_chunk18.csv data/训练数据/train_news_chunk7.csv data/训练数据/train_my_web_text_zh.csv data/训练数据/train_news_chunk19.csv data/训练数据/train_news_chunk8.csv data/训练数据/train_news_chunk1.csv data/训练数据/train_news_chunk2.csv data/训练数据/train_news_chunk9.csv data/训练数据/train_news_chunk10.csv data/训练数据/train_news_chunk20.csv data/训练数据/train_sft_train.csv data/训练数据/train_news_chunk11.csv data/训练数据/train_news_chunk21.csv data/训练数据/train_web_poetry.csv data/训练数据/train_news_chunk12.csv data/训练数据/train_news_chunk22.csv data/训练数据/train_zhihu_kol.csv data/训练数据/train_news_chunk13.csv data/训练数据/train_news_chunk23.csv data/训练数据/train_news_chunk14.csv data/训练数据/train_news_chunk3.csv --validation_files data/dev_sft.csv --per_device_train_batch_size 24 --per_device_eval_batch_size 24 --do_train --output_dir output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 2000 --eval_steps 5000000 --save_total_limit 5 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2024-04-12 10:12:56,702] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 10:12:56,916] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2024-04-12 10:12:56,916] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-04-12 10:12:56,916] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-12 10:12:56,916] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-12 10:12:56,916] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-12 10:12:56,916] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-04-12 10:12:56,916] [INFO] [launch.py:253:main] process 3335 spawned with command: ['/home/cc/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--model_name_or_path', 'output_model/checkpoint-14200', '--tokenizer_name', 'YeungNLP/firefly-llama2-7b-base', '--train_files', 'data/训练数据/train_all_data_random.csv', 'data/训练数据/train_news_chunk15.csv', 'data/训练数据/train_news_chunk4.csv', 'data/训练数据/train_my_baike_qa.csv', 'data/训练数据/train_news_chunk16.csv', 'data/训练数据/train_news_chunk5.csv', 'data/训练数据/train_my_belll_3M_cn.csv', 'data/训练数据/train_news_chunk17.csv', 'data/训练数据/train_news_chunk6.csv', 'data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv', 'data/训练数据/train_news_chunk18.csv', 'data/训练数据/train_news_chunk7.csv', 'data/训练数据/train_my_web_text_zh.csv', 'data/训练数据/train_news_chunk19.csv', 'data/训练数据/train_news_chunk8.csv', 'data/训练数据/train_news_chunk1.csv', 'data/训练数据/train_news_chunk2.csv', 'data/训练数据/train_news_chunk9.csv', 'data/训练数据/train_news_chunk10.csv', 'data/训练数据/train_news_chunk20.csv', 'data/训练数据/train_sft_train.csv', 'data/训练数据/train_news_chunk11.csv', 'data/训练数据/train_news_chunk21.csv', 'data/训练数据/train_web_poetry.csv', 'data/训练数据/train_news_chunk12.csv', 'data/训练数据/train_news_chunk22.csv', 'data/训练数据/train_zhihu_kol.csv', 'data/训练数据/train_news_chunk13.csv', 'data/训练数据/train_news_chunk23.csv', 'data/训练数据/train_news_chunk14.csv', 'data/训练数据/train_news_chunk3.csv', '--validation_files', 'data/dev_sft.csv', '--per_device_train_batch_size', '24', '--per_device_eval_batch_size', '24', '--do_train', '--output_dir', 'output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', 'output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '2000', '--eval_steps', '5000000', '--save_total_limit', '5', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', 'output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2024-04-12 10:12:58,528] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 10:12:59,222] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-12 10:12:59,222] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
04/12/2024 10:12:59 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
04/12/2024 10:12:59 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=18000000,
debug=[],
deepspeed=./ds_config_zero3.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=5000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=True,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_model/logs,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=output_model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=24,
per_device_train_batch_size=24,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=output_model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=2000,
save_strategy=steps,
save_total_limit=5,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=5000,
weight_decay=0.0,
)
['data/训练数据/train_all_data_random.csv', 'data/训练数据/train_news_chunk15.csv', 'data/训练数据/train_news_chunk4.csv', 'data/训练数据/train_my_baike_qa.csv', 'data/训练数据/train_news_chunk16.csv', 'data/训练数据/train_news_chunk5.csv', 'data/训练数据/train_my_belll_3M_cn.csv', 'data/训练数据/train_news_chunk17.csv', 'data/训练数据/train_news_chunk6.csv', 'data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv', 'data/训练数据/train_news_chunk18.csv', 'data/训练数据/train_news_chunk7.csv', 'data/训练数据/train_my_web_text_zh.csv', 'data/训练数据/train_news_chunk19.csv', 'data/训练数据/train_news_chunk8.csv', 'data/训练数据/train_news_chunk1.csv', 'data/训练数据/train_news_chunk2.csv', 'data/训练数据/train_news_chunk9.csv', 'data/训练数据/train_news_chunk10.csv', 'data/训练数据/train_news_chunk20.csv', 'data/训练数据/train_sft_train.csv', 'data/训练数据/train_news_chunk11.csv', 'data/训练数据/train_news_chunk21.csv', 'data/训练数据/train_web_poetry.csv', 'data/训练数据/train_news_chunk12.csv', 'data/训练数据/train_news_chunk22.csv', 'data/训练数据/train_zhihu_kol.csv', 'data/训练数据/train_news_chunk13.csv', 'data/训练数据/train_news_chunk23.csv', 'data/训练数据/train_news_chunk14.csv', 'data/训练数据/train_news_chunk3.csv']
训练文件总个数 31
04/12/2024 10:13:00 - INFO - datasets.builder - Using custom data configuration default-1187581384cc2f62
04/12/2024 10:13:00 - INFO - datasets.info - Loading Dataset Infos from /home/cc/anaconda3/envs/llama-chinese/lib/python3.10/site-packages/datasets/packaged_modules/csv
04/12/2024 10:13:00 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/12/2024 10:13:00 - INFO - datasets.info - Loading Dataset info from output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
04/12/2024 10:13:00 - INFO - datasets.builder - Found cached dataset csv (/home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6)
04/12/2024 10:13:00 - INFO - datasets.info - Loading Dataset info from /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
0 start load tokenizer
0 end load tokenizer
0 start load model
[2024-04-12 10:13:03,777] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 291, num_elems = 0.39B
0 end load model
['text']
04/12/2024 10:13:04 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00000_of_00010.arrow
04/12/2024 10:13:04 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00001_of_00010.arrow
04/12/2024 10:13:04 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00002_of_00010.arrow
04/12/2024 10:13:04 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00003_of_00010.arrow
04/12/2024 10:13:04 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00004_of_00010.arrow
04/12/2024 10:13:04 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00005_of_00010.arrow
04/12/2024 10:13:04 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00006_of_00010.arrow
04/12/2024 10:13:04 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00007_of_00010.arrow
04/12/2024 10:13:04 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00008_of_00010.arrow
04/12/2024 10:13:04 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00009_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_*_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00000_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00001_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00002_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00003_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00004_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00005_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00006_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00007_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00008_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00009_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_*_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00000_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00001_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00002_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00003_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00004_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00005_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00006_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00007_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00008_of_00010.arrow
04/12/2024 10:13:09 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00009_of_00010.arrow
04/12/2024 10:13:12 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_*_of_00010.arrow
04/12/2024 10:13:12 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/12/2024 10:13:12 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00000_of_00010.arrow
04/12/2024 10:13:12 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00001_of_00010.arrow
04/12/2024 10:13:12 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00002_of_00010.arrow
04/12/2024 10:13:12 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00003_of_00010.arrow
04/12/2024 10:13:12 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00004_of_00010.arrow
04/12/2024 10:13:12 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00005_of_00010.arrow
04/12/2024 10:13:12 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00006_of_00010.arrow
04/12/2024 10:13:12 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00007_of_00010.arrow
04/12/2024 10:13:12 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00008_of_00010.arrow
04/12/2024 10:13:12 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00009_of_00010.arrow
04/12/2024 10:13:12 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_*_of_00010.arrow
04/12/2024 10:13:12 - INFO - datasets.arrow_dataset - Concatenating 10 shards
0 start select train_dataset
0 end select train_dataset
0 start select eval_dataset
0 end select eval_dataset
0 start load metric
0 end load metric
0 Initialize our Trainer
0 start train
[2024-04-12 10:13:12,169] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-04-12 10:13:12,176] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
ninja: no work to do.
Time to load fused_adam op: 0.04047870635986328 seconds
[2024-04-12 10:13:12,457] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2024-04-12 10:13:12,457] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-04-12 10:13:12,465] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-04-12 10:13:12,465] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2024-04-12 10:13:12,465] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-04-12 10:13:12,465] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-04-12 10:13:12,535] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
[2024-04-12 10:13:12,535] [INFO] [utils.py:801:see_memory_usage] MA 0.75 GB         Max_MA 1.07 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-12 10:13:12,536] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.9 GB, percent = 16.6%
[2024-04-12 10:13:12,536] [INFO] [stage3.py:130:__init__] Reduce bucket size 589824
[2024-04-12 10:13:12,536] [INFO] [stage3.py:131:__init__] Prefetch bucket size 530841
[2024-04-12 10:13:12,603] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-04-12 10:13:12,604] [INFO] [utils.py:801:see_memory_usage] MA 0.75 GB         Max_MA 0.75 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-12 10:13:12,604] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.9 GB, percent = 16.6%
Parameter Offload: Total persistent parameters: 49920 in 65 params
[2024-04-12 10:13:12,685] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-04-12 10:13:12,685] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.82 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-12 10:13:12,685] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.9 GB, percent = 16.6%
[2024-04-12 10:13:12,750] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
[2024-04-12 10:13:12,750] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.74 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-12 10:13:12,750] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.9 GB, percent = 16.6%
[2024-04-12 10:13:13,394] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 1
[2024-04-12 10:13:13,395] [INFO] [utils.py:801:see_memory_usage] MA 0.72 GB         Max_MA 0.74 GB         CA 0.73 GB         Max_CA 1 GB 
[2024-04-12 10:13:13,395] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.91 GB, percent = 16.7%
[2024-04-12 10:13:13,464] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
[2024-04-12 10:13:13,464] [INFO] [utils.py:801:see_memory_usage] MA 0.72 GB         Max_MA 0.72 GB         CA 0.73 GB         Max_CA 1 GB 
[2024-04-12 10:13:13,464] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.91 GB, percent = 16.7%
[2024-04-12 10:13:13,540] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
[2024-04-12 10:13:13,540] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 2.89 GB         CA 2.9 GB         Max_CA 3 GB 
[2024-04-12 10:13:13,540] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.91 GB, percent = 16.7%
[2024-04-12 10:13:13,609] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-04-12 10:13:13,609] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 2.17 GB         CA 2.9 GB         Max_CA 3 GB 
[2024-04-12 10:13:13,609] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.92 GB, percent = 16.7%
[2024-04-12 10:13:13,681] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-04-12 10:13:13,681] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 3.61 GB         CA 4.34 GB         Max_CA 4 GB 
[2024-04-12 10:13:13,681] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.91 GB, percent = 16.7%
[2024-04-12 10:13:13,682] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-04-12 10:13:13,782] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-04-12 10:13:13,782] [INFO] [utils.py:801:see_memory_usage] MA 2.89 GB         Max_MA 3.05 GB         CA 4.34 GB         Max_CA 4 GB 
[2024-04-12 10:13:13,782] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.91 GB, percent = 16.7%
[2024-04-12 10:13:13,782] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2024-04-12 10:13:13,782] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-04-12 10:13:13,782] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f42c919baf0>
[2024-04-12 10:13:13,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2024-04-12 10:13:13,783] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-04-12 10:13:13,783] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-04-12 10:13:13,783] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-04-12 10:13:13,783] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-04-12 10:13:13,784] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-04-12 10:13:13,784] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-04-12 10:13:13,784] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-04-12 10:13:13,784] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-04-12 10:13:13,784] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-04-12 10:13:13,784] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-04-12 10:13:13,784] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-04-12 10:13:13,784] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f43ce06c430>
[2024-04-12 10:13:13,784] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-04-12 10:13:13,784] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-04-12 10:13:13,784] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-04-12 10:13:13,784] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-04-12 10:13:13,784] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-04-12 10:13:13,784] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-04-12 10:13:13,784] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 2
[2024-04-12 10:13:13,785] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupDecayLR
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   scheduler_params ............. {'last_batch_iteration': -1, 'total_num_steps': 228381, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 5000}
[2024-04-12 10:13:13,786] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-04-12 10:13:13,787] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-04-12 10:13:13,787] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-04-12 10:13:13,787] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-04-12 10:13:13,787] [INFO] [config.py:1000:print]   train_batch_size ............. 48
[2024-04-12 10:13:13,787] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24
[2024-04-12 10:13:13,787] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-04-12 10:13:13,787] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-04-12 10:13:13,787] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-04-12 10:13:13,787] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-04-12 10:13:13,787] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-04-12 10:13:13,787] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-04-12 10:13:13,787] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=589824 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=530841 param_persistence_threshold=7680 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-04-12 10:13:13,787] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-04-12 10:13:13,787] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-04-12 10:13:13,787] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-04-12 10:13:13,787] [INFO] [config.py:986:print_user_config]   json = {
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "fp16_opt_level": "O2"
    }, 
    "bf16": {
        "enabled": true
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0001, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0.0
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "last_batch_iteration": -1, 
            "total_num_steps": 2.283810e+05, 
            "warmup_min_lr": 0, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 5.000000e+03
        }
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 5.898240e+05, 
        "stage3_prefetch_bucket_size": 5.308416e+05, 
        "stage3_param_persistence_threshold": 7.680000e+03, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 2, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 48, 
    "train_micro_batch_size_per_gpu": 24, 
    "wall_clock_breakdown": false
}
[2024-04-12 10:14:10,000] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 10:14:10,316] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-12 10:14:10,316] [INFO] [runner.py:568:main] cmd = /home/cc/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --model_name_or_path output_model/checkpoint-14200 --tokenizer_name YeungNLP/firefly-llama2-7b-base --train_files data/训练数据/train_all_data_random.csv data/训练数据/train_news_chunk15.csv data/训练数据/train_news_chunk4.csv data/训练数据/train_my_baike_qa.csv data/训练数据/train_news_chunk16.csv data/训练数据/train_news_chunk5.csv data/训练数据/train_my_belll_3M_cn.csv data/训练数据/train_news_chunk17.csv data/训练数据/train_news_chunk6.csv data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv data/训练数据/train_news_chunk18.csv data/训练数据/train_news_chunk7.csv data/训练数据/train_my_web_text_zh.csv data/训练数据/train_news_chunk19.csv data/训练数据/train_news_chunk8.csv data/训练数据/train_news_chunk1.csv data/训练数据/train_news_chunk2.csv data/训练数据/train_news_chunk9.csv data/训练数据/train_news_chunk10.csv data/训练数据/train_news_chunk20.csv data/训练数据/train_sft_train.csv data/训练数据/train_news_chunk11.csv data/训练数据/train_news_chunk21.csv data/训练数据/train_web_poetry.csv data/训练数据/train_news_chunk12.csv data/训练数据/train_news_chunk22.csv data/训练数据/train_zhihu_kol.csv data/训练数据/train_news_chunk13.csv data/训练数据/train_news_chunk23.csv data/训练数据/train_news_chunk14.csv data/训练数据/train_news_chunk3.csv --validation_files data/dev_sft.csv --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --do_train --output_dir output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 2000 --eval_steps 5000000 --save_total_limit 5 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2024-04-12 10:14:11,604] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 10:14:11,815] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2024-04-12 10:14:11,816] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-04-12 10:14:11,816] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-12 10:14:11,816] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-12 10:14:11,816] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-12 10:14:11,816] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-04-12 10:14:11,816] [INFO] [launch.py:253:main] process 4290 spawned with command: ['/home/cc/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--model_name_or_path', 'output_model/checkpoint-14200', '--tokenizer_name', 'YeungNLP/firefly-llama2-7b-base', '--train_files', 'data/训练数据/train_all_data_random.csv', 'data/训练数据/train_news_chunk15.csv', 'data/训练数据/train_news_chunk4.csv', 'data/训练数据/train_my_baike_qa.csv', 'data/训练数据/train_news_chunk16.csv', 'data/训练数据/train_news_chunk5.csv', 'data/训练数据/train_my_belll_3M_cn.csv', 'data/训练数据/train_news_chunk17.csv', 'data/训练数据/train_news_chunk6.csv', 'data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv', 'data/训练数据/train_news_chunk18.csv', 'data/训练数据/train_news_chunk7.csv', 'data/训练数据/train_my_web_text_zh.csv', 'data/训练数据/train_news_chunk19.csv', 'data/训练数据/train_news_chunk8.csv', 'data/训练数据/train_news_chunk1.csv', 'data/训练数据/train_news_chunk2.csv', 'data/训练数据/train_news_chunk9.csv', 'data/训练数据/train_news_chunk10.csv', 'data/训练数据/train_news_chunk20.csv', 'data/训练数据/train_sft_train.csv', 'data/训练数据/train_news_chunk11.csv', 'data/训练数据/train_news_chunk21.csv', 'data/训练数据/train_web_poetry.csv', 'data/训练数据/train_news_chunk12.csv', 'data/训练数据/train_news_chunk22.csv', 'data/训练数据/train_zhihu_kol.csv', 'data/训练数据/train_news_chunk13.csv', 'data/训练数据/train_news_chunk23.csv', 'data/训练数据/train_news_chunk14.csv', 'data/训练数据/train_news_chunk3.csv', '--validation_files', 'data/dev_sft.csv', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '16', '--do_train', '--output_dir', 'output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', 'output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '2000', '--eval_steps', '5000000', '--save_total_limit', '5', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', 'output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2024-04-12 10:14:13,413] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-12 10:14:14,092] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-12 10:14:14,092] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
04/12/2024 10:14:14 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
04/12/2024 10:14:14 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=18000000,
debug=[],
deepspeed=./ds_config_zero3.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=5000000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=True,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_model/logs,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=output_model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=16,
per_device_train_batch_size=16,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=output_model,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=2000,
save_strategy=steps,
save_total_limit=5,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=5000,
weight_decay=0.0,
)
['data/训练数据/train_all_data_random.csv', 'data/训练数据/train_news_chunk15.csv', 'data/训练数据/train_news_chunk4.csv', 'data/训练数据/train_my_baike_qa.csv', 'data/训练数据/train_news_chunk16.csv', 'data/训练数据/train_news_chunk5.csv', 'data/训练数据/train_my_belll_3M_cn.csv', 'data/训练数据/train_news_chunk17.csv', 'data/训练数据/train_news_chunk6.csv', 'data/训练数据/train_my_tran_poetry_zh_no_dulpticates.csv', 'data/训练数据/train_news_chunk18.csv', 'data/训练数据/train_news_chunk7.csv', 'data/训练数据/train_my_web_text_zh.csv', 'data/训练数据/train_news_chunk19.csv', 'data/训练数据/train_news_chunk8.csv', 'data/训练数据/train_news_chunk1.csv', 'data/训练数据/train_news_chunk2.csv', 'data/训练数据/train_news_chunk9.csv', 'data/训练数据/train_news_chunk10.csv', 'data/训练数据/train_news_chunk20.csv', 'data/训练数据/train_sft_train.csv', 'data/训练数据/train_news_chunk11.csv', 'data/训练数据/train_news_chunk21.csv', 'data/训练数据/train_web_poetry.csv', 'data/训练数据/train_news_chunk12.csv', 'data/训练数据/train_news_chunk22.csv', 'data/训练数据/train_zhihu_kol.csv', 'data/训练数据/train_news_chunk13.csv', 'data/训练数据/train_news_chunk23.csv', 'data/训练数据/train_news_chunk14.csv', 'data/训练数据/train_news_chunk3.csv']
训练文件总个数 31
04/12/2024 10:14:15 - INFO - datasets.builder - Using custom data configuration default-1187581384cc2f62
04/12/2024 10:14:15 - INFO - datasets.info - Loading Dataset Infos from /home/cc/anaconda3/envs/llama-chinese/lib/python3.10/site-packages/datasets/packaged_modules/csv
04/12/2024 10:14:15 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/12/2024 10:14:15 - INFO - datasets.info - Loading Dataset info from output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
04/12/2024 10:14:15 - INFO - datasets.builder - Found cached dataset csv (/home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6)
04/12/2024 10:14:15 - INFO - datasets.info - Loading Dataset info from /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
0 start load tokenizer
0 end load tokenizer
0 start load model
[2024-04-12 10:14:18,319] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 291, num_elems = 0.39B
0 end load model
['text']
04/12/2024 10:14:19 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00000_of_00010.arrow
04/12/2024 10:14:19 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00001_of_00010.arrow
04/12/2024 10:14:19 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00002_of_00010.arrow
04/12/2024 10:14:19 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00003_of_00010.arrow
04/12/2024 10:14:19 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00004_of_00010.arrow
04/12/2024 10:14:19 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00005_of_00010.arrow
04/12/2024 10:14:19 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00006_of_00010.arrow
04/12/2024 10:14:19 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00007_of_00010.arrow
04/12/2024 10:14:19 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00008_of_00010.arrow
04/12/2024 10:14:19 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_00009_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-eb405a96b5bd48ce_*_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00000_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00001_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00002_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00003_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00004_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00005_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00006_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00007_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00008_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_00009_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-aeccf69fe8e76f09_*_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00000_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00001_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00002_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00003_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00004_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00005_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00006_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00007_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00008_of_00010.arrow
04/12/2024 10:14:24 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_00009_of_00010.arrow
04/12/2024 10:14:26 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-3238503d50495828_*_of_00010.arrow
04/12/2024 10:14:26 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/12/2024 10:14:26 - INFO - datasets.arrow_dataset - Process #0 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00000_of_00010.arrow
04/12/2024 10:14:26 - INFO - datasets.arrow_dataset - Process #1 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00001_of_00010.arrow
04/12/2024 10:14:26 - INFO - datasets.arrow_dataset - Process #2 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00002_of_00010.arrow
04/12/2024 10:14:26 - INFO - datasets.arrow_dataset - Process #3 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00003_of_00010.arrow
04/12/2024 10:14:26 - INFO - datasets.arrow_dataset - Process #4 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00004_of_00010.arrow
04/12/2024 10:14:26 - INFO - datasets.arrow_dataset - Process #5 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00005_of_00010.arrow
04/12/2024 10:14:26 - INFO - datasets.arrow_dataset - Process #6 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00006_of_00010.arrow
04/12/2024 10:14:26 - INFO - datasets.arrow_dataset - Process #7 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00007_of_00010.arrow
04/12/2024 10:14:26 - INFO - datasets.arrow_dataset - Process #8 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00008_of_00010.arrow
04/12/2024 10:14:26 - INFO - datasets.arrow_dataset - Process #9 will write at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_00009_of_00010.arrow
04/12/2024 10:14:26 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/cc/Llama-Chinese/output_model/dataset_cache/csv/default-1187581384cc2f62/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-a3c4ad19ef0d5e23_*_of_00010.arrow
04/12/2024 10:14:26 - INFO - datasets.arrow_dataset - Concatenating 10 shards
0 start select train_dataset
0 end select train_dataset
0 start select eval_dataset
0 end select eval_dataset
0 start load metric
0 end load metric
0 Initialize our Trainer
0 start train
[2024-04-12 10:14:26,602] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-04-12 10:14:26,609] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
ninja: no work to do.
Time to load fused_adam op: 0.04090070724487305 seconds
[2024-04-12 10:14:26,947] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2024-04-12 10:14:26,947] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-04-12 10:14:26,955] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-04-12 10:14:26,955] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2024-04-12 10:14:26,955] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-04-12 10:14:26,955] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-04-12 10:14:27,021] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
[2024-04-12 10:14:27,021] [INFO] [utils.py:801:see_memory_usage] MA 0.75 GB         Max_MA 1.07 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-12 10:14:27,022] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.79 GB, percent = 16.1%
[2024-04-12 10:14:27,023] [INFO] [stage3.py:130:__init__] Reduce bucket size 589824
[2024-04-12 10:14:27,023] [INFO] [stage3.py:131:__init__] Prefetch bucket size 530841
[2024-04-12 10:14:27,087] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-04-12 10:14:27,087] [INFO] [utils.py:801:see_memory_usage] MA 0.75 GB         Max_MA 0.75 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-12 10:14:27,088] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.79 GB, percent = 16.1%
Parameter Offload: Total persistent parameters: 49920 in 65 params
[2024-04-12 10:14:27,170] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-04-12 10:14:27,170] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.82 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-12 10:14:27,170] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.79 GB, percent = 16.1%
[2024-04-12 10:14:27,238] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
[2024-04-12 10:14:27,239] [INFO] [utils.py:801:see_memory_usage] MA 0.74 GB         Max_MA 0.74 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-04-12 10:14:27,239] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.79 GB, percent = 16.1%
[2024-04-12 10:14:27,895] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 1
[2024-04-12 10:14:27,895] [INFO] [utils.py:801:see_memory_usage] MA 0.72 GB         Max_MA 0.74 GB         CA 0.73 GB         Max_CA 1 GB 
[2024-04-12 10:14:27,895] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.79 GB, percent = 16.1%
[2024-04-12 10:14:27,960] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
[2024-04-12 10:14:27,961] [INFO] [utils.py:801:see_memory_usage] MA 0.72 GB         Max_MA 0.72 GB         CA 0.73 GB         Max_CA 1 GB 
[2024-04-12 10:14:27,961] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.79 GB, percent = 16.1%
[2024-04-12 10:14:28,035] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
[2024-04-12 10:14:28,035] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 2.89 GB         CA 2.9 GB         Max_CA 3 GB 
[2024-04-12 10:14:28,035] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.79 GB, percent = 16.1%
[2024-04-12 10:14:28,178] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-04-12 10:14:28,178] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 2.17 GB         CA 2.9 GB         Max_CA 3 GB 
[2024-04-12 10:14:28,178] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.79 GB, percent = 16.2%
[2024-04-12 10:14:28,251] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-04-12 10:14:28,251] [INFO] [utils.py:801:see_memory_usage] MA 2.17 GB         Max_MA 3.61 GB         CA 4.34 GB         Max_CA 4 GB 
[2024-04-12 10:14:28,251] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.79 GB, percent = 16.1%
[2024-04-12 10:14:28,251] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-04-12 10:14:28,351] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-04-12 10:14:28,352] [INFO] [utils.py:801:see_memory_usage] MA 2.89 GB         Max_MA 3.05 GB         CA 4.34 GB         Max_CA 4 GB 
[2024-04-12 10:14:28,352] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.79 GB, percent = 16.2%
[2024-04-12 10:14:28,352] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2024-04-12 10:14:28,352] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-04-12 10:14:28,352] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f2530fa7af0>
[2024-04-12 10:14:28,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2024-04-12 10:14:28,353] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f265c16ab90>
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-04-12 10:14:28,353] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 2
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupDecayLR
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   scheduler_params ............. {'last_batch_iteration': -1, 'total_num_steps': 342570, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 5000}
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  16
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=589824 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=530841 param_persistence_threshold=7680 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-04-12 10:14:28,354] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-04-12 10:14:28,354] [INFO] [config.py:986:print_user_config]   json = {
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "fp16_opt_level": "O2"
    }, 
    "bf16": {
        "enabled": true
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0001, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0.0
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "last_batch_iteration": -1, 
            "total_num_steps": 3.425700e+05, 
            "warmup_min_lr": 0, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 5.000000e+03
        }
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 5.898240e+05, 
        "stage3_prefetch_bucket_size": 5.308416e+05, 
        "stage3_param_persistence_threshold": 7.680000e+03, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 2, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 16, 
    "wall_clock_breakdown": false
}
{'loss': 5.6529, 'grad_norm': 2.7164444117179487, 'learning_rate': 1.889634150903278e-05, 'epoch': 0.0}
{'loss': 4.891, 'grad_norm': 1.2159580335005395, 'learning_rate': 2.7034552830322406e-05, 'epoch': 0.0}
{'loss': 4.4722, 'grad_norm': 0.9521706777444732, 'learning_rate': 3.1795101276221204e-05, 'epoch': 0.0}
{'loss': 4.2274, 'grad_norm': 0.7324024874817809, 'learning_rate': 3.5172764151612024e-05, 'epoch': 0.0}
{'loss': 4.1049, 'grad_norm': 0.665724866721824, 'learning_rate': 3.779268301806556e-05, 'epoch': 0.0}
{'loss': 4.0128, 'grad_norm': 0.6166144841134552, 'learning_rate': 3.993331259751083e-05, 'epoch': 0.0}
{'loss': 3.9079, 'grad_norm': 0.5881028455298936, 'learning_rate': 4.1743189118600124e-05, 'epoch': 0.0}
{'loss': 3.8586, 'grad_norm': 0.562587261504549, 'learning_rate': 4.331097547290165e-05, 'epoch': 0.0}
{'loss': 3.8569, 'grad_norm': 0.5765101079819049, 'learning_rate': 4.4693861043409635e-05, 'epoch': 0.0}
{'loss': 3.8021, 'grad_norm': 0.5713310978377285, 'learning_rate': 4.5930894339355186e-05, 'epoch': 0.0}
{'loss': 3.7942, 'grad_norm': 0.5763215857900384, 'learning_rate': 4.7049927073054124e-05, 'epoch': 0.0}
{'loss': 3.7444, 'grad_norm': 0.5683143792760885, 'learning_rate': 4.8071523918800455e-05, 'epoch': 0.0}
{'loss': 3.7158, 'grad_norm': 0.5732791956940052, 'learning_rate': 4.9011301916958396e-05, 'epoch': 0.0}
{'loss': 3.647, 'grad_norm': 0.5772453500048004, 'learning_rate': 4.9881400439889756e-05, 'epoch': 0.0}
{'loss': 3.7296, 'grad_norm': 0.5533835617885251, 'learning_rate': 5.0691442785253985e-05, 'epoch': 0.0}
{'loss': 3.6395, 'grad_norm': 0.5819324627516627, 'learning_rate': 5.1449186794191275e-05, 'epoch': 0.0}
{'loss': 3.6476, 'grad_norm': 0.5668612790718202, 'learning_rate': 5.2160977879046945e-05, 'epoch': 0.0}
{'loss': 3.6703, 'grad_norm': 0.5568045646555163, 'learning_rate': 5.283207236469926e-05, 'epoch': 0.0}
{'loss': 3.5976, 'grad_norm': 0.575725906146223, 'learning_rate': 5.3466873290957045e-05, 'epoch': 0.0}
{'loss': 3.5449, 'grad_norm': 0.5493259454528839, 'learning_rate': 5.406910566064481e-05, 'epoch': 0.0}
{'loss': 3.5761, 'grad_norm': 0.5454728908074654, 'learning_rate': 5.4641948885788554e-05, 'epoch': 0.0}
{'loss': 3.5304, 'grad_norm': 0.5710890281748783, 'learning_rate': 5.518813839434375e-05, 'epoch': 0.0}
{'loss': 3.5331, 'grad_norm': 0.5767889135365928, 'learning_rate': 5.5710044632370994e-05, 'epoch': 0.0}
{'loss': 3.5545, 'grad_norm': 0.5386665399772346, 'learning_rate': 5.620973524009008e-05, 'epoch': 0.0}
{'loss': 3.5528, 'grad_norm': 0.5459257457809031, 'learning_rate': 5.668902452709834e-05, 'epoch': 0.0}
{'loss': 3.5594, 'grad_norm': 0.527084956738389, 'learning_rate': 5.714951323824802e-05, 'epoch': 0.0}
{'loss': 3.5157, 'grad_norm': 0.5571340302417748, 'learning_rate': 5.7592620810598065e-05, 'epoch': 0.0}
{'loss': 3.4991, 'grad_norm': 0.5515610772526238, 'learning_rate': 5.8019611761179374e-05, 'epoch': 0.0}
{'loss': 3.4901, 'grad_norm': 0.5316548493669714, 'learning_rate': 5.843161744218982e-05, 'epoch': 0.0}
{'loss': 3.4983, 'grad_norm': 0.5367322481664366, 'learning_rate': 5.882965410654361e-05, 'epoch': 0.0}
{'loss': 3.4913, 'grad_norm': 0.5544740260203628, 'learning_rate': 5.9214638010114533e-05, 'epoch': 0.0}
{'loss': 3.5031, 'grad_norm': 0.5814023265857173, 'learning_rate': 5.95873981154809e-05, 'epoch': 0.0}
{'loss': 3.532, 'grad_norm': 0.5252786360095381, 'learning_rate': 5.994868684024254e-05, 'epoch': 0.0}
{'loss': 3.5002, 'grad_norm': 0.5253157904278838, 'learning_rate': 6.029918920033657e-05, 'epoch': 0.0}
{'loss': 3.485, 'grad_norm': 0.5838969911674632, 'learning_rate': 6.063953062763291e-05, 'epoch': 0.0}
{'loss': 3.4663, 'grad_norm': 0.5360234052431596, 'learning_rate': 6.0970283685988885e-05, 'epoch': 0.0}
{'loss': 3.4756, 'grad_norm': 0.5254458115250796, 'learning_rate': 6.129197386692462e-05, 'epoch': 0.0}
{'loss': 3.4232, 'grad_norm': 0.5627641027558374, 'learning_rate': 6.160508461224668e-05, 'epoch': 0.0}
{'loss': 3.4337, 'grad_norm': 0.5322236519356092, 'learning_rate': 6.191006168414683e-05, 'epoch': 0.0}
{'loss': 3.4377, 'grad_norm': 0.5311122114789611, 'learning_rate': 6.220731698193443e-05, 'epoch': 0.0}
{'loss': 3.4704, 'grad_norm': 0.5431831019574791, 'learning_rate': 6.249723188741359e-05, 'epoch': 0.0}
{'loss': 3.4953, 'grad_norm': 0.5235846537881942, 'learning_rate': 6.278016020707817e-05, 'epoch': 0.0}
{'loss': 3.4316, 'grad_norm': 0.5242701538377905, 'learning_rate': 6.305643076806426e-05, 'epoch': 0.0}
{'loss': 3.4583, 'grad_norm': 0.5510740770948896, 'learning_rate': 6.332634971563337e-05, 'epoch': 0.0}
{'loss': 3.4084, 'grad_norm': 0.5204212113040884, 'learning_rate': 6.359020255244241e-05, 'epoch': 0.0}
{'loss': 3.4268, 'grad_norm': 0.5177230534285426, 'learning_rate': 6.384825595366063e-05, 'epoch': 0.0}
{'loss': 3.396, 'grad_norm': 0.5450309634704722, 'learning_rate': 6.410075938686486e-05, 'epoch': 0.0}
{'loss': 3.4153, 'grad_norm': 0.5721711156313198, 'learning_rate': 6.43479465613797e-05, 'epoch': 0.0}
{'loss': 3.4462, 'grad_norm': 0.5200240201041759, 'learning_rate': 6.459003672816747e-05, 'epoch': 0.0}
{'loss': 3.3991, 'grad_norm': 0.5269913980386575, 'learning_rate': 6.482723584838796e-05, 'epoch': 0.0}
{'loss': 3.4172, 'grad_norm': 0.5062052855239857, 'learning_rate': 6.505973764623537e-05, 'epoch': 0.0}
{'loss': 3.3877, 'grad_norm': 0.5049057598896824, 'learning_rate': 6.528772455953764e-05, 'epoch': 0.0}
{'loss': 3.3548, 'grad_norm': 0.5176534851607796, 'learning_rate': 6.551136859980541e-05, 'epoch': 0.0}
{'loss': 3.3483, 'grad_norm': 0.5195013477783086, 'learning_rate': 6.573083213188768e-05, 'epoch': 0.0}
{'loss': 3.3677, 'grad_norm': 0.5167745661350771, 'learning_rate': 6.59462685820869e-05, 'epoch': 0.0}
{'loss': 3.429, 'grad_norm': 0.5121602311407273, 'learning_rate': 6.6157823082469e-05, 'epoch': 0.0}
{'loss': 3.4278, 'grad_norm': 0.5473904480224004, 'learning_rate': 6.636563305814549e-05, 'epoch': 0.0}
{'loss': 3.3282, 'grad_norm': 0.5393318924499715, 'learning_rate': 6.656982876347945e-05, 'epoch': 0.0}
{'loss': 3.3608, 'grad_norm': 0.5157638857023421, 'learning_rate': 6.677053377245504e-05, 'epoch': 0.0}
{'loss': 3.4279, 'grad_norm': 1.012116880031657, 'learning_rate': 6.696786542783324e-05, 'epoch': 0.0}
{'loss': 3.3849, 'grad_norm': 0.5376680972045832, 'learning_rate': 6.716193525318215e-05, 'epoch': 0.0}
{'loss': 3.3445, 'grad_norm': 0.4950479609872787, 'learning_rate': 6.735284933140416e-05, 'epoch': 0.0}
{'loss': 3.4001, 'grad_norm': 0.5517504688862892, 'learning_rate': 6.754070865297698e-05, 'epoch': 0.0}
{'loss': 3.3909, 'grad_norm': 0.5463704567186753, 'learning_rate': 6.772560943677052e-05, 'epoch': 0.0}
{'loss': 3.3409, 'grad_norm': 0.5036738069658915, 'learning_rate': 6.790764342599117e-05, 'epoch': 0.0}
{'loss': 3.3363, 'grad_norm': 0.5200583747965143, 'learning_rate': 6.808689816153217e-05, 'epoch': 0.0}
{'loss': 3.3521, 'grad_norm': 0.5034542986137259, 'learning_rate': 6.826345723476884e-05, 'epoch': 0.0}
{'loss': 3.3675, 'grad_norm': 0.5100542743374471, 'learning_rate': 6.84374005216262e-05, 'epoch': 0.0}
{'loss': 3.3134, 'grad_norm': 0.5194613067365325, 'learning_rate': 6.860880439955943e-05, 'epoch': 0.0}
{'loss': 3.3792, 'grad_norm': 0.5351724536235586, 'learning_rate': 6.877774194892253e-05, 'epoch': 0.0}
{'loss': 3.3567, 'grad_norm': 0.5240002151498396, 'learning_rate': 6.894428314005404e-05, 'epoch': 0.0}
{'loss': 3.3303, 'grad_norm': 0.5082619958771706, 'learning_rate': 6.910849500727851e-05, 'epoch': 0.0}
{'loss': 3.3458, 'grad_norm': 0.5161594238452505, 'learning_rate': 6.927044181090669e-05, 'epoch': 0.0}
{'loss': 3.3634, 'grad_norm': 0.5558469811652874, 'learning_rate': 6.943018518821426e-05, 'epoch': 0.0}
{'loss': 3.3215, 'grad_norm': 0.5188649550563447, 'learning_rate': 6.958778429428677e-05, 'epoch': 0.0}
{'loss': 3.289, 'grad_norm': 0.5072563222559052, 'learning_rate': 6.97432959335363e-05, 'epoch': 0.0}
{'loss': 3.3271, 'grad_norm': 0.5155634394505076, 'learning_rate': 6.989677468262147e-05, 'epoch': 0.0}
{'loss': 3.3415, 'grad_norm': 0.5029585167687803, 'learning_rate': 7.004827300543644e-05, 'epoch': 0.0}
{'loss': 3.3455, 'grad_norm': 0.5134774812375273, 'learning_rate': 7.019784136077525e-05, 'epoch': 0.0}
{'loss': 3.3058, 'grad_norm': 0.5106145016283465, 'learning_rate': 7.034552830322405e-05, 'epoch': 0.0}
{'loss': 3.2881, 'grad_norm': 0.51992496096693, 'learning_rate': 7.049138057778648e-05, 'epoch': 0.0}
{'loss': 3.3476, 'grad_norm': 0.5199661761462935, 'learning_rate': 7.063544320870321e-05, 'epoch': 0.0}
{'loss': 3.2762, 'grad_norm': 0.5059507380167768, 'learning_rate': 7.077775958288809e-05, 'epoch': 0.0}
{'loss': 3.3364, 'grad_norm': 0.5220493984228869, 'learning_rate': 7.09183715283678e-05, 'epoch': 0.0}
{'loss': 3.283, 'grad_norm': 0.5250908489551466, 'learning_rate': 7.105731938807974e-05, 'epoch': 0.0}
{'loss': 3.2791, 'grad_norm': 0.4974026539108647, 'learning_rate': 7.119464208935388e-05, 'epoch': 0.0}
{'loss': 3.3233, 'grad_norm': 0.5298750466531356, 'learning_rate': 7.133037720937826e-05, 'epoch': 0.0}
{'loss': 3.3105, 'grad_norm': 0.5496648466251893, 'learning_rate': 7.146456103692298e-05, 'epoch': 0.0}
{'loss': 3.278, 'grad_norm': 0.5169764944211165, 'learning_rate': 7.159722863057722e-05, 'epoch': 0.0}
{'loss': 3.2729, 'grad_norm': 0.5200177999105683, 'learning_rate': 7.172841387373204e-05, 'epoch': 0.0}
{'loss': 3.3122, 'grad_norm': 0.5127210471031544, 'learning_rate': 7.185814952652574e-05, 'epoch': 0.0}
{'loss': 3.2851, 'grad_norm': 0.5153552998624034, 'learning_rate': 7.198646727495026e-05, 'epoch': 0.0}
{'loss': 3.2976, 'grad_norm': 0.5155563679629002, 'learning_rate': 7.211339777730295e-05, 'epoch': 0.0}
{'loss': 3.2946, 'grad_norm': 0.5114493108739725, 'learning_rate': 7.223897070815449e-05, 'epoch': 0.0}
{'loss': 3.2608, 'grad_norm': 0.580819911345615, 'learning_rate': 7.236321479998983e-05, 'epoch': 0.0}
{'loss': 3.2412, 'grad_norm': 0.4996216745390281, 'learning_rate': 7.248615788266932e-05, 'epoch': 0.0}
{'loss': 3.3047, 'grad_norm': 0.5047604621910066, 'learning_rate': 7.260782692084485e-05, 'epoch': 0.0}
{'loss': 3.2935, 'grad_norm': 0.5142312629308586, 'learning_rate': 7.272824804945709e-05, 'epoch': 0.0}
{'loss': 3.2671, 'grad_norm': 0.5057808565128293, 'learning_rate': 7.284744660743098e-05, 'epoch': 0.0}
{'loss': 3.3123, 'grad_norm': 0.5201650401512199, 'learning_rate': 7.296544716967758e-05, 'epoch': 0.0}
{'loss': 3.283, 'grad_norm': 0.48804633744655856, 'learning_rate': 7.308227357750401e-05, 'epoch': 0.0}
{'loss': 3.2647, 'grad_norm': 0.5082673903271439, 'learning_rate': 7.319794896752499e-05, 'epoch': 0.0}
{'loss': 3.3074, 'grad_norm': 0.5071548193666048, 'learning_rate': 7.331249579916429e-05, 'epoch': 0.0}
{'loss': 3.2193, 'grad_norm': 0.49083100715790845, 'learning_rate': 7.342593588082727e-05, 'epoch': 0.0}
{'loss': 3.2411, 'grad_norm': 0.5296698265484562, 'learning_rate': 7.353829039482133e-05, 'epoch': 0.0}
{'loss': 3.2752, 'grad_norm': 0.5005447948744842, 'learning_rate': 7.364957992109503e-05, 'epoch': 0.0}
{'loss': 3.2533, 'grad_norm': 0.5056871071226547, 'learning_rate': 7.375982445986283e-05, 'epoch': 0.0}
{'loss': 3.2596, 'grad_norm': 0.49422934014123887, 'learning_rate': 7.386904345317732e-05, 'epoch': 0.0}
{'loss': 3.2957, 'grad_norm': 0.4978113349213621, 'learning_rate': 7.397725580550732e-05, 'epoch': 0.0}
{'loss': 3.2471, 'grad_norm': 0.48989772240242635, 'learning_rate': 7.408447990337652e-05, 'epoch': 0.0}
{'loss': 3.2734, 'grad_norm': 0.5152719966548173, 'learning_rate': 7.419073363411306e-05, 'epoch': 0.0}
{'loss': 3.2545, 'grad_norm': 0.5060853447251997, 'learning_rate': 7.429603440375862e-05, 'epoch': 0.0}
{'loss': 3.2758, 'grad_norm': 0.4943891823164413, 'learning_rate': 7.440039915418139e-05, 'epoch': 0.0}
{'loss': 3.2566, 'grad_norm': 0.49373937652881905, 'learning_rate': 7.450384437943511e-05, 'epoch': 0.0}
{'loss': 3.2473, 'grad_norm': 0.4951031587139764, 'learning_rate': 7.460638614140379e-05, 'epoch': 0.01}
{'loss': 3.2688, 'grad_norm': 0.5003664975409501, 'learning_rate': 7.470804008476907e-05, 'epoch': 0.01}
{'loss': 3.2332, 'grad_norm': 0.49772774576558987, 'learning_rate': 7.480882145133525e-05, 'epoch': 0.01}
{'loss': 3.2386, 'grad_norm': 0.4893597480825484, 'learning_rate': 7.490874509374465e-05, 'epoch': 0.01}
{'loss': 3.2295, 'grad_norm': 0.5055176018508873, 'learning_rate': 7.50078254886143e-05, 'epoch': 0.01}
{'loss': 3.2697, 'grad_norm': 0.49900701569416034, 'learning_rate': 7.510607674912285e-05, 'epoch': 0.01}
{'loss': 3.2346, 'grad_norm': 0.5064463444726529, 'learning_rate': 7.520351263707547e-05, 'epoch': 0.01}
{'loss': 3.2377, 'grad_norm': 0.5141870961499986, 'learning_rate': 7.530014657447177e-05, 'epoch': 0.01}
{'loss': 3.2789, 'grad_norm': 0.4927789304101167, 'learning_rate': 7.539599165460201e-05, 'epoch': 0.01}
{'loss': 3.1829, 'grad_norm': 0.5007646230851226, 'learning_rate': 7.549106065269378e-05, 'epoch': 0.01}
{'loss': 3.2166, 'grad_norm': 0.49749564605805596, 'learning_rate': 7.558536603613112e-05, 'epoch': 0.01}
{'loss': 3.2471, 'grad_norm': 0.5052100664705493, 'learning_rate': 7.567891997426661e-05, 'epoch': 0.01}
{'loss': 3.1939, 'grad_norm': 0.4919925903446776, 'learning_rate': 7.577173434784545e-05, 'epoch': 0.01}
{'loss': 3.2069, 'grad_norm': 0.5027738027950143, 'learning_rate': 7.586382075806015e-05, 'epoch': 0.01}
{'loss': 3.2272, 'grad_norm': 0.48025579553673886, 'learning_rate': 7.595519053525268e-05, 'epoch': 0.01}
{'loss': 3.2266, 'grad_norm': 0.4818688743224686, 'learning_rate': 7.604585474728082e-05, 'epoch': 0.01}
{'loss': 3.2161, 'grad_norm': 0.5033011733794818, 'learning_rate': 7.61358242075637e-05, 'epoch': 0.01}
{'loss': 3.2274, 'grad_norm': 0.494716685988419, 'learning_rate': 7.62251094828218e-05, 'epoch': 0.01}
{'loss': 3.2197, 'grad_norm': 0.5025020032384471, 'learning_rate': 7.631372090052439e-05, 'epoch': 0.01}
{'loss': 3.1809, 'grad_norm': 0.5051443243554404, 'learning_rate': 7.640166855605846e-05, 'epoch': 0.01}
{'loss': 3.224, 'grad_norm': 0.5103003968760691, 'learning_rate': 7.648896231963085e-05, 'epoch': 0.01}
{'loss': 3.2086, 'grad_norm': 0.481677444383247, 'learning_rate': 7.65756118429158e-05, 'epoch': 0.01}
{'loss': 3.2228, 'grad_norm': 0.5035174032997897, 'learning_rate': 7.66616265654591e-05, 'epoch': 0.01}
{'loss': 3.2198, 'grad_norm': 0.5209739453167236, 'learning_rate': 7.674701572084905e-05, 'epoch': 0.01}
{'loss': 3.2033, 'grad_norm': 0.4949794533151161, 'learning_rate': 7.683178834266492e-05, 'epoch': 0.01}
{'loss': 3.2305, 'grad_norm': 0.49506272831616105, 'learning_rate': 7.691595327021215e-05, 'epoch': 0.01}
{'loss': 3.208, 'grad_norm': 0.5074143296652847, 'learning_rate': 7.69995191540533e-05, 'epoch': 0.01}
{'loss': 3.1818, 'grad_norm': 0.5011048234592202, 'learning_rate': 7.708249446134367e-05, 'epoch': 0.01}
{'loss': 3.213, 'grad_norm': 0.49510433203886955, 'learning_rate': 7.716488748097975e-05, 'epoch': 0.01}
{'loss': 3.2297, 'grad_norm': 0.5111362552229959, 'learning_rate': 7.724670632856813e-05, 'epoch': 0.01}
{'loss': 3.1829, 'grad_norm': 0.5071085709718668, 'learning_rate': 7.73279589512226e-05, 'epoch': 0.01}
{'loss': 3.1606, 'grad_norm': 0.5052722579473187, 'learning_rate': 7.740865313219632e-05, 'epoch': 0.01}
{'loss': 3.1769, 'grad_norm': 0.49535287724708277, 'learning_rate': 7.74887964953559e-05, 'epoch': 0.01}
{'loss': 3.2126, 'grad_norm': 0.5241222463034656, 'learning_rate': 7.756839650950389e-05, 'epoch': 0.01}
{'loss': 3.1981, 'grad_norm': 0.5019364543056198, 'learning_rate': 7.764746049255561e-05, 'epoch': 0.01}
{'loss': 3.1843, 'grad_norm': 0.4959368502789724, 'learning_rate': 7.772599561557638e-05, 'epoch': 0.01}
{'loss': 3.1868, 'grad_norm': 0.5186125500287943, 'learning_rate': 7.780400890668461e-05, 'epoch': 0.01}
{'loss': 3.1844, 'grad_norm': 0.49881293547233674, 'learning_rate': 7.788150725482592e-05, 'epoch': 0.01}
{'loss': 3.1771, 'grad_norm': 0.5058160526219782, 'learning_rate': 7.79584974134238e-05, 'epoch': 0.01}
{'loss': 3.1654, 'grad_norm': 0.49523911003546495, 'learning_rate': 7.803498600391108e-05, 'epoch': 0.01}
{'loss': 3.1719, 'grad_norm': 0.5006415574252722, 'learning_rate': 7.811097951914732e-05, 'epoch': 0.01}
{'loss': 3.1736, 'grad_norm': 0.4931452759917088, 'learning_rate': 7.818648432672608e-05, 'epoch': 0.01}
{'loss': 3.2146, 'grad_norm': 0.5087183951572302, 'learning_rate': 7.82615066721768e-05, 'epoch': 0.01}
{'loss': 3.1765, 'grad_norm': 0.49226054154727483, 'learning_rate': 7.833605268206489e-05, 'epoch': 0.01}
{'loss': 3.1442, 'grad_norm': 0.48754063502430867, 'learning_rate': 7.841012836699384e-05, 'epoch': 0.01}
{'loss': 3.1614, 'grad_norm': 0.49873850436131495, 'learning_rate': 7.848373962451368e-05, 'epoch': 0.01}
{'loss': 3.1677, 'grad_norm': 0.5190378483867981, 'learning_rate': 7.855689224193834e-05, 'epoch': 0.01}
{'loss': 3.1778, 'grad_norm': 0.5118160336477404, 'learning_rate': 7.862959189907611e-05, 'epoch': 0.01}
{'loss': 3.1616, 'grad_norm': 0.5227743828763786, 'learning_rate': 7.870184417087593e-05, 'epoch': 0.01}
{'loss': 3.1867, 'grad_norm': 0.5016344725834668, 'learning_rate': 7.877365452999284e-05, 'epoch': 0.01}
{'loss': 3.1526, 'grad_norm': 0.5087654661277714, 'learning_rate': 7.884502834927533e-05, 'epoch': 0.01}
{'loss': 3.187, 'grad_norm': 0.5165630409049978, 'learning_rate': 7.89159709041777e-05, 'epoch': 0.01}
{'loss': 3.2107, 'grad_norm': 0.4961331289678903, 'learning_rate': 7.89864873750999e-05, 'epoch': 0.01}
{'loss': 3.2417, 'grad_norm': 0.5447152915994744, 'learning_rate': 7.905658284965742e-05, 'epoch': 0.01}
{'loss': 3.1617, 'grad_norm': 0.49313950544345825, 'learning_rate': 7.912626232488402e-05, 'epoch': 0.01}
{'loss': 3.145, 'grad_norm': 0.48770180270999536, 'learning_rate': 7.919553070936936e-05, 'epoch': 0.01}
{'loss': 3.1555, 'grad_norm': 0.5017792631575657, 'learning_rate': 7.92643928253339e-05, 'epoch': 0.01}
{'loss': 3.1471, 'grad_norm': 0.4916263271508475, 'learning_rate': 7.933285341064351e-05, 'epoch': 0.01}
{'loss': 3.1672, 'grad_norm': 0.5181521280244454, 'learning_rate': 7.940091712076538e-05, 'epoch': 0.01}
{'loss': 3.1308, 'grad_norm': 0.5026989316053136, 'learning_rate': 7.946858853066788e-05, 'epoch': 0.01}
{'loss': 3.1952, 'grad_norm': 0.4927076511595482, 'learning_rate': 7.953587213666568e-05, 'epoch': 0.01}
{'loss': 3.1472, 'grad_norm': 0.4886133490134295, 'learning_rate': 7.960277235821263e-05, 'epoch': 0.01}
{'loss': 3.162, 'grad_norm': 0.4833424134354433, 'learning_rate': 7.966929353964345e-05, 'epoch': 0.01}
{'loss': 3.1607, 'grad_norm': 0.4884889426471995, 'learning_rate': 7.973543995186684e-05, 'epoch': 0.01}
{'loss': 3.1914, 'grad_norm': 0.5013461156365077, 'learning_rate': 7.980121579401066e-05, 'epoch': 0.01}
{'loss': 3.1977, 'grad_norm': 0.4858427401735162, 'learning_rate': 7.986662519502166e-05, 'epoch': 0.01}
{'loss': 3.1353, 'grad_norm': 0.49616619350109303, 'learning_rate': 7.993167221522075e-05, 'epoch': 0.01}
{'loss': 3.1333, 'grad_norm': 0.5021264960257743, 'learning_rate': 7.999636084781537e-05, 'epoch': 0.01}
{'loss': 3.1514, 'grad_norm': 0.49457963772934055, 'learning_rate': 8.006069502037056e-05, 'epoch': 0.01}
{'loss': 3.1316, 'grad_norm': 0.5044051316320302, 'learning_rate': 8.012467859623988e-05, 'epoch': 0.01}
{'loss': 3.1708, 'grad_norm': 0.49685136239292077, 'learning_rate': 8.018831537595742e-05, 'epoch': 0.01}
{'loss': 3.1205, 'grad_norm': 0.4904089328999888, 'learning_rate': 8.025160909859258e-05, 'epoch': 0.01}
{'loss': 3.133, 'grad_norm': 0.504396014714067, 'learning_rate': 8.031456344306829e-05, 'epoch': 0.01}
{'loss': 3.1472, 'grad_norm': 0.4978362673651721, 'learning_rate': 8.037718202944411e-05, 'epoch': 0.01}
{'loss': 3.1688, 'grad_norm': 0.49324506797677464, 'learning_rate': 8.043946842016542e-05, 'epoch': 0.01}
{'loss': 3.1406, 'grad_norm': 0.4960660273635031, 'learning_rate': 8.050142612127945e-05, 'epoch': 0.01}
{'loss': 3.1534, 'grad_norm': 0.49713955916971736, 'learning_rate': 8.056305858361968e-05, 'epoch': 0.01}
{'loss': 3.1121, 'grad_norm': 0.5149264966393525, 'learning_rate': 8.062436920395896e-05, 'epoch': 0.01}
{'loss': 3.1109, 'grad_norm': 0.5225642961612917, 'learning_rate': 8.068536132613294e-05, 'epoch': 0.01}
{'loss': 3.1792, 'grad_norm': 0.49682486307310725, 'learning_rate': 8.074603824213446e-05, 'epoch': 0.01}
{'loss': 3.1427, 'grad_norm': 0.5055029253461274, 'learning_rate': 8.080640319317962e-05, 'epoch': 0.01}
{'loss': 3.1446, 'grad_norm': 0.49272265191493897, 'learning_rate': 8.086645937074672e-05, 'epoch': 0.01}
{'loss': 3.1279, 'grad_norm': 0.4901772102292897, 'learning_rate': 8.092620991758884e-05, 'epoch': 0.01}
{'loss': 3.1134, 'grad_norm': 0.5111641406229008, 'learning_rate': 8.09856579287206e-05, 'epoch': 0.01}
{'loss': 3.1025, 'grad_norm': 0.5056445871151592, 'learning_rate': 8.104480645238018e-05, 'epoch': 0.01}
{'loss': 3.1597, 'grad_norm': 0.4841234562781598, 'learning_rate': 8.110365849096721e-05, 'epoch': 0.01}
{'loss': 3.1276, 'grad_norm': 0.4996286034187232, 'learning_rate': 8.116221700195726e-05, 'epoch': 0.01}
{'loss': 3.0971, 'grad_norm': 0.4914640737517345, 'learning_rate': 8.122048489879363e-05, 'epoch': 0.01}
{'loss': 3.1231, 'grad_norm': 0.5034019289690205, 'learning_rate': 8.127846505175717e-05, 'epoch': 0.01}
{'loss': 3.1271, 'grad_norm': 0.4991545254337578, 'learning_rate': 8.133616028881462e-05, 'epoch': 0.01}
{'loss': 3.1139, 'grad_norm': 0.49681999420486944, 'learning_rate': 8.139357339644637e-05, 'epoch': 0.01}
{'loss': 3.1549, 'grad_norm': 0.5107514555502223, 'learning_rate': 8.145070712045392e-05, 'epoch': 0.01}
{'loss': 3.14, 'grad_norm': 0.4956020740718925, 'learning_rate': 8.150756416674785e-05, 'epoch': 0.01}
{'loss': 3.1572, 'grad_norm': 0.50894213107832, 'learning_rate': 8.15641472021169e-05, 'epoch': 0.01}
{'loss': 3.109, 'grad_norm': 0.48668618041602796, 'learning_rate': 8.162045885497838e-05, 'epoch': 0.01}
{'loss': 3.1191, 'grad_norm': 0.4952875891631952, 'learning_rate': 8.167650171611095e-05, 'epoch': 0.01}
{'loss': 3.0839, 'grad_norm': 0.5047330078798764, 'learning_rate': 8.173227833936972e-05, 'epoch': 0.01}
{'loss': 3.1296, 'grad_norm': 0.4830486742681456, 'learning_rate': 8.178779124238466e-05, 'epoch': 0.01}
{'loss': 3.1064, 'grad_norm': 0.5068024632073862, 'learning_rate': 8.184304290724247e-05, 'epoch': 0.01}
{'loss': 3.1153, 'grad_norm': 0.49209740613254105, 'learning_rate': 8.189803578115246e-05, 'epoch': 0.01}
{'loss': 3.1204, 'grad_norm': 0.4854535455309129, 'learning_rate': 8.195277227709704e-05, 'epoch': 0.01}
{'loss': 3.1207, 'grad_norm': 0.5067290941144872, 'learning_rate': 8.200725477446693e-05, 'epoch': 0.01}
{'loss': 3.1082, 'grad_norm': 0.48919309600592026, 'learning_rate': 8.206148561968188e-05, 'epoch': 0.01}
{'loss': 3.1275, 'grad_norm': 0.48817062767542413, 'learning_rate': 8.211546712679696e-05, 'epoch': 0.01}
{'loss': 3.0747, 'grad_norm': 0.4964497179310302, 'learning_rate': 8.216920157809512e-05, 'epoch': 0.01}
{'loss': 3.0701, 'grad_norm': 0.5039516722733831, 'learning_rate': 8.222269122466616e-05, 'epoch': 0.01}
{'loss': 3.1141, 'grad_norm': 0.4940356934249443, 'learning_rate': 8.227593828697257e-05, 'epoch': 0.01}
{'loss': 3.08, 'grad_norm': 0.485501287762442, 'learning_rate': 8.232894495540269e-05, 'epoch': 0.01}
{'loss': 3.1205, 'grad_norm': 0.49535987836914974, 'learning_rate': 8.23817133908113e-05, 'epoch': 0.01}
{'loss': 3.1286, 'grad_norm': 0.4954960485486234, 'learning_rate': 8.243424572504824e-05, 'epoch': 0.01}
{'loss': 3.1324, 'grad_norm': 0.4946052932652078, 'learning_rate': 8.24865440614752e-05, 'epoch': 0.01}
{'loss': 3.0891, 'grad_norm': 0.48556429238037496, 'learning_rate': 8.2538610475471e-05, 'epoch': 0.01}
{'loss': 3.0749, 'grad_norm': 0.4818281217377617, 'learning_rate': 8.259044701492588e-05, 'epoch': 0.01}
{'loss': 3.1052, 'grad_norm': 0.4940908441292221, 'learning_rate': 8.264205570072473e-05, 'epoch': 0.01}
{'loss': 3.1059, 'grad_norm': 0.5111481123901211, 'learning_rate': 8.269343852721983e-05, 'epoch': 0.01}
{'loss': 3.0846, 'grad_norm': 0.49087053133580544, 'learning_rate': 8.27445974626934e-05, 'epoch': 0.01}
{'loss': 3.1143, 'grad_norm': 0.5084132646818691, 'learning_rate': 8.279553444980988e-05, 'epoch': 0.01}
{'loss': 3.1022, 'grad_norm': 0.49090522940725023, 'learning_rate': 8.284625140605869e-05, 'epoch': 0.01}
{'loss': 3.0373, 'grad_norm': 0.5078464476451764, 'learning_rate': 8.289675022418723e-05, 'epoch': 0.01}
{'loss': 3.1007, 'grad_norm': 0.49702222368854665, 'learning_rate': 8.294703277262488e-05, 'epoch': 0.01}
{'loss': 3.0948, 'grad_norm': 0.5085705667197732, 'learning_rate': 8.299710089589764e-05, 'epoch': 0.01}
{'loss': 3.1486, 'grad_norm': 0.4934383420202479, 'learning_rate': 8.304695641503428e-05, 'epoch': 0.01}
{'loss': 3.0882, 'grad_norm': 0.5124408185817819, 'learning_rate': 8.309660112796368e-05, 'epoch': 0.01}
{'loss': 3.0847, 'grad_norm': 0.5200774552126215, 'learning_rate': 8.31460368099039e-05, 'epoch': 0.01}
{'loss': 3.0568, 'grad_norm': 0.5027549019390999, 'learning_rate': 8.319526521374312e-05, 'epoch': 0.01}
{'loss': 3.0779, 'grad_norm': 0.4961727567067992, 'learning_rate': 8.324428807041249e-05, 'epoch': 0.01}
{'loss': 3.1367, 'grad_norm': 0.5109406882499451, 'learning_rate': 8.329310708925138e-05, 'epoch': 0.01}
{'loss': 3.0792, 'grad_norm': 0.48763656528645827, 'learning_rate': 8.334172395836509e-05, 'epoch': 0.01}
{'loss': 3.0705, 'grad_norm': 0.5157661144911376, 'learning_rate': 8.339014034497491e-05, 'epoch': 0.01}
{'loss': 3.0757, 'grad_norm': 0.4874371907605979, 'learning_rate': 8.34383578957614e-05, 'epoch': 0.01}
{'loss': 3.1144, 'grad_norm': 0.5088183932067791, 'learning_rate': 8.348637823720025e-05, 'epoch': 0.01}
{'loss': 3.0902, 'grad_norm': 0.4952265081283842, 'learning_rate': 8.353420297589165e-05, 'epoch': 0.01}
{'loss': 3.0705, 'grad_norm': 0.5129141945915829, 'learning_rate': 8.358183369888267e-05, 'epoch': 0.01}
{'loss': 3.1005, 'grad_norm': 0.49465876097389677, 'learning_rate': 8.362927197398341e-05, 'epoch': 0.01}
{'loss': 3.0672, 'grad_norm': 0.4962268397137096, 'learning_rate': 8.367651935007651e-05, 'epoch': 0.01}
{'loss': 3.0445, 'grad_norm': 0.5014685259889178, 'learning_rate': 8.372357735742074e-05, 'epoch': 0.01}
{'loss': 3.0876, 'grad_norm': 0.5078191901662478, 'learning_rate': 8.377044750794827e-05, 'epoch': 0.01}
{'loss': 3.0589, 'grad_norm': 0.4979307285173839, 'learning_rate': 8.381713129555623e-05, 'epoch': 0.01}
{'loss': 3.0597, 'grad_norm': 0.5038075214899654, 'learning_rate': 8.386363019639234e-05, 'epoch': 0.01}
{'loss': 3.1051, 'grad_norm': 0.5090379510829339, 'learning_rate': 8.390994566913507e-05, 'epoch': 0.01}
{'loss': 3.0743, 'grad_norm': 0.495643503896624, 'learning_rate': 8.395607915526815e-05, 'epoch': 0.01}
{'loss': 3.091, 'grad_norm': 0.49748638935463796, 'learning_rate': 8.400203207934977e-05, 'epoch': 0.01}
{'loss': 3.0587, 'grad_norm': 0.5107672403994675, 'learning_rate': 8.404780584927655e-05, 'epoch': 0.01}
{'loss': 3.057, 'grad_norm': 0.5308302606200964, 'learning_rate': 8.409340185654231e-05, 'epoch': 0.01}
{'loss': 3.0816, 'grad_norm': 0.49030115113444506, 'learning_rate': 8.413882147649197e-05, 'epoch': 0.01}
{'loss': 3.0649, 'grad_norm': 0.5117779829209608, 'learning_rate': 8.418406606857043e-05, 'epoch': 0.01}
{'loss': 3.0677, 'grad_norm': 0.4885014740250971, 'learning_rate': 8.422913697656668e-05, 'epoch': 0.01}
{'loss': 3.0595, 'grad_norm': 0.5081073471056622, 'learning_rate': 8.427403552885332e-05, 'epoch': 0.01}
{'loss': 3.0394, 'grad_norm': 0.4961806049216322, 'learning_rate': 8.431876303862155e-05, 'epoch': 0.01}
{'loss': 3.0421, 'grad_norm': 0.5019859203819834, 'learning_rate': 8.436332080411142e-05, 'epoch': 0.01}
{'loss': 3.0644, 'grad_norm': 0.5064490446765219, 'learning_rate': 8.44077101088382e-05, 'epoch': 0.01}
{'loss': 3.0727, 'grad_norm': 0.5020040978585429, 'learning_rate': 8.445193222181402e-05, 'epoch': 0.01}
{'loss': 3.061, 'grad_norm': 0.5139637043237685, 'learning_rate': 8.449598839776565e-05, 'epoch': 0.01}
{'loss': 3.0743, 'grad_norm': 0.5068893922384696, 'learning_rate': 8.453987987734808e-05, 'epoch': 0.01}
{'loss': 3.0551, 'grad_norm': 0.5145443822831743, 'learning_rate': 8.45836078873542e-05, 'epoch': 0.01}
{'loss': 3.0407, 'grad_norm': 0.5102119730677595, 'learning_rate': 8.462717364092046e-05, 'epoch': 0.01}
{'loss': 3.1366, 'grad_norm': 0.5056741976408656, 'learning_rate': 8.467057833772894e-05, 'epoch': 0.01}
{'loss': 3.0759, 'grad_norm': 0.49144512497126674, 'learning_rate': 8.471382316420545e-05, 'epoch': 0.01}
{'loss': 3.0419, 'grad_norm': 0.5359464380439711, 'learning_rate': 8.475690929371418e-05, 'epoch': 0.01}
{'loss': 3.0639, 'grad_norm': 0.4937707822327798, 'learning_rate': 8.479983788674874e-05, 'epoch': 0.01}
{'loss': 3.0735, 'grad_norm': 0.5073058425163735, 'learning_rate': 8.484261009111968e-05, 'epoch': 0.01}
{'loss': 3.055, 'grad_norm': 0.5043735147936524, 'learning_rate': 8.488522704213867e-05, 'epoch': 0.01}
{'loss': 3.021, 'grad_norm': 0.4951860076597731, 'learning_rate': 8.492768986279929e-05, 'epoch': 0.01}
{'loss': 3.0205, 'grad_norm': 0.49953262498285633, 'learning_rate': 8.496999966395455e-05, 'epoch': 0.01}
{'loss': 3.048, 'grad_norm': 0.49991521151519347, 'learning_rate': 8.501215754449139e-05, 'epoch': 0.01}
{'loss': 3.06, 'grad_norm': 0.5238897601990974, 'learning_rate': 8.505416459150177e-05, 'epoch': 0.01}
{'loss': 3.049, 'grad_norm': 0.5027141761495558, 'learning_rate': 8.509602188045101e-05, 'epoch': 0.01}
{'loss': 3.0537, 'grad_norm': 0.5052923825674284, 'learning_rate': 8.513773047534291e-05, 'epoch': 0.01}
{'loss': 3.0769, 'grad_norm': 0.5247818150924911, 'learning_rate': 8.517929142888206e-05, 'epoch': 0.01}
{'loss': 3.0707, 'grad_norm': 0.5226894601983793, 'learning_rate': 8.522070578263329e-05, 'epoch': 0.01}
{'loss': 3.0359, 'grad_norm': 0.4940120699165947, 'learning_rate': 8.526197456717826e-05, 'epoch': 0.01}
{'loss': 3.0283, 'grad_norm': 0.50942223745646, 'learning_rate': 8.530309880226936e-05, 'epoch': 0.01}
{'loss': 3.0231, 'grad_norm': 0.5127381705114059, 'learning_rate': 8.534407949698094e-05, 'epoch': 0.01}
{'loss': 3.0596, 'grad_norm': 0.5240973305532538, 'learning_rate': 8.538491764985775e-05, 'epoch': 0.01}
{'loss': 3.0549, 'grad_norm': 0.5091310929866153, 'learning_rate': 8.542561424906111e-05, 'epoch': 0.01}
{'loss': 3.0396, 'grad_norm': 0.514618379654403, 'learning_rate': 8.546617027251222e-05, 'epoch': 0.01}
{'loss': 3.1305, 'grad_norm': 0.506218949724743, 'learning_rate': 8.550658668803327e-05, 'epoch': 0.01}
{'loss': 3.0308, 'grad_norm': 0.505188628945981, 'learning_rate': 8.554686445348594e-05, 'epoch': 0.01}
{'loss': 3.0909, 'grad_norm': 0.5076941822037558, 'learning_rate': 8.558700451690766e-05, 'epoch': 0.01}
{'loss': 3.0221, 'grad_norm': 0.5081650964820631, 'learning_rate': 8.562700781664552e-05, 'epoch': 0.01}
{'loss': 3.0298, 'grad_norm': 0.5149186215500186, 'learning_rate': 8.566687528148781e-05, 'epoch': 0.01}
{'loss': 3.0406, 'grad_norm': 0.503199259159674, 'learning_rate': 8.57066078307935e-05, 'epoch': 0.01}
{'loss': 3.0649, 'grad_norm': 0.5126733969697348, 'learning_rate': 8.574620637461941e-05, 'epoch': 0.01}
{'loss': 3.0365, 'grad_norm': 0.5096193176356487, 'learning_rate': 8.578567181384524e-05, 'epoch': 0.01}
{'loss': 3.0018, 'grad_norm': 0.5111404006690665, 'learning_rate': 8.582500504029662e-05, 'epoch': 0.01}
{'loss': 3.0297, 'grad_norm': 0.5075141138671135, 'learning_rate': 8.586420693686602e-05, 'epoch': 0.01}
{'loss': 3.0324, 'grad_norm': 0.5219702131795051, 'learning_rate': 8.59032783776316e-05, 'epoch': 0.01}
{'loss': 3.0279, 'grad_norm': 0.5014160291893038, 'learning_rate': 8.594222022797423e-05, 'epoch': 0.01}
{'loss': 3.0448, 'grad_norm': 0.49157203379362063, 'learning_rate': 8.598103334469243e-05, 'epoch': 0.01}
{'loss': 3.0179, 'grad_norm': 0.5016458306295994, 'learning_rate': 8.601971857611555e-05, 'epoch': 0.01}
{'loss': 3.047, 'grad_norm': 0.5053001017774744, 'learning_rate': 8.605827676221493e-05, 'epoch': 0.01}
{'loss': 3.0648, 'grad_norm': 0.5112112960499643, 'learning_rate': 8.609670873471342e-05, 'epoch': 0.01}
{'loss': 3.0464, 'grad_norm': 0.5479287045810747, 'learning_rate': 8.613501531719302e-05, 'epoch': 0.01}
{'loss': 3.038, 'grad_norm': 0.5099845293611706, 'learning_rate': 8.617319732520071e-05, 'epoch': 0.01}
{'loss': 3.0124, 'grad_norm': 0.5340615309787158, 'learning_rate': 8.621125556635271e-05, 'epoch': 0.01}
{'loss': 3.01, 'grad_norm': 0.5012049109842435, 'learning_rate': 8.624919084043694e-05, 'epoch': 0.01}
{'loss': 3.0326, 'grad_norm': 0.5152941415026057, 'learning_rate': 8.628700393951384e-05, 'epoch': 0.01}
{'loss': 3.0112, 'grad_norm': 0.5016378062055559, 'learning_rate': 8.632469564801571e-05, 'epoch': 0.01}
{'loss': 3.029, 'grad_norm': 0.5265109515011475, 'learning_rate': 8.636226674284417e-05, 'epoch': 0.01}
{'loss': 3.0314, 'grad_norm': 0.5213197043059822, 'learning_rate': 8.639971799346644e-05, 'epoch': 0.01}
{'loss': 3.0235, 'grad_norm': 0.5114833185008628, 'learning_rate': 8.643705016200976e-05, 'epoch': 0.01}
{'loss': 3.0684, 'grad_norm': 0.5396204330153468, 'learning_rate': 8.647426400335451e-05, 'epoch': 0.01}
{'loss': 3.01, 'grad_norm': 0.4935454263749582, 'learning_rate': 8.651136026522576e-05, 'epoch': 0.01}
{'loss': 3.01, 'grad_norm': 0.4985717362648132, 'learning_rate': 8.654833968828348e-05, 'epoch': 0.01}
{'loss': 3.0379, 'grad_norm': 0.507450180564821, 'learning_rate': 8.658520300621115e-05, 'epoch': 0.01}
{'loss': 3.0334, 'grad_norm': 0.5122740337486701, 'learning_rate': 8.66219509458033e-05, 'epoch': 0.01}
{'loss': 3.0183, 'grad_norm': 0.5184689037298125, 'learning_rate': 8.665858422705127e-05, 'epoch': 0.01}
{'loss': 2.9683, 'grad_norm': 0.509218467552688, 'learning_rate': 8.669510356322798e-05, 'epoch': 0.01}
{'loss': 3.0148, 'grad_norm': 0.509885706540976, 'learning_rate': 8.673150966097122e-05, 'epoch': 0.01}
{'loss': 3.0398, 'grad_norm': 0.5103492045218354, 'learning_rate': 8.676780322036573e-05, 'epoch': 0.01}
{'loss': 3.0225, 'grad_norm': 0.5293903415568413, 'learning_rate': 8.680398493502396e-05, 'epoch': 0.01}
{'loss': 3.0075, 'grad_norm': 0.5047619642943133, 'learning_rate': 8.684005549216557e-05, 'epoch': 0.01}
{'loss': 2.9996, 'grad_norm': 0.5228997218029561, 'learning_rate': 8.687601557269576e-05, 'epoch': 0.01}
{'loss': 3.0296, 'grad_norm': 0.5081550040125911, 'learning_rate': 8.691186585128246e-05, 'epoch': 0.01}
{'loss': 3.0403, 'grad_norm': 0.4931401660280471, 'learning_rate': 8.694760699643221e-05, 'epoch': 0.01}
{'loss': 3.0337, 'grad_norm': 0.5162754843457399, 'learning_rate': 8.698323967056495e-05, 'epoch': 0.01}
{'loss': 3.0524, 'grad_norm': 0.5435164641085403, 'learning_rate': 8.701876453008776e-05, 'epoch': 0.01}
{'loss': 2.9953, 'grad_norm': 0.5059754456265758, 'learning_rate': 8.705418222546732e-05, 'epoch': 0.01}
{'loss': 2.9792, 'grad_norm': 0.5152063962031731, 'learning_rate': 8.70894934013015e-05, 'epoch': 0.01}
{'loss': 3.0046, 'grad_norm': 0.5156391166697712, 'learning_rate': 8.712469869638952e-05, 'epoch': 0.01}
{'loss': 3.0224, 'grad_norm': 0.5260955567329263, 'learning_rate': 8.71597987438016e-05, 'epoch': 0.01}
{'loss': 3.0068, 'grad_norm': 0.5000441489177034, 'learning_rate': 8.719479417094704e-05, 'epoch': 0.01}
{'loss': 3.0015, 'grad_norm': 0.5267327806707254, 'learning_rate': 8.722968559964157e-05, 'epoch': 0.01}
{'loss': 2.9949, 'grad_norm': 0.5030903527199247, 'learning_rate': 8.726447364617366e-05, 'epoch': 0.01}
{'loss': 2.9883, 'grad_norm': 0.49930530099083825, 'learning_rate': 8.72991589213698e-05, 'epoch': 0.01}
{'loss': 3.0114, 'grad_norm': 0.5399602895068811, 'learning_rate': 8.733374203065898e-05, 'epoch': 0.01}
{'loss': 3.0097, 'grad_norm': 0.4980460796110064, 'learning_rate': 8.736822357413588e-05, 'epoch': 0.01}
{'loss': 3.0704, 'grad_norm': 0.5077487222630603, 'learning_rate': 8.740260414662352e-05, 'epoch': 0.01}
{'loss': 2.985, 'grad_norm': 0.5087571624193842, 'learning_rate': 8.74368843377348e-05, 'epoch': 0.02}
{'loss': 3.0128, 'grad_norm': 0.5107023900809945, 'learning_rate': 8.747106473193313e-05, 'epoch': 0.02}
{'loss': 3.0165, 'grad_norm': 0.5107179001973422, 'learning_rate': 8.75051459085922e-05, 'epoch': 0.02}
{'loss': 3.0411, 'grad_norm': 0.5138094074481258, 'learning_rate': 8.753912844205501e-05, 'epoch': 0.02}
{'loss': 3.0369, 'grad_norm': 0.5265997777517523, 'learning_rate': 8.757301290169182e-05, 'epoch': 0.02}
{'loss': 2.9725, 'grad_norm': 0.4947145874384513, 'learning_rate': 8.76067998519575e-05, 'epoch': 0.02}
{'loss': 2.9909, 'grad_norm': 0.5050413061934037, 'learning_rate': 8.764048985244785e-05, 'epoch': 0.02}
{'loss': 2.9886, 'grad_norm': 0.5074794631609966, 'learning_rate': 8.76740834579553e-05, 'epoch': 0.02}
{'loss': 3.021, 'grad_norm': 0.5169447399088982, 'learning_rate': 8.770758121852369e-05, 'epoch': 0.02}
{'loss': 3.0248, 'grad_norm': 0.5009269726073069, 'learning_rate': 8.774098367950224e-05, 'epoch': 0.02}
{'loss': 2.9392, 'grad_norm': 0.5042639302031099, 'learning_rate': 8.777429138159897e-05, 'epoch': 0.02}
{'loss': 3.0054, 'grad_norm': 0.5088581827407318, 'learning_rate': 8.780750486093308e-05, 'epoch': 0.02}
{'loss': 2.9932, 'grad_norm': 0.5097778034186595, 'learning_rate': 8.784062464908682e-05, 'epoch': 0.02}
{'loss': 2.9957, 'grad_norm': 0.5220938631599507, 'learning_rate': 8.787365127315646e-05, 'epoch': 0.02}
{'loss': 3.0109, 'grad_norm': 0.5344049932545394, 'learning_rate': 8.79065852558027e-05, 'epoch': 0.02}
{'loss': 2.9936, 'grad_norm': 0.5189418868662282, 'learning_rate': 8.79394271153003e-05, 'epoch': 0.02}
{'loss': 2.9855, 'grad_norm': 0.5092417949626796, 'learning_rate': 8.797217736558683e-05, 'epoch': 0.02}
{'loss': 2.9821, 'grad_norm': 0.5160041339349409, 'learning_rate': 8.800483651631128e-05, 'epoch': 0.02}
{'loss': 2.9492, 'grad_norm': 0.5187952334427339, 'learning_rate': 8.803740507288132e-05, 'epoch': 0.02}
{'loss': 3.0053, 'grad_norm': 0.494625521034779, 'learning_rate': 8.806988353651037e-05, 'epoch': 0.02}
{'loss': 2.974, 'grad_norm': 0.5048190390335099, 'learning_rate': 8.810227240426389e-05, 'epoch': 0.02}
{'loss': 3.01, 'grad_norm': 0.49874686860520717, 'learning_rate': 8.813457216910499e-05, 'epoch': 0.02}
{'loss': 2.9637, 'grad_norm': 0.5039945282520818, 'learning_rate': 8.816678331993947e-05, 'epoch': 0.02}
{'loss': 3.0234, 'grad_norm': 0.5241144755881512, 'learning_rate': 8.81989063416602e-05, 'epoch': 0.02}
{'loss': 2.9695, 'grad_norm': 0.507608258201597, 'learning_rate': 8.823094171519092e-05, 'epoch': 0.02}
{'loss': 2.9617, 'grad_norm': 0.5084607574565359, 'learning_rate': 8.82628899175295e-05, 'epoch': 0.02}
{'loss': 2.9797, 'grad_norm': 0.5164384744088496, 'learning_rate': 8.829475142179045e-05, 'epoch': 0.02}
{'loss': 2.9772, 'grad_norm': 0.5214701335470784, 'learning_rate': 8.832652669724704e-05, 'epoch': 0.02}
{'loss': 2.9383, 'grad_norm': 0.5123998795308549, 'learning_rate': 8.835821620937276e-05, 'epoch': 0.02}
{'loss': 2.9849, 'grad_norm': 0.5091494296168211, 'learning_rate': 8.838982041988221e-05, 'epoch': 0.02}
{'loss': 2.9824, 'grad_norm': 0.5113316569791959, 'learning_rate': 8.842133978677145e-05, 'epoch': 0.02}
{'loss': 2.9586, 'grad_norm': 0.4997053176858187, 'learning_rate': 8.845277476435792e-05, 'epoch': 0.02}
{'loss': 3.0198, 'grad_norm': 0.5511694050310052, 'learning_rate': 8.848412580331955e-05, 'epoch': 0.02}
{'loss': 3.0036, 'grad_norm': 0.5112538615684664, 'learning_rate': 8.851539335073373e-05, 'epoch': 0.02}
{'loss': 2.9574, 'grad_norm': 0.522965758752773, 'learning_rate': 8.854657785011544e-05, 'epoch': 0.02}
{'loss': 2.9365, 'grad_norm': 0.5081801772991432, 'learning_rate': 8.857767974145503e-05, 'epoch': 0.02}
{'loss': 2.9712, 'grad_norm': 0.51790411132939, 'learning_rate': 8.860869946125547e-05, 'epoch': 0.02}
{'loss': 2.9675, 'grad_norm': 0.5269923127249077, 'learning_rate': 8.863963744256908e-05, 'epoch': 0.02}
{'loss': 2.9771, 'grad_norm': 0.5136529783453239, 'learning_rate': 8.867049411503387e-05, 'epoch': 0.02}
{'loss': 2.9135, 'grad_norm': 0.5269631346048261, 'learning_rate': 8.87012699049093e-05, 'epoch': 0.02}
{'loss': 2.9939, 'grad_norm': 0.5233341813569112, 'learning_rate': 8.873196523511157e-05, 'epoch': 0.02}
{'loss': 2.9673, 'grad_norm': 0.5092883312843428, 'learning_rate': 8.876258052524857e-05, 'epoch': 0.02}
{'loss': 2.9521, 'grad_norm': 0.5235480269420644, 'learning_rate': 8.879311619165424e-05, 'epoch': 0.02}
{'loss': 2.9352, 'grad_norm': 0.5057344364138403, 'learning_rate': 8.882357264742258e-05, 'epoch': 0.02}
{'loss': 2.9987, 'grad_norm': 0.523900844824135, 'learning_rate': 8.885395030244112e-05, 'epoch': 0.02}
{'loss': 2.9922, 'grad_norm': 0.5201507688480966, 'learning_rate': 8.88842495634241e-05, 'epoch': 0.02}
{'loss': 2.9898, 'grad_norm': 0.5228620309116411, 'learning_rate': 8.891447083394507e-05, 'epoch': 0.02}
{'loss': 2.9775, 'grad_norm': 0.5390546951878693, 'learning_rate': 8.894461451446924e-05, 'epoch': 0.02}
{'loss': 2.9571, 'grad_norm': 0.5287148960814393, 'learning_rate': 8.897468100238516e-05, 'epoch': 0.02}
{'loss': 2.9566, 'grad_norm': 0.5111281462350049, 'learning_rate': 8.900467069203634e-05, 'epoch': 0.02}
{'loss': 2.9723, 'grad_norm': 0.5305359504736936, 'learning_rate': 8.903458397475213e-05, 'epoch': 0.02}
{'loss': 2.9775, 'grad_norm': 0.5217731136868193, 'learning_rate': 8.906442123887845e-05, 'epoch': 0.02}
{'loss': 2.9936, 'grad_norm': 0.5198028498047217, 'learning_rate': 8.909418286980804e-05, 'epoch': 0.02}
{'loss': 3.0205, 'grad_norm': 0.5163038045348277, 'learning_rate': 8.912386925001022e-05, 'epoch': 0.02}
{'loss': 2.9529, 'grad_norm': 0.5196381746128411, 'learning_rate': 8.915348075906055e-05, 'epoch': 0.02}
{'loss': 2.9269, 'grad_norm': 0.5123727661036128, 'learning_rate': 8.918301777366981e-05, 'epoch': 0.02}
{'loss': 2.9719, 'grad_norm': 0.508026148612639, 'learning_rate': 8.921248066771283e-05, 'epoch': 0.02}
{'loss': 3.0086, 'grad_norm': 0.5122062064386251, 'learning_rate': 8.924186981225684e-05, 'epoch': 0.02}
[2024-04-12 11:35:28,456] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!
[2024-04-12 11:35:28,462] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 11:35:28,462] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 11:35:28,469] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 11:35:28,469] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 11:35:34,120] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 11:35:34,121] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 11:35:34,122] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
{'loss': 2.9156, 'grad_norm': 0.521101559970644, 'learning_rate': 8.927118557558958e-05, 'epoch': 0.02}
{'loss': 2.9553, 'grad_norm': 0.5087822630505194, 'learning_rate': 8.930042832324688e-05, 'epoch': 0.02}
{'loss': 2.9669, 'grad_norm': 0.5160776861816245, 'learning_rate': 8.932959841804015e-05, 'epoch': 0.02}
{'loss': 2.9403, 'grad_norm': 0.5105769976634553, 'learning_rate': 8.935869622008325e-05, 'epoch': 0.02}
{'loss': 2.9527, 'grad_norm': 0.5151525400720659, 'learning_rate': 8.938772208681927e-05, 'epoch': 0.02}
{'loss': 2.9143, 'grad_norm': 0.5118112312469547, 'learning_rate': 8.941667637304679e-05, 'epoch': 0.02}
{'loss': 2.9351, 'grad_norm': 0.5091818922648682, 'learning_rate': 8.944555943094597e-05, 'epoch': 0.02}
{'loss': 2.9726, 'grad_norm': 0.504988280144926, 'learning_rate': 8.947437161010425e-05, 'epoch': 0.02}
{'loss': 2.9018, 'grad_norm': 0.5075907699333677, 'learning_rate': 8.950311325754165e-05, 'epoch': 0.02}
{'loss': 2.9572, 'grad_norm': 0.5358316504562792, 'learning_rate': 8.953178471773599e-05, 'epoch': 0.02}
{'loss': 2.9546, 'grad_norm': 0.5205036698864769, 'learning_rate': 8.956038633264753e-05, 'epoch': 0.02}
{'loss': 2.95, 'grad_norm': 0.5145633668927733, 'learning_rate': 8.958891844174354e-05, 'epoch': 0.02}
{'loss': 2.9537, 'grad_norm': 0.5231277010348327, 'learning_rate': 8.961738138202236e-05, 'epoch': 0.02}
{'loss': 2.9508, 'grad_norm': 0.5410140080808581, 'learning_rate': 8.964577548803747e-05, 'epoch': 0.02}
{'loss': 2.8856, 'grad_norm': 0.5221802911953767, 'learning_rate': 8.967410109192087e-05, 'epoch': 0.02}
{'loss': 2.9428, 'grad_norm': 0.5250357881660442, 'learning_rate': 8.970235852340652e-05, 'epoch': 0.02}
{'loss': 2.9839, 'grad_norm': 0.5124990525335306, 'learning_rate': 8.973054810985336e-05, 'epoch': 0.02}
{'loss': 2.9058, 'grad_norm': 0.5248289965351541, 'learning_rate': 8.975867017626801e-05, 'epoch': 0.02}
{'loss': 2.9285, 'grad_norm': 0.5028173609056161, 'learning_rate': 8.978672504532734e-05, 'epoch': 0.02}
{'loss': 2.9167, 'grad_norm': 0.5128853837564901, 'learning_rate': 8.981471303740057e-05, 'epoch': 0.02}
{'loss': 2.9478, 'grad_norm': 0.5268371985549406, 'learning_rate': 8.984263447057135e-05, 'epoch': 0.02}
{'loss': 2.9658, 'grad_norm': 0.5123442116300436, 'learning_rate': 8.987048966065936e-05, 'epoch': 0.02}
{'loss': 2.9815, 'grad_norm': 0.5344545115640907, 'learning_rate': 8.98982789212417e-05, 'epoch': 0.02}
{'loss': 2.9586, 'grad_norm': 0.5178402240544326, 'learning_rate': 8.99260025636743e-05, 'epoch': 0.02}
{'loss': 2.9126, 'grad_norm': 0.5220874008704929, 'learning_rate': 8.99536608971125e-05, 'epoch': 0.02}
{'loss': 2.9304, 'grad_norm': 0.5027142772085703, 'learning_rate': 8.998125422853208e-05, 'epoch': 0.02}
{'loss': 2.9085, 'grad_norm': 0.5140275524228778, 'learning_rate': 9.000878286274949e-05, 'epoch': 0.02}
{'loss': 2.9117, 'grad_norm': 0.5162766155006342, 'learning_rate': 9.003624710244208e-05, 'epoch': 0.02}
{'loss': 2.9701, 'grad_norm': 0.5342103726866297, 'learning_rate': 9.006364724816817e-05, 'epoch': 0.02}
{'loss': 2.8847, 'grad_norm': 0.5060640487674104, 'learning_rate': 9.009098359838666e-05, 'epoch': 0.02}
{'loss': 2.9881, 'grad_norm': 0.5471532915660188, 'learning_rate': 9.011825644947662e-05, 'epoch': 0.02}
{'loss': 2.9832, 'grad_norm': 0.5347343379851599, 'learning_rate': 9.014546609575655e-05, 'epoch': 0.02}
{'loss': 2.9438, 'grad_norm': 0.5338927874476672, 'learning_rate': 9.017261282950343e-05, 'epoch': 0.02}
{'loss': 2.913, 'grad_norm': 0.5191751073648813, 'learning_rate': 9.01996969409715e-05, 'epoch': 0.02}
{'loss': 2.9495, 'grad_norm': 0.5169204046750169, 'learning_rate': 9.022671871841103e-05, 'epoch': 0.02}
{'loss': 2.9364, 'grad_norm': 0.5309383729690745, 'learning_rate': 9.025367844808657e-05, 'epoch': 0.02}
{'loss': 2.9046, 'grad_norm': 0.5287170738119231, 'learning_rate': 9.028057641429526e-05, 'epoch': 0.02}
{'loss': 2.9011, 'grad_norm': 0.5207458810821863, 'learning_rate': 9.030741289938474e-05, 'epoch': 0.02}
{'loss': 2.9196, 'grad_norm': 0.5338250039454789, 'learning_rate': 9.033418818377097e-05, 'epoch': 0.02}
{'loss': 2.9147, 'grad_norm': 0.5307963573616361, 'learning_rate': 9.036090254595577e-05, 'epoch': 0.02}
{'loss': 2.8744, 'grad_norm': 0.5186540210953786, 'learning_rate': 9.038755626254433e-05, 'epoch': 0.02}
{'loss': 2.949, 'grad_norm': 0.5362923097111045, 'learning_rate': 9.041414960826219e-05, 'epoch': 0.02}
{'loss': 2.9336, 'grad_norm': 0.5263913569003658, 'learning_rate': 9.044068285597243e-05, 'epoch': 0.02}
{'loss': 2.9164, 'grad_norm': 0.5391240125220232, 'learning_rate': 9.046715627669231e-05, 'epoch': 0.02}
{'loss': 2.921, 'grad_norm': 0.5237775695349927, 'learning_rate': 9.049357013960999e-05, 'epoch': 0.02}
{'loss': 2.9151, 'grad_norm': 0.521515921275573, 'learning_rate': 9.051992471210092e-05, 'epoch': 0.02}
{'loss': 2.9387, 'grad_norm': 0.5157325514847646, 'learning_rate': 9.054622025974403e-05, 'epoch': 0.02}
{'loss': 2.9141, 'grad_norm': 0.5245203624693486, 'learning_rate': 9.057245704633787e-05, 'epoch': 0.02}
{'loss': 2.9048, 'grad_norm': 0.5479402233866227, 'learning_rate': 9.059863533391642e-05, 'epoch': 0.02}
{'loss': 2.9606, 'grad_norm': 0.5264386630705321, 'learning_rate': 9.062475538276482e-05, 'epoch': 0.02}
{'loss': 2.9389, 'grad_norm': 0.5258657214954751, 'learning_rate': 9.065081745143493e-05, 'epoch': 0.02}
{'loss': 2.8882, 'grad_norm': 0.5573303934342515, 'learning_rate': 9.067682179676063e-05, 'epoch': 0.02}
{'loss': 2.9127, 'grad_norm': 0.5191675751173382, 'learning_rate': 9.070276867387303e-05, 'epoch': 0.02}
{'loss': 2.899, 'grad_norm': 0.52589855002447, 'learning_rate': 9.07286583362155e-05, 'epoch': 0.02}
{'loss': 2.9027, 'grad_norm': 0.52749793766046, 'learning_rate': 9.075449103555852e-05, 'epoch': 0.02}
{'loss': 2.9041, 'grad_norm': 0.5307511994253794, 'learning_rate': 9.078026702201434e-05, 'epoch': 0.02}
{'loss': 2.9444, 'grad_norm': 0.5354831832178184, 'learning_rate': 9.080598654405155e-05, 'epoch': 0.02}
{'loss': 2.9194, 'grad_norm': 0.532285501651938, 'learning_rate': 9.083164984850946e-05, 'epoch': 0.02}
{'loss': 2.9048, 'grad_norm': 0.5355271904844424, 'learning_rate': 9.085725718061223e-05, 'epoch': 0.02}
{'loss': 2.9465, 'grad_norm': 0.5374707794922882, 'learning_rate': 9.088280878398303e-05, 'epoch': 0.02}
{'loss': 2.892, 'grad_norm': 0.5292108835327376, 'learning_rate': 9.090830490065791e-05, 'epoch': 0.02}
{'loss': 2.9264, 'grad_norm': 0.5317861227638884, 'learning_rate': 9.093374577109952e-05, 'epoch': 0.02}
{'loss': 2.8203, 'grad_norm': 0.5275968409830352, 'learning_rate': 9.095913163421078e-05, 'epoch': 0.02}
{'loss': 2.8914, 'grad_norm': 0.5194515929038169, 'learning_rate': 9.098446272734832e-05, 'epoch': 0.02}
{'loss': 2.9024, 'grad_norm': 0.5309799915299998, 'learning_rate': 9.100973928633574e-05, 'epoch': 0.02}
{'loss': 2.9368, 'grad_norm': 0.528580572011083, 'learning_rate': 9.103496154547686e-05, 'epoch': 0.02}
{'loss': 2.8772, 'grad_norm': 0.5299183274195888, 'learning_rate': 9.106012973756875e-05, 'epoch': 0.02}
{'loss': 2.9545, 'grad_norm': 0.5943905464326297, 'learning_rate': 9.10852440939145e-05, 'epoch': 0.02}
{'loss': 2.9311, 'grad_norm': 0.5298726445269115, 'learning_rate': 9.111030484433618e-05, 'epoch': 0.02}
{'loss': 2.9139, 'grad_norm': 0.5275538263443742, 'learning_rate': 9.113531221718726e-05, 'epoch': 0.02}
{'loss': 2.939, 'grad_norm': 0.5310729159684163, 'learning_rate': 9.116026643936524e-05, 'epoch': 0.02}
{'loss': 2.94, 'grad_norm': 0.5478708302314846, 'learning_rate': 9.118516773632391e-05, 'epoch': 0.02}
{'loss': 2.9508, 'grad_norm': 0.518807786856851, 'learning_rate': 9.12100163320856e-05, 'epoch': 0.02}
{'loss': 2.9654, 'grad_norm': 0.5502200711836536, 'learning_rate': 9.12348124492533e-05, 'epoch': 0.02}
{'loss': 2.9277, 'grad_norm': 0.5227470467870388, 'learning_rate': 9.125955630902261e-05, 'epoch': 0.02}
{'loss': 2.8776, 'grad_norm': 0.5487762702408423, 'learning_rate': 9.128424813119353e-05, 'epoch': 0.02}
{'loss': 2.869, 'grad_norm': 0.5228803783479269, 'learning_rate': 9.130888813418227e-05, 'epoch': 0.02}
{'loss': 2.9219, 'grad_norm': 0.5373459133804726, 'learning_rate': 9.133347653503274e-05, 'epoch': 0.02}
{'loss': 2.8731, 'grad_norm': 0.5664032575399237, 'learning_rate': 9.135801354942812e-05, 'epoch': 0.02}
{'loss': 2.902, 'grad_norm': 0.5325985576241756, 'learning_rate': 9.13824993917021e-05, 'epoch': 0.02}
{'loss': 2.9109, 'grad_norm': 0.5558521576914037, 'learning_rate': 9.140693427485026e-05, 'epoch': 0.02}
{'loss': 2.8878, 'grad_norm': 0.550698643776704, 'learning_rate': 9.143131841054102e-05, 'epoch': 0.02}
{'loss': 2.9126, 'grad_norm': 0.5439773853132761, 'learning_rate': 9.145565200912677e-05, 'epoch': 0.02}
{'loss': 2.878, 'grad_norm': 0.5343821262494705, 'learning_rate': 9.14799352796547e-05, 'epoch': 0.02}
{'loss': 2.8989, 'grad_norm': 0.5743708047891705, 'learning_rate': 9.150416842987763e-05, 'epoch': 0.02}
{'loss': 2.9232, 'grad_norm': 0.5440548169829965, 'learning_rate': 9.152835166626453e-05, 'epoch': 0.02}
{'loss': 2.9017, 'grad_norm': 0.549065324867411, 'learning_rate': 9.15524851940113e-05, 'epoch': 0.02}
{'loss': 2.8616, 'grad_norm': 0.5364274264624076, 'learning_rate': 9.157656921705101e-05, 'epoch': 0.02}
{'loss': 2.8813, 'grad_norm': 0.5379765155183613, 'learning_rate': 9.160060393806437e-05, 'epoch': 0.02}
{'loss': 2.9148, 'grad_norm': 0.5373132338254294, 'learning_rate': 9.162458955848987e-05, 'epoch': 0.02}
{'loss': 2.859, 'grad_norm': 0.5655963223582805, 'learning_rate': 9.164852627853402e-05, 'epoch': 0.02}
{'loss': 2.8929, 'grad_norm': 0.5349593366084027, 'learning_rate': 9.167241429718126e-05, 'epoch': 0.02}
{'loss': 2.886, 'grad_norm': 0.5318854906371773, 'learning_rate': 9.169625381220399e-05, 'epoch': 0.02}
{'loss': 2.8587, 'grad_norm': 0.5263042343118097, 'learning_rate': 9.172004502017228e-05, 'epoch': 0.02}
{'loss': 2.8915, 'grad_norm': 0.5410433281505292, 'learning_rate': 9.174378811646375e-05, 'epoch': 0.02}
{'loss': 2.8509, 'grad_norm': 0.5271846289428779, 'learning_rate': 9.176748329527303e-05, 'epoch': 0.02}
{'loss': 2.8912, 'grad_norm': 0.5340297798605877, 'learning_rate': 9.179113074962138e-05, 'epoch': 0.02}
{'loss': 2.9375, 'grad_norm': 0.544394372607758, 'learning_rate': 9.181473067136614e-05, 'epoch': 0.02}
{'loss': 2.8669, 'grad_norm': 0.5426076698161076, 'learning_rate': 9.183828325121003e-05, 'epoch': 0.02}
{'loss': 2.8488, 'grad_norm': 0.5204354730820229, 'learning_rate': 9.186178867871037e-05, 'epoch': 0.02}
{'loss': 2.953, 'grad_norm': 0.5457396748372859, 'learning_rate': 9.188524714228833e-05, 'epoch': 0.02}
{'loss': 2.896, 'grad_norm': 0.5512013482893245, 'learning_rate': 9.19086588292379e-05, 'epoch': 0.02}
{'loss': 2.8511, 'grad_norm': 0.5440013100412998, 'learning_rate': 9.19320239257349e-05, 'epoch': 0.02}
{'loss': 2.8791, 'grad_norm': 0.5231822691840406, 'learning_rate': 9.195534261684585e-05, 'epoch': 0.02}
{'loss': 2.8771, 'grad_norm': 0.5537891950189732, 'learning_rate': 9.197861508653679e-05, 'epoch': 0.02}
{'loss': 2.8729, 'grad_norm': 0.5446914530120301, 'learning_rate': 9.200184151768197e-05, 'epoch': 0.02}
{'loss': 2.8895, 'grad_norm': 0.5464195419791865, 'learning_rate': 9.202502209207245e-05, 'epoch': 0.02}
{'loss': 2.7956, 'grad_norm': 0.5612686241577969, 'learning_rate': 9.20481569904247e-05, 'epoch': 0.02}
{'loss': 2.8536, 'grad_norm': 0.5447463957082266, 'learning_rate': 9.207124639238897e-05, 'epoch': 0.02}
{'loss': 2.9174, 'grad_norm': 0.5575864624106502, 'learning_rate': 9.209429047655778e-05, 'epoch': 0.02}
{'loss': 2.8604, 'grad_norm': 0.5372428505622988, 'learning_rate': 9.211728942047402e-05, 'epoch': 0.02}
{'loss': 2.8361, 'grad_norm': 0.5532028303564626, 'learning_rate': 9.214024340063939e-05, 'epoch': 0.02}
{'loss': 2.8594, 'grad_norm': 0.549002196440622, 'learning_rate': 9.216315259252232e-05, 'epoch': 0.02}
{'loss': 2.8505, 'grad_norm': 0.5390483992874291, 'learning_rate': 9.218601717056616e-05, 'epoch': 0.02}
{'loss': 2.845, 'grad_norm': 0.5378971127215496, 'learning_rate': 9.220883730819707e-05, 'epoch': 0.02}
{'loss': 2.8662, 'grad_norm': 0.5325982145509752, 'learning_rate': 9.223161317783193e-05, 'epoch': 0.02}
{'loss': 2.8733, 'grad_norm': 0.5511822218996435, 'learning_rate': 9.22543449508862e-05, 'epoch': 0.02}
{'loss': 2.9099, 'grad_norm': 0.537393638887599, 'learning_rate': 9.22770327977816e-05, 'epoch': 0.02}
{'loss': 2.8503, 'grad_norm': 0.5523232725304852, 'learning_rate': 9.229967688795381e-05, 'epoch': 0.02}
{'loss': 2.87, 'grad_norm': 0.5413791672398879, 'learning_rate': 9.232227738986005e-05, 'epoch': 0.02}
{'loss': 2.8815, 'grad_norm': 0.537096928484548, 'learning_rate': 9.234483447098662e-05, 'epoch': 0.02}
{'loss': 2.8607, 'grad_norm': 0.55569277524017, 'learning_rate': 9.236734829785629e-05, 'epoch': 0.02}
{'loss': 2.8652, 'grad_norm': 0.5490439395296233, 'learning_rate': 9.238981903603578e-05, 'epoch': 0.02}
{'loss': 2.8734, 'grad_norm': 0.5346933677343033, 'learning_rate': 9.241224685014296e-05, 'epoch': 0.02}
{'loss': 2.8731, 'grad_norm': 0.550434678028541, 'learning_rate': 9.24346319038541e-05, 'epoch': 0.02}
{'loss': 2.8674, 'grad_norm': 0.5516932661746661, 'learning_rate': 9.245697435991117e-05, 'epoch': 0.02}
{'loss': 2.8766, 'grad_norm': 0.5768919381428235, 'learning_rate': 9.24792743801287e-05, 'epoch': 0.02}
{'loss': 2.8674, 'grad_norm': 0.5520598330900541, 'learning_rate': 9.250153212540105e-05, 'epoch': 0.02}
{'loss': 2.8843, 'grad_norm': 0.5444758236037354, 'learning_rate': 9.252374775570922e-05, 'epoch': 0.02}
{'loss': 2.8408, 'grad_norm': 0.5417907041529892, 'learning_rate': 9.254592143012783e-05, 'epoch': 0.02}
{'loss': 2.8291, 'grad_norm': 0.5507950420671919, 'learning_rate': 9.256805330683189e-05, 'epoch': 0.02}
{'loss': 2.8264, 'grad_norm': 0.5542682067096496, 'learning_rate': 9.259014354310365e-05, 'epoch': 0.02}
{'loss': 2.8299, 'grad_norm': 0.5462198092551819, 'learning_rate': 9.261219229533921e-05, 'epoch': 0.02}
{'loss': 2.8336, 'grad_norm': 0.5584639169160244, 'learning_rate': 9.263419971905527e-05, 'epoch': 0.02}
{'loss': 2.868, 'grad_norm': 0.5726223331194241, 'learning_rate': 9.265616596889561e-05, 'epoch': 0.02}
{'loss': 2.9249, 'grad_norm': 0.5810307999968682, 'learning_rate': 9.267809119863771e-05, 'epoch': 0.02}
{'loss': 2.8693, 'grad_norm': 0.5595415515005759, 'learning_rate': 9.26999755611991e-05, 'epoch': 0.02}
{'loss': 2.8643, 'grad_norm': 0.550327903890799, 'learning_rate': 9.272181920864383e-05, 'epoch': 0.02}
{'loss': 2.8834, 'grad_norm': 0.5519971010034755, 'learning_rate': 9.27436222921888e-05, 'epoch': 0.02}
{'loss': 2.8338, 'grad_norm': 0.5412521205827768, 'learning_rate': 9.276538496221008e-05, 'epoch': 0.02}
{'loss': 2.8447, 'grad_norm': 0.556125512776236, 'learning_rate': 9.278710736824903e-05, 'epoch': 0.02}
{'loss': 2.8633, 'grad_norm': 0.5484721290924846, 'learning_rate': 9.280878965901856e-05, 'epoch': 0.02}
{'loss': 2.8209, 'grad_norm': 0.5528312021191486, 'learning_rate': 9.283043198240917e-05, 'epoch': 0.02}
{'loss': 2.8219, 'grad_norm': 0.5559524004506726, 'learning_rate': 9.285203448549506e-05, 'epoch': 0.02}
{'loss': 4.0408, 'grad_norm': 1.1584403698350703, 'learning_rate': 9.28735973145401e-05, 'epoch': 0.02}
{'loss': 4.2615, 'grad_norm': 0.912520188306031, 'learning_rate': 9.28951206150038e-05, 'epoch': 0.02}
{'loss': 4.3621, 'grad_norm': 0.8777267647540109, 'learning_rate': 9.29166045315471e-05, 'epoch': 0.02}
{'loss': 4.1412, 'grad_norm': 0.8984799840607363, 'learning_rate': 9.293804920803834e-05, 'epoch': 0.02}
{'loss': 4.2397, 'grad_norm': 1.0379808632008967, 'learning_rate': 9.2959454787559e-05, 'epoch': 0.02}
{'loss': 4.3003, 'grad_norm': 0.7852725599545224, 'learning_rate': 9.29808214124093e-05, 'epoch': 0.02}
{'loss': 4.0396, 'grad_norm': 1.0536044844902808, 'learning_rate': 9.300214922411409e-05, 'epoch': 0.02}
{'loss': 4.2616, 'grad_norm': 0.7756546095522427, 'learning_rate': 9.30234383634283e-05, 'epoch': 0.02}
{'loss': 3.9712, 'grad_norm': 1.2524497417720133, 'learning_rate': 9.30446889703426e-05, 'epoch': 0.02}
{'loss': 4.1867, 'grad_norm': 0.7031422040667142, 'learning_rate': 9.30659011840889e-05, 'epoch': 0.02}
{'loss': 4.1388, 'grad_norm': 0.7016688729307514, 'learning_rate': 9.308707514314584e-05, 'epoch': 0.02}
{'loss': 4.2224, 'grad_norm': 0.865506100521634, 'learning_rate': 9.310821098524417e-05, 'epoch': 0.02}
{'loss': 4.0949, 'grad_norm': 0.7667957474150235, 'learning_rate': 9.312930884737219e-05, 'epoch': 0.02}
{'loss': 4.2161, 'grad_norm': 1.0900091574854331, 'learning_rate': 9.3150368865781e-05, 'epoch': 0.02}
{'loss': 4.099, 'grad_norm': 0.6656931279960357, 'learning_rate': 9.317139117598988e-05, 'epoch': 0.02}
{'loss': 4.0937, 'grad_norm': 0.8428252459628083, 'learning_rate': 9.31923759127914e-05, 'epoch': 0.02}
{'loss': 4.1582, 'grad_norm': 0.6655482601909251, 'learning_rate': 9.321332321025671e-05, 'epoch': 0.02}
{'loss': 4.074, 'grad_norm': 0.632050525985303, 'learning_rate': 9.323423320174064e-05, 'epoch': 0.02}
{'loss': 4.0354, 'grad_norm': 1.3231466895834474, 'learning_rate': 9.325510601988679e-05, 'epoch': 0.02}
{'loss': 3.9745, 'grad_norm': 0.6697233571141796, 'learning_rate': 9.327594179663253e-05, 'epoch': 0.02}
{'loss': 4.0502, 'grad_norm': 0.6326739368516411, 'learning_rate': 9.329674066321416e-05, 'epoch': 0.02}
{'loss': 3.9896, 'grad_norm': 0.9982821064293789, 'learning_rate': 9.33175027501717e-05, 'epoch': 0.02}
{'loss': 3.8693, 'grad_norm': 0.65109638542022, 'learning_rate': 9.333822818735383e-05, 'epoch': 0.02}
{'loss': 4.0014, 'grad_norm': 0.6425487728311845, 'learning_rate': 9.335891710392291e-05, 'epoch': 0.02}
{'loss': 3.9778, 'grad_norm': 0.6297890372823941, 'learning_rate': 9.337956962835963e-05, 'epoch': 0.02}
{'loss': 3.9358, 'grad_norm': 0.7140683309051199, 'learning_rate': 9.340018588846787e-05, 'epoch': 0.02}
{'loss': 3.8317, 'grad_norm': 0.6600172880531389, 'learning_rate': 9.342076601137951e-05, 'epoch': 0.03}
{'loss': 4.0383, 'grad_norm': 0.6386506940265804, 'learning_rate': 9.344131012355898e-05, 'epoch': 0.03}
{'loss': 3.9105, 'grad_norm': 0.6244937650716058, 'learning_rate': 9.346181835080809e-05, 'epoch': 0.03}
{'loss': 3.8723, 'grad_norm': 0.6745272638796457, 'learning_rate': 9.348229081827054e-05, 'epoch': 0.03}
{'loss': 4.1729, 'grad_norm': 0.6280605426080161, 'learning_rate': 9.350272765043656e-05, 'epoch': 0.03}
{'loss': 3.9733, 'grad_norm': 0.6240540198006254, 'learning_rate': 9.352312897114738e-05, 'epoch': 0.03}
{'loss': 4.0574, 'grad_norm': 0.6845681709024455, 'learning_rate': 9.354349490359983e-05, 'epoch': 0.03}
{'loss': 4.0536, 'grad_norm': 0.6030997231785389, 'learning_rate': 9.356382557035074e-05, 'epoch': 0.03}
{'loss': 4.1073, 'grad_norm': 0.6638994656203054, 'learning_rate': 9.358412109332137e-05, 'epoch': 0.03}
{'loss': 3.8559, 'grad_norm': 0.6657488103002003, 'learning_rate': 9.360438159380185e-05, 'epoch': 0.03}
{'loss': 4.0407, 'grad_norm': 0.6449898081067588, 'learning_rate': 9.362460719245542e-05, 'epoch': 0.03}
{'loss': 3.8007, 'grad_norm': 1.1575700312194022, 'learning_rate': 9.364479800932289e-05, 'epoch': 0.03}
{'loss': 3.9049, 'grad_norm': 0.6798158491859118, 'learning_rate': 9.366495416382676e-05, 'epoch': 0.03}
{'loss': 3.8509, 'grad_norm': 0.6745053406516796, 'learning_rate': 9.368507577477557e-05, 'epoch': 0.03}
{'loss': 3.9335, 'grad_norm': 0.6447509520659326, 'learning_rate': 9.370516296036804e-05, 'epoch': 0.03}
{'loss': 3.9471, 'grad_norm': 0.6741945752147114, 'learning_rate': 9.372521583819729e-05, 'epoch': 0.03}
{'loss': 3.4977, 'grad_norm': 0.6925371926780576, 'learning_rate': 9.374523452525492e-05, 'epoch': 0.03}
{'loss': 4.1302, 'grad_norm': 0.6782239698284588, 'learning_rate': 9.376521913793514e-05, 'epoch': 0.03}
{'loss': 4.0172, 'grad_norm': 0.6476960831229389, 'learning_rate': 9.37851697920388e-05, 'epoch': 0.03}
{'loss': 3.6021, 'grad_norm': 0.6169048614420785, 'learning_rate': 9.380508660277744e-05, 'epoch': 0.03}
{'loss': 3.7532, 'grad_norm': 0.680698000817764, 'learning_rate': 9.382496968477727e-05, 'epoch': 0.03}
{'loss': 4.0512, 'grad_norm': 0.6098579633012379, 'learning_rate': 9.384481915208312e-05, 'epoch': 0.03}
{'loss': 4.0737, 'grad_norm': 2.20233406879324, 'learning_rate': 9.386463511816243e-05, 'epoch': 0.03}
{'loss': 3.8255, 'grad_norm': 0.7327090572579106, 'learning_rate': 9.388441769590903e-05, 'epoch': 0.03}
{'loss': 3.9073, 'grad_norm': 0.6676917101273686, 'learning_rate': 9.390416699764708e-05, 'epoch': 0.03}
{'loss': 3.6802, 'grad_norm': 0.6405107093377994, 'learning_rate': 9.392388313513486e-05, 'epoch': 0.03}
{'loss': 4.0665, 'grad_norm': 0.6140978008053142, 'learning_rate': 9.394356621956862e-05, 'epoch': 0.03}
{'loss': 4.0738, 'grad_norm': 0.6125283168250195, 'learning_rate': 9.396321636158625e-05, 'epoch': 0.03}
{'loss': 4.0941, 'grad_norm': 0.8376777408971473, 'learning_rate': 9.398283367127109e-05, 'epoch': 0.03}
{'loss': 4.1488, 'grad_norm': 0.7202682363654418, 'learning_rate': 9.400241825815563e-05, 'epoch': 0.03}
{'loss': 3.7279, 'grad_norm': 0.5944673117691979, 'learning_rate': 9.402197023122511e-05, 'epoch': 0.03}
{'loss': 3.9254, 'grad_norm': 0.6573247229863435, 'learning_rate': 9.404148969892124e-05, 'epoch': 0.03}
{'loss': 3.9925, 'grad_norm': 0.6648946276944253, 'learning_rate': 9.406097676914569e-05, 'epoch': 0.03}
{'loss': 4.0952, 'grad_norm': 0.6028639525171448, 'learning_rate': 9.408043154926386e-05, 'epoch': 0.03}
{'loss': 3.7945, 'grad_norm': 0.5901534298482918, 'learning_rate': 9.409985414610825e-05, 'epoch': 0.03}
{'loss': 3.9209, 'grad_norm': 0.6453924077964931, 'learning_rate': 9.411924466598205e-05, 'epoch': 0.03}
{'loss': 3.9467, 'grad_norm': 0.5873347204426299, 'learning_rate': 9.41386032146627e-05, 'epoch': 0.03}
{'loss': 3.8309, 'grad_norm': 0.6527945557415497, 'learning_rate': 9.415792989740516e-05, 'epoch': 0.03}
{'loss': 4.0834, 'grad_norm': 1.167028844323944, 'learning_rate': 9.417722481894558e-05, 'epoch': 0.03}
{'loss': 4.0591, 'grad_norm': 0.5859712448090434, 'learning_rate': 9.419648808350454e-05, 'epoch': 0.03}
{'loss': 3.8784, 'grad_norm': 0.6647557161868408, 'learning_rate': 9.421571979479048e-05, 'epoch': 0.03}
{'loss': 3.9967, 'grad_norm': 0.5957190455101107, 'learning_rate': 9.423492005600304e-05, 'epoch': 0.03}
{'loss': 3.9995, 'grad_norm': 0.6006036932440403, 'learning_rate': 9.425408896983643e-05, 'epoch': 0.03}
{'loss': 3.9614, 'grad_norm': 0.5741637000319907, 'learning_rate': 9.427322663848265e-05, 'epoch': 0.03}
{'loss': 3.9744, 'grad_norm': 0.5966528769955023, 'learning_rate': 9.429233316363481e-05, 'epoch': 0.03}
{'loss': 3.7707, 'grad_norm': 0.5740430880940175, 'learning_rate': 9.431140864649033e-05, 'epoch': 0.03}
{'loss': 3.9387, 'grad_norm': 0.5854891588796994, 'learning_rate': 9.433045318775428e-05, 'epoch': 0.03}
{'loss': 3.8746, 'grad_norm': 0.5880583569380524, 'learning_rate': 9.434946688764234e-05, 'epoch': 0.03}
{'loss': 3.8939, 'grad_norm': 0.6149058593317359, 'learning_rate': 9.43684498458842e-05, 'epoch': 0.03}
{'loss': 4.0261, 'grad_norm': 0.6206647680471437, 'learning_rate': 9.438740216172656e-05, 'epoch': 0.03}
{'loss': 3.9976, 'grad_norm': 0.6280618895570779, 'learning_rate': 9.440632393393627e-05, 'epoch': 0.03}
{'loss': 4.0457, 'grad_norm': 0.5996806762106538, 'learning_rate': 9.442521526080346e-05, 'epoch': 0.03}
{'loss': 3.4557, 'grad_norm': 0.5701705551390783, 'learning_rate': 9.444407624014455e-05, 'epoch': 0.03}
{'loss': 3.8493, 'grad_norm': 0.7198591539432636, 'learning_rate': 9.446290696930533e-05, 'epoch': 0.03}
{'loss': 2.9616, 'grad_norm': 0.8212577831404364, 'learning_rate': 9.44817075451639e-05, 'epoch': 0.03}
{'loss': 4.0579, 'grad_norm': 0.5960997607181857, 'learning_rate': 9.450047806413379e-05, 'epoch': 0.03}
{'loss': 3.7962, 'grad_norm': 0.5866588865423671, 'learning_rate': 9.451921862216682e-05, 'epoch': 0.03}
{'loss': 3.8587, 'grad_norm': 0.621086560131869, 'learning_rate': 9.453792931475607e-05, 'epoch': 0.03}
{'loss': 3.8601, 'grad_norm': 0.7096366794727068, 'learning_rate': 9.455661023693879e-05, 'epoch': 0.03}
{'loss': 3.9401, 'grad_norm': 0.9017919946550048, 'learning_rate': 9.457526148329938e-05, 'epoch': 0.03}
{'loss': 4.03, 'grad_norm': 0.6051293886177442, 'learning_rate': 9.459388314797212e-05, 'epoch': 0.03}
{'loss': 3.8677, 'grad_norm': 0.6029690558484896, 'learning_rate': 9.461247532464413e-05, 'epoch': 0.03}
{'loss': 3.9239, 'grad_norm': 0.6218248476522503, 'learning_rate': 9.463103810655815e-05, 'epoch': 0.03}
{'loss': 3.7722, 'grad_norm': 0.8712968139543732, 'learning_rate': 9.464957158651539e-05, 'epoch': 0.03}
{'loss': 3.8853, 'grad_norm': 0.5915027650466752, 'learning_rate': 9.466807585687823e-05, 'epoch': 0.03}
{'loss': 3.8862, 'grad_norm': 0.6154296412321235, 'learning_rate': 9.468655100957309e-05, 'epoch': 0.03}
{'loss': 3.7837, 'grad_norm': 0.5563472231962987, 'learning_rate': 9.47049971360931e-05, 'epoch': 0.03}
{'loss': 3.8965, 'grad_norm': 0.6084168341178746, 'learning_rate': 9.472341432750078e-05, 'epoch': 0.03}
{'loss': 3.761, 'grad_norm': 0.6399519483639892, 'learning_rate': 9.47418026744309e-05, 'epoch': 0.03}
{'loss': 3.9689, 'grad_norm': 0.6960583047495488, 'learning_rate': 9.476016226709293e-05, 'epoch': 0.03}
{'loss': 3.8818, 'grad_norm': 0.6018101109251711, 'learning_rate': 9.477849319527389e-05, 'epoch': 0.03}
{'loss': 3.8963, 'grad_norm': 0.7858554895710695, 'learning_rate': 9.479679554834087e-05, 'epoch': 0.03}
{'loss': 3.9134, 'grad_norm': 0.6343234290715964, 'learning_rate': 9.481506941524376e-05, 'epoch': 0.03}
{'loss': 3.8148, 'grad_norm': 0.6238006114667204, 'learning_rate': 9.483331488451758e-05, 'epoch': 0.03}
{'loss': 3.8831, 'grad_norm': 0.5984948074560096, 'learning_rate': 9.485153204428545e-05, 'epoch': 0.03}
{'loss': 3.9861, 'grad_norm': 0.5997491780649384, 'learning_rate': 9.486972098226083e-05, 'epoch': 0.03}
{'loss': 3.8589, 'grad_norm': 0.6160463320672237, 'learning_rate': 9.488788178575016e-05, 'epoch': 0.03}
{'loss': 3.6101, 'grad_norm': 0.6924568282146063, 'learning_rate': 9.490601454165536e-05, 'epoch': 0.03}
{'loss': 4.0243, 'grad_norm': 0.6226718926236557, 'learning_rate': 9.492411933647638e-05, 'epoch': 0.03}
{'loss': 3.9322, 'grad_norm': 0.6057492381818429, 'learning_rate': 9.494219625631358e-05, 'epoch': 0.03}
{'loss': 3.7899, 'grad_norm': 0.5940068838581156, 'learning_rate': 9.49602453868703e-05, 'epoch': 0.03}
{'loss': 4.0214, 'grad_norm': 0.6212217811820651, 'learning_rate': 9.497826681345518e-05, 'epoch': 0.03}
{'loss': 3.7402, 'grad_norm': 0.5971702697098947, 'learning_rate': 9.499626062098467e-05, 'epoch': 0.03}
{'loss': 3.8748, 'grad_norm': 0.6168646731419999, 'learning_rate': 9.501422689398538e-05, 'epoch': 0.03}
{'loss': 3.8922, 'grad_norm': 0.5904958225479905, 'learning_rate': 9.503216571659649e-05, 'epoch': 0.03}
{'loss': 3.8955, 'grad_norm': 0.5639729799720946, 'learning_rate': 9.505007717257209e-05, 'epoch': 0.03}
{'loss': 3.5585, 'grad_norm': 0.6620696080473149, 'learning_rate': 9.506796134528355e-05, 'epoch': 0.03}
{'loss': 3.8732, 'grad_norm': 0.5955336946490117, 'learning_rate': 9.508581831772183e-05, 'epoch': 0.03}
{'loss': 3.7988, 'grad_norm': 0.6176835242641796, 'learning_rate': 9.51036481724998e-05, 'epoch': 0.03}
{'loss': 3.8379, 'grad_norm': 0.5812832758021147, 'learning_rate': 9.512145099185458e-05, 'epoch': 0.03}
{'loss': 3.8707, 'grad_norm': 0.724666214672299, 'learning_rate': 9.513922685764966e-05, 'epoch': 0.03}
{'loss': 3.9297, 'grad_norm': 0.5929952410942637, 'learning_rate': 9.515697585137739e-05, 'epoch': 0.03}
{'loss': 3.8062, 'grad_norm': 0.6123930186271502, 'learning_rate': 9.517469805416098e-05, 'epoch': 0.03}
{'loss': 4.0111, 'grad_norm': 0.6116636910351912, 'learning_rate': 9.519239354675697e-05, 'epoch': 0.03}
{'loss': 3.995, 'grad_norm': 0.5867118984964815, 'learning_rate': 9.521006240955718e-05, 'epoch': 0.03}
{'loss': 3.9317, 'grad_norm': 0.5730459869230643, 'learning_rate': 9.52277047225911e-05, 'epoch': 0.03}
{'loss': 3.6479, 'grad_norm': 0.6279501461149654, 'learning_rate': 9.524532056552803e-05, 'epoch': 0.03}
{'loss': 3.421, 'grad_norm': 0.6724681883971961, 'learning_rate': 9.526291001767917e-05, 'epoch': 0.03}
{'loss': 3.9734, 'grad_norm': 0.5772788197830154, 'learning_rate': 9.528047315799973e-05, 'epoch': 0.03}
{'loss': 3.7565, 'grad_norm': 1.4976709521452654, 'learning_rate': 9.529801006509122e-05, 'epoch': 0.03}
{'loss': 3.7388, 'grad_norm': 0.5968916932226526, 'learning_rate': 9.531552081720348e-05, 'epoch': 0.03}
{'loss': 3.8925, 'grad_norm': 0.5647466289118693, 'learning_rate': 9.533300549223666e-05, 'epoch': 0.03}
{'loss': 3.786, 'grad_norm': 0.5752343847066639, 'learning_rate': 9.53504641677435e-05, 'epoch': 0.03}
{'loss': 3.8875, 'grad_norm': 0.594477484600987, 'learning_rate': 9.536789692093117e-05, 'epoch': 0.03}
{'loss': 3.918, 'grad_norm': 0.6043281549980941, 'learning_rate': 9.538530382866361e-05, 'epoch': 0.03}
{'loss': 3.8684, 'grad_norm': 0.6382869021814254, 'learning_rate': 9.540268496746328e-05, 'epoch': 0.03}
{'loss': 3.6933, 'grad_norm': 0.5748874095609043, 'learning_rate': 9.542004041351328e-05, 'epoch': 0.03}
{'loss': 3.7352, 'grad_norm': 0.5750746800164639, 'learning_rate': 9.543737024265944e-05, 'epoch': 0.03}
{'loss': 3.4924, 'grad_norm': 0.6290812277191431, 'learning_rate': 9.545467453041218e-05, 'epoch': 0.03}
{'loss': 3.8899, 'grad_norm': 0.6154651559874458, 'learning_rate': 9.54719533519486e-05, 'epoch': 0.03}
{'loss': 3.8451, 'grad_norm': 0.6579164936469822, 'learning_rate': 9.54892067821143e-05, 'epoch': 0.03}
{'loss': 3.747, 'grad_norm': 0.6257969688312212, 'learning_rate': 9.55064348954255e-05, 'epoch': 0.03}
{'loss': 3.8591, 'grad_norm': 0.5929439387373502, 'learning_rate': 9.552363776607079e-05, 'epoch': 0.03}
{'loss': 3.8422, 'grad_norm': 0.5702829691995752, 'learning_rate': 9.554081546791315e-05, 'epoch': 0.03}
{'loss': 3.5397, 'grad_norm': 0.830103825249926, 'learning_rate': 9.555796807449189e-05, 'epoch': 0.03}
{'loss': 3.9, 'grad_norm': 0.5760917245875635, 'learning_rate': 9.557509565902442e-05, 'epoch': 0.03}
{'loss': 3.6544, 'grad_norm': 0.5951148921908226, 'learning_rate': 9.559219829440827e-05, 'epoch': 0.03}
{'loss': 3.9705, 'grad_norm': 0.5741853417696943, 'learning_rate': 9.560927605322276e-05, 'epoch': 0.03}
{'loss': 3.7257, 'grad_norm': 0.5567564882807657, 'learning_rate': 9.562632900773103e-05, 'epoch': 0.03}
{'loss': 3.9656, 'grad_norm': 0.5586659216112208, 'learning_rate': 9.564335722988183e-05, 'epoch': 0.03}
{'loss': 3.6933, 'grad_norm': 0.5796606173181195, 'learning_rate': 9.566036079131124e-05, 'epoch': 0.03}
{'loss': 3.8951, 'grad_norm': 0.5855733320999209, 'learning_rate': 9.567733976334464e-05, 'epoch': 0.03}
{'loss': 3.9963, 'grad_norm': 0.6200637393169378, 'learning_rate': 9.569429421699832e-05, 'epoch': 0.03}
{'loss': 3.7912, 'grad_norm': 0.6081817708943714, 'learning_rate': 9.571122422298146e-05, 'epoch': 0.03}
{'loss': 3.8498, 'grad_norm': 0.5835345309882553, 'learning_rate': 9.572812985169772e-05, 'epoch': 0.03}
{'loss': 3.781, 'grad_norm': 0.5661878517002676, 'learning_rate': 9.574501117324711e-05, 'epoch': 0.03}
{'loss': 3.6899, 'grad_norm': 0.569777234456479, 'learning_rate': 9.576186825742774e-05, 'epoch': 0.03}
{'loss': 3.8402, 'grad_norm': 0.6576189674397266, 'learning_rate': 9.577870117373746e-05, 'epoch': 0.03}
{'loss': 3.7691, 'grad_norm': 0.6234440369036816, 'learning_rate': 9.579550999137567e-05, 'epoch': 0.03}
{'loss': 3.7151, 'grad_norm': 0.60106408699504, 'learning_rate': 9.581229477924493e-05, 'epoch': 0.03}
{'loss': 3.9256, 'grad_norm': 0.5891823157310934, 'learning_rate': 9.582905560595276e-05, 'epoch': 0.03}
{'loss': 3.7517, 'grad_norm': 0.7122362657530668, 'learning_rate': 9.584579253981332e-05, 'epoch': 0.03}
{'loss': 3.9022, 'grad_norm': 0.5833549970351484, 'learning_rate': 9.586250564884891e-05, 'epoch': 0.03}
{'loss': 3.7826, 'grad_norm': 0.59694467248985, 'learning_rate': 9.587919500079186e-05, 'epoch': 0.03}
{'loss': 3.6631, 'grad_norm': 0.848003746467836, 'learning_rate': 9.589586066308606e-05, 'epoch': 0.03}
{'loss': 3.6641, 'grad_norm': 0.9905464839509492, 'learning_rate': 9.59125027028886e-05, 'epoch': 0.03}
{'loss': 3.5968, 'grad_norm': 0.5842792922647232, 'learning_rate': 9.592912118707136e-05, 'epoch': 0.03}
{'loss': 3.9305, 'grad_norm': 0.7568624676748748, 'learning_rate': 9.594571618222272e-05, 'epoch': 0.03}
{'loss': 3.7158, 'grad_norm': 1.1229748205139085, 'learning_rate': 9.596228775464906e-05, 'epoch': 0.03}
{'loss': 3.6453, 'grad_norm': 0.5777205491806598, 'learning_rate': 9.597883597037644e-05, 'epoch': 0.03}
{'loss': 3.7258, 'grad_norm': 0.6311136894417673, 'learning_rate': 9.599536089515212e-05, 'epoch': 0.03}
{'loss': 3.8654, 'grad_norm': 0.5837773796805475, 'learning_rate': 9.601186259444608e-05, 'epoch': 0.03}
{'loss': 3.5781, 'grad_norm': 0.6089466454474475, 'learning_rate': 9.602834113345275e-05, 'epoch': 0.03}
{'loss': 3.9294, 'grad_norm': 0.6727076902089856, 'learning_rate': 9.604479657709234e-05, 'epoch': 0.03}
{'loss': 3.8901, 'grad_norm': 0.5980192607231511, 'learning_rate': 9.606122899001251e-05, 'epoch': 0.03}
{'loss': 3.7149, 'grad_norm': 0.645687356245443, 'learning_rate': 9.60776384365899e-05, 'epoch': 0.03}
{'loss': 3.7793, 'grad_norm': 0.6127154068363178, 'learning_rate': 9.609402498093155e-05, 'epoch': 0.03}
{'loss': 3.826, 'grad_norm': 0.5812064795943102, 'learning_rate': 9.611038868687647e-05, 'epoch': 0.03}
{'loss': 3.6952, 'grad_norm': 0.6971270145179602, 'learning_rate': 9.612672961799713e-05, 'epoch': 0.03}
{'loss': 3.7072, 'grad_norm': 0.5939838697299127, 'learning_rate': 9.614304783760091e-05, 'epoch': 0.03}
{'loss': 3.8215, 'grad_norm': 0.6078849061254068, 'learning_rate': 9.615934340873163e-05, 'epoch': 0.03}
{'loss': 3.845, 'grad_norm': 0.5560952322637551, 'learning_rate': 9.617561639417095e-05, 'epoch': 0.03}
{'loss': 3.9478, 'grad_norm': 0.594194560366768, 'learning_rate': 9.619186685643982e-05, 'epoch': 0.03}
{'loss': 3.7422, 'grad_norm': 0.6655772066549337, 'learning_rate': 9.62080948578e-05, 'epoch': 0.03}
{'loss': 3.9456, 'grad_norm': 0.5803823803237966, 'learning_rate': 9.622430046025538e-05, 'epoch': 0.03}
{'loss': 3.8489, 'grad_norm': 0.630791724210968, 'learning_rate': 9.624048372555352e-05, 'epoch': 0.03}
{'loss': 3.8903, 'grad_norm': 0.5697236795144962, 'learning_rate': 9.625664471518695e-05, 'epoch': 0.03}
{'loss': 3.886, 'grad_norm': 0.5876053421145804, 'learning_rate': 9.627278349039461e-05, 'epoch': 0.03}
{'loss': 3.9086, 'grad_norm': 0.6031726704923284, 'learning_rate': 9.628890011216334e-05, 'epoch': 0.03}
{'loss': 3.8343, 'grad_norm': 0.5698495983531318, 'learning_rate': 9.63049946412291e-05, 'epoch': 0.03}
{'loss': 3.7727, 'grad_norm': 0.6554585240067498, 'learning_rate': 9.632106713807842e-05, 'epoch': 0.03}
{'loss': 3.7408, 'grad_norm': 0.5702421491432559, 'learning_rate': 9.633711766294981e-05, 'epoch': 0.03}
{'loss': 3.926, 'grad_norm': 0.6003193698550332, 'learning_rate': 9.635314627583506e-05, 'epoch': 0.03}
{'loss': 3.8227, 'grad_norm': 0.5837487602129472, 'learning_rate': 9.636915303648055e-05, 'epoch': 0.03}
{'loss': 3.6329, 'grad_norm': 0.584869798067039, 'learning_rate': 9.638513800438867e-05, 'epoch': 0.03}
{'loss': 3.8503, 'grad_norm': 0.5940628911071032, 'learning_rate': 9.640110123881911e-05, 'epoch': 0.03}
{'loss': 3.6253, 'grad_norm': 0.5805968397739898, 'learning_rate': 9.641704279879017e-05, 'epoch': 0.03}
{'loss': 3.9647, 'grad_norm': 0.5853948417724091, 'learning_rate': 9.643296274308006e-05, 'epoch': 0.03}
{'loss': 3.7997, 'grad_norm': 0.6030875702269712, 'learning_rate': 9.644886113022825e-05, 'epoch': 0.03}
{'loss': 3.3466, 'grad_norm': 0.7826862784762217, 'learning_rate': 9.646473801853665e-05, 'epoch': 0.03}
{'loss': 3.7492, 'grad_norm': 0.6710238946527954, 'learning_rate': 9.648059346607108e-05, 'epoch': 0.03}
{'loss': 3.8943, 'grad_norm': 0.577334806132553, 'learning_rate': 9.649642753066238e-05, 'epoch': 0.03}
{'loss': 3.9563, 'grad_norm': 0.5684011168613703, 'learning_rate': 9.651224026990769e-05, 'epoch': 0.03}
{'loss': 3.7276, 'grad_norm': 0.6142287692348066, 'learning_rate': 9.652803174117183e-05, 'epoch': 0.03}
{'loss': 3.5411, 'grad_norm': 0.5787102109974485, 'learning_rate': 9.654380200158838e-05, 'epoch': 0.03}
{'loss': 3.8138, 'grad_norm': 0.5786220475971986, 'learning_rate': 9.655955110806109e-05, 'epoch': 0.03}
{'loss': 3.6849, 'grad_norm': 0.5699025990141118, 'learning_rate': 9.657527911726495e-05, 'epoch': 0.03}
{'loss': 3.8282, 'grad_norm': 0.5932587657151663, 'learning_rate': 9.659098608564754e-05, 'epoch': 0.03}
{'loss': 3.8438, 'grad_norm': 0.5998521060068134, 'learning_rate': 9.660667206943018e-05, 'epoch': 0.03}
{'loss': 3.7339, 'grad_norm': 3.183225730559816, 'learning_rate': 9.662233712460918e-05, 'epoch': 0.03}
{'loss': 3.755, 'grad_norm': 0.5697418722423644, 'learning_rate': 9.663798130695696e-05, 'epoch': 0.03}
{'loss': 3.8957, 'grad_norm': 0.5547057258638489, 'learning_rate': 9.665360467202336e-05, 'epoch': 0.03}
{'loss': 3.758, 'grad_norm': 0.5846390844837824, 'learning_rate': 9.66692072751367e-05, 'epoch': 0.03}
{'loss': 3.8169, 'grad_norm': 0.6294401449484351, 'learning_rate': 9.668478917140505e-05, 'epoch': 0.03}
{'loss': 3.6509, 'grad_norm': 0.6101155896367245, 'learning_rate': 9.670035041571738e-05, 'epoch': 0.03}
{'loss': 3.7495, 'grad_norm': 0.5460471232016311, 'learning_rate': 9.671589106274465e-05, 'epoch': 0.03}
{'loss': 3.9112, 'grad_norm': 0.6052987614193189, 'learning_rate': 9.673141116694105e-05, 'epoch': 0.03}
{'loss': 3.5887, 'grad_norm': 0.5487677527428527, 'learning_rate': 9.674691078254508e-05, 'epoch': 0.03}
{'loss': 3.8434, 'grad_norm': 0.5833574418040912, 'learning_rate': 9.676238996358077e-05, 'epoch': 0.03}
{'loss': 3.7431, 'grad_norm': 0.5703235485811302, 'learning_rate': 9.67778487638587e-05, 'epoch': 0.03}
{'loss': 3.5222, 'grad_norm': 0.8021401805182432, 'learning_rate': 9.679328723697721e-05, 'epoch': 0.03}
{'loss': 3.7237, 'grad_norm': 0.5955027036964616, 'learning_rate': 9.68087054363235e-05, 'epoch': 0.03}
{'loss': 3.763, 'grad_norm': 0.5631217792836825, 'learning_rate': 9.682410341507466e-05, 'epoch': 0.03}
{'loss': 3.6668, 'grad_norm': 0.5844494492353094, 'learning_rate': 9.683948122619892e-05, 'epoch': 0.03}
{'loss': 3.6761, 'grad_norm': 0.577896393914808, 'learning_rate': 9.685483892245659e-05, 'epoch': 0.03}
{'loss': 3.7026, 'grad_norm': 0.5996434155154141, 'learning_rate': 9.68701765564012e-05, 'epoch': 0.03}
{'loss': 3.8501, 'grad_norm': 0.5568829528990601, 'learning_rate': 9.688549418038065e-05, 'epoch': 0.03}
{'loss': 3.7973, 'grad_norm': 0.639837581694358, 'learning_rate': 9.690079184653818e-05, 'epoch': 0.03}
{'loss': 3.8431, 'grad_norm': 0.6305366329131603, 'learning_rate': 9.691606960681354e-05, 'epoch': 0.03}
{'loss': 3.8581, 'grad_norm': 0.6105541498917952, 'learning_rate': 9.693132751294389e-05, 'epoch': 0.03}
{'loss': 3.6998, 'grad_norm': 0.5979876231170687, 'learning_rate': 9.694656561646496e-05, 'epoch': 0.03}
{'loss': 3.6656, 'grad_norm': 0.5617088060903183, 'learning_rate': 9.69617839687122e-05, 'epoch': 0.03}
{'loss': 3.6666, 'grad_norm': 0.5806209855316831, 'learning_rate': 9.697698262082157e-05, 'epoch': 0.03}
{'loss': 3.8908, 'grad_norm': 0.6016496374881793, 'learning_rate': 9.699216162373072e-05, 'epoch': 0.03}
{'loss': 3.7028, 'grad_norm': 0.5740204997753823, 'learning_rate': 9.70073210281801e-05, 'epoch': 0.03}
{'loss': 3.83, 'grad_norm': 0.5622934755445236, 'learning_rate': 9.70224608847137e-05, 'epoch': 0.03}
{'loss': 3.91, 'grad_norm': 0.5701845166997367, 'learning_rate': 9.703758124368041e-05, 'epoch': 0.03}
{'loss': 3.6398, 'grad_norm': 0.5832006375359531, 'learning_rate': 9.70526821552347e-05, 'epoch': 0.03}
{'loss': 3.7523, 'grad_norm': 0.5947821008500311, 'learning_rate': 9.706776366933785e-05, 'epoch': 0.03}
{'loss': 3.9004, 'grad_norm': 0.5729091049921219, 'learning_rate': 9.708282583575884e-05, 'epoch': 0.03}
{'loss': 3.4806, 'grad_norm': 0.5666585109497045, 'learning_rate': 9.709786870407539e-05, 'epoch': 0.03}
{'loss': 3.5794, 'grad_norm': 0.5602746628636406, 'learning_rate': 9.711289232367479e-05, 'epoch': 0.03}
{'loss': 3.7892, 'grad_norm': 0.5951104969113743, 'learning_rate': 9.712789674375509e-05, 'epoch': 0.03}
{'loss': 3.6417, 'grad_norm': 0.6618792613995631, 'learning_rate': 9.714288201332597e-05, 'epoch': 0.03}
{'loss': 3.7221, 'grad_norm': 0.680923108590277, 'learning_rate': 9.715784818120958e-05, 'epoch': 0.03}
{'loss': 3.4282, 'grad_norm': 0.607756488813808, 'learning_rate': 9.717279529604176e-05, 'epoch': 0.03}
{'loss': 3.7638, 'grad_norm': 0.5936300940793421, 'learning_rate': 9.718772340627269e-05, 'epoch': 0.03}
{'loss': 3.7708, 'grad_norm': 0.6499844250290477, 'learning_rate': 9.72026325601681e-05, 'epoch': 0.03}
{'loss': 3.8184, 'grad_norm': 0.5959259506733978, 'learning_rate': 9.721752280580996e-05, 'epoch': 0.03}
{'loss': 3.7138, 'grad_norm': 0.5788866764680572, 'learning_rate': 9.723239419109766e-05, 'epoch': 0.03}
{'loss': 3.3278, 'grad_norm': 0.5702331651205395, 'learning_rate': 9.724724676374872e-05, 'epoch': 0.03}
{'loss': 3.7489, 'grad_norm': 0.5707449039147979, 'learning_rate': 9.726208057129985e-05, 'epoch': 0.03}
{'loss': 3.7278, 'grad_norm': 0.6097455217138135, 'learning_rate': 9.727689566110776e-05, 'epoch': 0.03}
{'loss': 3.8191, 'grad_norm': 0.6431362031680754, 'learning_rate': 9.729169208035017e-05, 'epoch': 0.03}
{'loss': 3.9516, 'grad_norm': 0.5578403488552713, 'learning_rate': 9.730646987602663e-05, 'epoch': 0.03}
{'loss': 3.6618, 'grad_norm': 0.5635678003495173, 'learning_rate': 9.732122909495942e-05, 'epoch': 0.03}
{'loss': 3.3468, 'grad_norm': 0.8192228274416052, 'learning_rate': 9.733596978379453e-05, 'epoch': 0.03}
{'loss': 3.6449, 'grad_norm': 0.5796962231900318, 'learning_rate': 9.735069198900246e-05, 'epoch': 0.03}
{'loss': 3.661, 'grad_norm': 1.0945573140704035, 'learning_rate': 9.736539575687902e-05, 'epoch': 0.03}
{'loss': 3.6913, 'grad_norm': 0.6253823483982346, 'learning_rate': 9.738008113354646e-05, 'epoch': 0.04}
[2024-04-12 12:55:17,179] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4000 is about to be saved!
[2024-04-12 12:55:17,184] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-4000/global_step4000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 12:55:17,184] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4000/global_step4000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 12:55:17,191] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4000/global_step4000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 12:55:17,191] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-4000/global_step4000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 12:55:21,132] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-4000/global_step4000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 12:55:21,132] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-4000/global_step4000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 12:55:21,133] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
{'loss': 3.7629, 'grad_norm': 0.566753925777784, 'learning_rate': 9.739474816495406e-05, 'epoch': 0.04}
{'loss': 3.4491, 'grad_norm': 0.5693041931288974, 'learning_rate': 9.740939689687919e-05, 'epoch': 0.04}
{'loss': 3.2846, 'grad_norm': 0.6038930820683677, 'learning_rate': 9.742402737492802e-05, 'epoch': 0.04}
{'loss': 3.7329, 'grad_norm': 0.5747474320908391, 'learning_rate': 9.74386396445365e-05, 'epoch': 0.04}
{'loss': 3.8242, 'grad_norm': 0.6287998385327757, 'learning_rate': 9.745323375097112e-05, 'epoch': 0.04}
{'loss': 3.6868, 'grad_norm': 0.5743433854821235, 'learning_rate': 9.746780973932978e-05, 'epoch': 0.04}
{'loss': 3.817, 'grad_norm': 0.7079789350299476, 'learning_rate': 9.748236765454262e-05, 'epoch': 0.04}
{'loss': 3.7377, 'grad_norm': 0.5977714122832009, 'learning_rate': 9.749690754137288e-05, 'epoch': 0.04}
{'loss': 3.7157, 'grad_norm': 0.5681897854394681, 'learning_rate': 9.751142944441768e-05, 'epoch': 0.04}
{'loss': 3.8186, 'grad_norm': 0.5605903132770479, 'learning_rate': 9.752593340810889e-05, 'epoch': 0.04}
{'loss': 3.6444, 'grad_norm': 0.5912106112471515, 'learning_rate': 9.754041947671388e-05, 'epoch': 0.04}
{'loss': 3.6831, 'grad_norm': 0.6101305440447219, 'learning_rate': 9.755488769433642e-05, 'epoch': 0.04}
{'loss': 3.7079, 'grad_norm': 0.5854535024480467, 'learning_rate': 9.756933810491737e-05, 'epoch': 0.04}
{'loss': 3.8961, 'grad_norm': 0.5734838818198846, 'learning_rate': 9.75837707522356e-05, 'epoch': 0.04}
{'loss': 3.816, 'grad_norm': 0.5641172963097948, 'learning_rate': 9.75981856799087e-05, 'epoch': 0.04}
{'loss': 3.7877, 'grad_norm': 0.5904380053313181, 'learning_rate': 9.761258293139387e-05, 'epoch': 0.04}
{'loss': 3.7858, 'grad_norm': 0.5657281241893518, 'learning_rate': 9.762696254998853e-05, 'epoch': 0.04}
{'loss': 3.6322, 'grad_norm': 0.6534179556791305, 'learning_rate': 9.764132457883128e-05, 'epoch': 0.04}
{'loss': 3.7125, 'grad_norm': 0.5743790542184384, 'learning_rate': 9.765566906090259e-05, 'epoch': 0.04}
{'loss': 3.8215, 'grad_norm': 0.5596960733315054, 'learning_rate': 9.766999603902561e-05, 'epoch': 0.04}
{'loss': 3.6276, 'grad_norm': 0.5509409815138088, 'learning_rate': 9.76843055558669e-05, 'epoch': 0.04}
{'loss': 3.6682, 'grad_norm': 0.5971553748167027, 'learning_rate': 9.769859765393716e-05, 'epoch': 0.04}
{'loss': 3.7299, 'grad_norm': 0.5808975076544328, 'learning_rate': 9.771287237559211e-05, 'epoch': 0.04}
{'loss': 3.7241, 'grad_norm': 0.59937534427275, 'learning_rate': 9.772712976303316e-05, 'epoch': 0.04}
{'loss': 3.6302, 'grad_norm': 0.607617379756298, 'learning_rate': 9.774136985830812e-05, 'epoch': 0.04}
{'loss': 3.8466, 'grad_norm': 0.556453879900824, 'learning_rate': 9.775559270331201e-05, 'epoch': 0.04}
{'loss': 3.7352, 'grad_norm': 0.5721517198744752, 'learning_rate': 9.77697983397878e-05, 'epoch': 0.04}
{'loss': 3.5716, 'grad_norm': 0.5562514280854735, 'learning_rate': 9.77839868093271e-05, 'epoch': 0.04}
{'loss': 3.7545, 'grad_norm': 0.5773490962282692, 'learning_rate': 9.779815815337096e-05, 'epoch': 0.04}
{'loss': 3.7454, 'grad_norm': 0.5768372204970438, 'learning_rate': 9.78123124132105e-05, 'epoch': 0.04}
{'loss': 3.551, 'grad_norm': 0.6234033299384143, 'learning_rate': 9.782644962998771e-05, 'epoch': 0.04}
{'loss': 3.6679, 'grad_norm': 0.6013382008025275, 'learning_rate': 9.784056984469614e-05, 'epoch': 0.04}
{'loss': 3.8072, 'grad_norm': 0.5628980714707763, 'learning_rate': 9.785467309818163e-05, 'epoch': 0.04}
{'loss': 3.59, 'grad_norm': 0.5594836957538127, 'learning_rate': 9.786875943114299e-05, 'epoch': 0.04}
{'loss': 3.6396, 'grad_norm': 0.5899853586472387, 'learning_rate': 9.788282888413269e-05, 'epoch': 0.04}
{'loss': 3.6725, 'grad_norm': 0.5952329423312765, 'learning_rate': 9.789688149755763e-05, 'epoch': 0.04}
{'loss': 3.8309, 'grad_norm': 0.5776739413122973, 'learning_rate': 9.791091731167981e-05, 'epoch': 0.04}
{'loss': 3.6238, 'grad_norm': 0.608848960290453, 'learning_rate': 9.792493636661695e-05, 'epoch': 0.04}
{'loss': 3.5688, 'grad_norm': 0.6150464145502366, 'learning_rate': 9.79389387023433e-05, 'epoch': 0.04}
{'loss': 3.7952, 'grad_norm': 0.6356503458547847, 'learning_rate': 9.79529243586902e-05, 'epoch': 0.04}
{'loss': 3.8678, 'grad_norm': 0.5730770269372097, 'learning_rate': 9.796689337534688e-05, 'epoch': 0.04}
{'loss': 3.8423, 'grad_norm': 0.5906964282727746, 'learning_rate': 9.798084579186098e-05, 'epoch': 0.04}
{'loss': 3.6045, 'grad_norm': 0.5735321926549964, 'learning_rate': 9.799478164763945e-05, 'epoch': 0.04}
{'loss': 3.7302, 'grad_norm': 0.559646442311992, 'learning_rate': 9.800870098194897e-05, 'epoch': 0.04}
{'loss': 3.7902, 'grad_norm': 0.5532262396873375, 'learning_rate': 9.802260383391679e-05, 'epoch': 0.04}
{'loss': 3.6365, 'grad_norm': 0.5753989030331303, 'learning_rate': 9.803649024253134e-05, 'epoch': 0.04}
{'loss': 3.7586, 'grad_norm': 0.5518462665341866, 'learning_rate': 9.80503602466428e-05, 'epoch': 0.04}
{'loss': 3.6412, 'grad_norm': 0.5722137452822751, 'learning_rate': 9.806421388496391e-05, 'epoch': 0.04}
{'loss': 3.7678, 'grad_norm': 0.5515189264923643, 'learning_rate': 9.80780511960705e-05, 'epoch': 0.04}
{'loss': 3.6658, 'grad_norm': 0.5730245962743067, 'learning_rate': 9.809187221840214e-05, 'epoch': 0.04}
{'loss': 3.7089, 'grad_norm': 0.6304099293199102, 'learning_rate': 9.810567699026286e-05, 'epoch': 0.04}
{'loss': 3.8013, 'grad_norm': 0.6310195322954781, 'learning_rate': 9.811946554982172e-05, 'epoch': 0.04}
{'loss': 3.7985, 'grad_norm': 0.5787852400625368, 'learning_rate': 9.813323793511346e-05, 'epoch': 0.04}
{'loss': 3.7734, 'grad_norm': 0.586267787060506, 'learning_rate': 9.814699418403912e-05, 'epoch': 0.04}
{'loss': 3.6665, 'grad_norm': 0.583752956627632, 'learning_rate': 9.816073433436669e-05, 'epoch': 0.04}
{'loss': 3.7695, 'grad_norm': 0.635400519081554, 'learning_rate': 9.817445842373171e-05, 'epoch': 0.04}
{'loss': 3.7969, 'grad_norm': 0.5543367992554264, 'learning_rate': 9.818816648963792e-05, 'epoch': 0.04}
{'loss': 3.9588, 'grad_norm': 0.5949941390709752, 'learning_rate': 9.820185856945779e-05, 'epoch': 0.04}
{'loss': 3.6881, 'grad_norm': 0.6784385557560212, 'learning_rate': 9.821553470043327e-05, 'epoch': 0.04}
{'loss': 3.8097, 'grad_norm': 0.6070228469493752, 'learning_rate': 9.822919491967628e-05, 'epoch': 0.04}
{'loss': 3.5894, 'grad_norm': 0.5731756265695637, 'learning_rate': 9.824283926416936e-05, 'epoch': 0.04}
{'loss': 3.7158, 'grad_norm': 0.5974244983528261, 'learning_rate': 9.825646777076624e-05, 'epoch': 0.04}
{'loss': 3.7898, 'grad_norm': 0.5615073934187749, 'learning_rate': 9.827008047619254e-05, 'epoch': 0.04}
{'loss': 3.7202, 'grad_norm': 0.5479392459296096, 'learning_rate': 9.828367741704618e-05, 'epoch': 0.04}
{'loss': 3.714, 'grad_norm': 0.5634942106462543, 'learning_rate': 9.829725862979816e-05, 'epoch': 0.04}
{'loss': 3.5988, 'grad_norm': 0.7429566227555076, 'learning_rate': 9.831082415079303e-05, 'epoch': 0.04}
{'loss': 3.4731, 'grad_norm': 0.5749743849690675, 'learning_rate': 9.832437401624955e-05, 'epoch': 0.04}
{'loss': 3.5591, 'grad_norm': 0.5636459876417331, 'learning_rate': 9.833790826226113e-05, 'epoch': 0.04}
{'loss': 3.8033, 'grad_norm': 0.5736815421394287, 'learning_rate': 9.83514269247966e-05, 'epoch': 0.04}
{'loss': 3.6813, 'grad_norm': 0.6192138397849282, 'learning_rate': 9.836493003970066e-05, 'epoch': 0.04}
{'loss': 3.4465, 'grad_norm': 0.5821027217223214, 'learning_rate': 9.837841764269445e-05, 'epoch': 0.04}
{'loss': 3.7317, 'grad_norm': 0.5920975449868768, 'learning_rate': 9.839188976937619e-05, 'epoch': 0.04}
{'loss': 3.572, 'grad_norm': 0.563042145878273, 'learning_rate': 9.840534645522169e-05, 'epoch': 0.04}
{'loss': 3.5536, 'grad_norm': 0.5871097445011192, 'learning_rate': 9.841878773558488e-05, 'epoch': 0.04}
{'loss': 3.758, 'grad_norm': 0.6357827189496064, 'learning_rate': 9.843221364569847e-05, 'epoch': 0.04}
{'loss': 3.7022, 'grad_norm': 0.5663622022654048, 'learning_rate': 9.844562422067437e-05, 'epoch': 0.04}
{'loss': 3.362, 'grad_norm': 0.5944339973056761, 'learning_rate': 9.845901949550436e-05, 'epoch': 0.04}
{'loss': 3.8869, 'grad_norm': 0.5703645414127495, 'learning_rate': 9.847239950506057e-05, 'epoch': 0.04}
{'loss': 3.7793, 'grad_norm': 0.5558915805955996, 'learning_rate': 9.84857642840961e-05, 'epoch': 0.04}
{'loss': 3.6463, 'grad_norm': 0.6182314104911352, 'learning_rate': 9.84991138672454e-05, 'epoch': 0.04}
{'loss': 3.881, 'grad_norm': 0.5756856116056922, 'learning_rate': 9.8512448289025e-05, 'epoch': 0.04}
{'loss': 3.6892, 'grad_norm': 0.5557740036944642, 'learning_rate': 9.852576758383395e-05, 'epoch': 0.04}
{'loss': 3.7825, 'grad_norm': 0.5623839055820763, 'learning_rate': 9.853907178595432e-05, 'epoch': 0.04}
{'loss': 3.834, 'grad_norm': 0.6729576077386349, 'learning_rate': 9.855236092955182e-05, 'epoch': 0.04}
{'loss': 3.728, 'grad_norm': 0.5757046850791934, 'learning_rate': 9.856563504867626e-05, 'epoch': 0.04}
{'loss': 3.8485, 'grad_norm': 0.5550100820977504, 'learning_rate': 9.857889417726204e-05, 'epoch': 0.04}
{'loss': 3.8105, 'grad_norm': 0.5728828980713464, 'learning_rate': 9.859213834912886e-05, 'epoch': 0.04}
{'loss': 3.7162, 'grad_norm': 0.5597507775017007, 'learning_rate': 9.860536759798193e-05, 'epoch': 0.04}
{'loss': 3.5847, 'grad_norm': 0.5621336342322336, 'learning_rate': 9.861858195741278e-05, 'epoch': 0.04}
{'loss': 3.7643, 'grad_norm': 0.5533778587847398, 'learning_rate': 9.863178146089962e-05, 'epoch': 0.04}
{'loss': 3.5686, 'grad_norm': 0.5352563448990063, 'learning_rate': 9.864496614180783e-05, 'epoch': 0.04}
{'loss': 3.5081, 'grad_norm': 0.5782409415757274, 'learning_rate': 9.865813603339054e-05, 'epoch': 0.04}
{'loss': 3.6418, 'grad_norm': 0.5613788938931004, 'learning_rate': 9.867129116878913e-05, 'epoch': 0.04}
{'loss': 3.8934, 'grad_norm': 0.5517672189882966, 'learning_rate': 9.868443158103365e-05, 'epoch': 0.04}
{'loss': 3.6787, 'grad_norm': 0.7695353619475879, 'learning_rate': 9.869755730304344e-05, 'epoch': 0.04}
{'loss': 3.5615, 'grad_norm': 0.5624545744387227, 'learning_rate': 9.871066836762749e-05, 'epoch': 0.04}
{'loss': 3.608, 'grad_norm': 0.5699729253979567, 'learning_rate': 9.872376480748504e-05, 'epoch': 0.04}
{'loss': 3.2087, 'grad_norm': 0.5878061611816614, 'learning_rate': 9.873684665520604e-05, 'epoch': 0.04}
{'loss': 3.6886, 'grad_norm': 0.6037469295367369, 'learning_rate': 9.874991394327158e-05, 'epoch': 0.04}
{'loss': 3.7674, 'grad_norm': 0.5679026100873478, 'learning_rate': 9.876296670405445e-05, 'epoch': 0.04}
{'loss': 3.8568, 'grad_norm': 0.5965363169188703, 'learning_rate': 9.877600496981959e-05, 'epoch': 0.04}
{'loss': 3.6949, 'grad_norm': 0.5844129980695385, 'learning_rate': 9.878902877272455e-05, 'epoch': 0.04}
{'loss': 3.766, 'grad_norm': 0.575497436299853, 'learning_rate': 9.880203814482003e-05, 'epoch': 0.04}
{'loss': 3.7719, 'grad_norm': 0.5551504840652889, 'learning_rate': 9.881503311805025e-05, 'epoch': 0.04}
{'loss': 3.6448, 'grad_norm': 0.8882775884300753, 'learning_rate': 9.882801372425351e-05, 'epoch': 0.04}
{'loss': 3.6784, 'grad_norm': 0.575882535763983, 'learning_rate': 9.884097999516265e-05, 'epoch': 0.04}
{'loss': 3.618, 'grad_norm': 0.5806952395510064, 'learning_rate': 9.885393196240544e-05, 'epoch': 0.04}
{'loss': 3.5851, 'grad_norm': 0.5675982377344726, 'learning_rate': 9.886686965750513e-05, 'epoch': 0.04}
{'loss': 3.6473, 'grad_norm': 0.5618866459141552, 'learning_rate': 9.887979311188086e-05, 'epoch': 0.04}
{'loss': 3.7701, 'grad_norm': 0.5862098784394731, 'learning_rate': 9.889270235684815e-05, 'epoch': 0.04}
{'loss': 3.7614, 'grad_norm': 0.5766138874228639, 'learning_rate': 9.890559742361931e-05, 'epoch': 0.04}
{'loss': 3.5896, 'grad_norm': 0.6282687876598331, 'learning_rate': 9.891847834330398e-05, 'epoch': 0.04}
{'loss': 3.8002, 'grad_norm': 0.5718831924408523, 'learning_rate': 9.893134514690943e-05, 'epoch': 0.04}
{'loss': 3.7285, 'grad_norm': 0.6567270787496451, 'learning_rate': 9.894419786534118e-05, 'epoch': 0.04}
{'loss': 3.605, 'grad_norm': 0.8047248164686711, 'learning_rate': 9.895703652940336e-05, 'epoch': 0.04}
{'loss': 3.757, 'grad_norm': 0.5895926153888099, 'learning_rate': 9.89698611697991e-05, 'epoch': 0.04}
{'loss': 3.7367, 'grad_norm': 0.5588766116523617, 'learning_rate': 9.898267181713106e-05, 'epoch': 0.04}
{'loss': 3.5951, 'grad_norm': 0.6363479090508725, 'learning_rate': 9.899546850190186e-05, 'epoch': 0.04}
{'loss': 3.5877, 'grad_norm': 0.7201088558361063, 'learning_rate': 9.900825125451447e-05, 'epoch': 0.04}
{'loss': 3.8368, 'grad_norm': 0.5882710639803483, 'learning_rate': 9.902102010527266e-05, 'epoch': 0.04}
{'loss': 3.7846, 'grad_norm': 0.8438685257213014, 'learning_rate': 9.903377508438144e-05, 'epoch': 0.04}
{'loss': 3.7526, 'grad_norm': 0.5656474531286485, 'learning_rate': 9.904651622194753e-05, 'epoch': 0.04}
{'loss': 3.7241, 'grad_norm': 0.616575605406465, 'learning_rate': 9.905924354797965e-05, 'epoch': 0.04}
{'loss': 3.5757, 'grad_norm': 0.5697862481435693, 'learning_rate': 9.907195709238913e-05, 'epoch': 0.04}
{'loss': 3.6854, 'grad_norm': 0.5574545116678039, 'learning_rate': 9.90846568849902e-05, 'epoch': 0.04}
{'loss': 3.7461, 'grad_norm': 0.5783468325856059, 'learning_rate': 9.90973429555004e-05, 'epoch': 0.04}
{'loss': 3.8247, 'grad_norm': 0.7213507023256811, 'learning_rate': 9.911001533354114e-05, 'epoch': 0.04}
{'loss': 3.7014, 'grad_norm': 0.644845891793361, 'learning_rate': 9.912267404863794e-05, 'epoch': 0.04}
{'loss': 3.7338, 'grad_norm': 0.5813294776861763, 'learning_rate': 9.913531913022097e-05, 'epoch': 0.04}
{'loss': 3.6184, 'grad_norm': 0.6135677000512809, 'learning_rate': 9.914795060762538e-05, 'epoch': 0.04}
{'loss': 3.819, 'grad_norm': 0.6031822933250258, 'learning_rate': 9.916056851009172e-05, 'epoch': 0.04}
{'loss': 3.6273, 'grad_norm': 0.6474970793159136, 'learning_rate': 9.91731728667665e-05, 'epoch': 0.04}
{'loss': 3.8056, 'grad_norm': 0.5856799349109301, 'learning_rate': 9.918576370670225e-05, 'epoch': 0.04}
{'loss': 3.6939, 'grad_norm': 0.5581155767585895, 'learning_rate': 9.919834105885836e-05, 'epoch': 0.04}
{'loss': 3.6317, 'grad_norm': 0.6307241124726207, 'learning_rate': 9.921090495210106e-05, 'epoch': 0.04}
{'loss': 3.7453, 'grad_norm': 0.5392494867231904, 'learning_rate': 9.922345541520412e-05, 'epoch': 0.04}
{'loss': 3.7291, 'grad_norm': 0.5606227975305987, 'learning_rate': 9.923599247684911e-05, 'epoch': 0.04}
{'loss': 3.4565, 'grad_norm': 0.6062896383780901, 'learning_rate': 9.924851616562581e-05, 'epoch': 0.04}
{'loss': 3.8617, 'grad_norm': 0.5704265271281052, 'learning_rate': 9.926102651003258e-05, 'epoch': 0.04}
{'loss': 3.4561, 'grad_norm': 3.236364616466295, 'learning_rate': 9.927352353847688e-05, 'epoch': 0.04}
{'loss': 3.5172, 'grad_norm': 1.431579080975462, 'learning_rate': 9.928600727927547e-05, 'epoch': 0.04}
{'loss': 2.4165, 'grad_norm': 0.6053886931022435, 'learning_rate': 9.929847776065486e-05, 'epoch': 0.04}
{'loss': 3.7178, 'grad_norm': 0.5848782251086307, 'learning_rate': 9.93109350107518e-05, 'epoch': 0.04}
{'loss': 3.7073, 'grad_norm': 0.5809081522541529, 'learning_rate': 9.932337905761354e-05, 'epoch': 0.04}
{'loss': 3.4666, 'grad_norm': 0.5601802711647498, 'learning_rate': 9.933580992919818e-05, 'epoch': 0.04}
{'loss': 3.8091, 'grad_norm': 0.5660384451309824, 'learning_rate': 9.934822765337523e-05, 'epoch': 0.04}
{'loss': 3.576, 'grad_norm': 0.5715461887285845, 'learning_rate': 9.936063225792575e-05, 'epoch': 0.04}
{'loss': 3.5926, 'grad_norm': 0.5419795503500655, 'learning_rate': 9.937302377054293e-05, 'epoch': 0.04}
{'loss': 3.5132, 'grad_norm': 0.5797180347486464, 'learning_rate': 9.938540221883231e-05, 'epoch': 0.04}
{'loss': 3.5723, 'grad_norm': 0.5951373900897623, 'learning_rate': 9.939776763031222e-05, 'epoch': 0.04}
{'loss': 3.6513, 'grad_norm': 0.5690871616596553, 'learning_rate': 9.941012003241418e-05, 'epoch': 0.04}
{'loss': 3.5713, 'grad_norm': 0.6185726891827259, 'learning_rate': 9.942245945248316e-05, 'epoch': 0.04}
{'loss': 3.4935, 'grad_norm': 0.5622958009165631, 'learning_rate': 9.943478591777802e-05, 'epoch': 0.04}
{'loss': 3.659, 'grad_norm': 0.6347353307591642, 'learning_rate': 9.94470994554719e-05, 'epoch': 0.04}
{'loss': 3.6953, 'grad_norm': 0.5779171613440733, 'learning_rate': 9.945940009265246e-05, 'epoch': 0.04}
{'loss': 3.6778, 'grad_norm': 0.5868321118954202, 'learning_rate': 9.947168785632237e-05, 'epoch': 0.04}
{'loss': 3.7005, 'grad_norm': 0.6891761231205017, 'learning_rate': 9.948396277339959e-05, 'epoch': 0.04}
{'loss': 3.7446, 'grad_norm': 0.5684846543240927, 'learning_rate': 9.949622487071775e-05, 'epoch': 0.04}
{'loss': 3.4797, 'grad_norm': 0.5638028137257932, 'learning_rate': 9.950847417502646e-05, 'epoch': 0.04}
{'loss': 3.6476, 'grad_norm': 0.5818588943158728, 'learning_rate': 9.952071071299174e-05, 'epoch': 0.04}
{'loss': 3.696, 'grad_norm': 0.5546967944537986, 'learning_rate': 9.953293451119627e-05, 'epoch': 0.04}
{'loss': 3.6332, 'grad_norm': 0.546573724579362, 'learning_rate': 9.954514559613988e-05, 'epoch': 0.04}
{'loss': 3.4558, 'grad_norm': 0.5737088954728772, 'learning_rate': 9.955734399423969e-05, 'epoch': 0.04}
{'loss': 3.7978, 'grad_norm': 0.5919631968601279, 'learning_rate': 9.956952973183062e-05, 'epoch': 0.04}
{'loss': 3.755, 'grad_norm': 0.5796768182046663, 'learning_rate': 9.958170283516574e-05, 'epoch': 0.04}
{'loss': 3.656, 'grad_norm': 0.575388495971048, 'learning_rate': 9.95938633304164e-05, 'epoch': 0.04}
{'loss': 3.6485, 'grad_norm': 0.545973844940263, 'learning_rate': 9.960601124367282e-05, 'epoch': 0.04}
{'loss': 3.4884, 'grad_norm': 0.5742659228023299, 'learning_rate': 9.961814660094432e-05, 'epoch': 0.04}
{'loss': 3.7131, 'grad_norm': 0.5617036755930886, 'learning_rate': 9.963026942815964e-05, 'epoch': 0.04}
{'loss': 3.6051, 'grad_norm': 0.6216471022171345, 'learning_rate': 9.964237975116724e-05, 'epoch': 0.04}
{'loss': 3.5438, 'grad_norm': 0.5525531900655942, 'learning_rate': 9.965447759573574e-05, 'epoch': 0.04}
{'loss': 3.5773, 'grad_norm': 0.5814422995441115, 'learning_rate': 9.966656298755416e-05, 'epoch': 0.04}
{'loss': 3.7464, 'grad_norm': 0.6028627293806892, 'learning_rate': 9.967863595223226e-05, 'epoch': 0.04}
{'loss': 3.5581, 'grad_norm': 0.5604314582330718, 'learning_rate': 9.969069651530092e-05, 'epoch': 0.04}
{'loss': 3.6914, 'grad_norm': 0.5847743874800903, 'learning_rate': 9.970274470221239e-05, 'epoch': 0.04}
{'loss': 3.7212, 'grad_norm': 0.617871749791916, 'learning_rate': 9.971478053834064e-05, 'epoch': 0.04}
{'loss': 3.6137, 'grad_norm': 0.539934115953675, 'learning_rate': 9.97268040489817e-05, 'epoch': 0.04}
{'loss': 3.6693, 'grad_norm': 0.7177157628260704, 'learning_rate': 9.973881525935398e-05, 'epoch': 0.04}
{'loss': 3.4588, 'grad_norm': 0.6284993614929748, 'learning_rate': 9.975081419459855e-05, 'epoch': 0.04}
{'loss': 3.6649, 'grad_norm': 0.5873480277670221, 'learning_rate': 9.976280087977951e-05, 'epoch': 0.04}
{'loss': 3.5877, 'grad_norm': 0.5543035346494745, 'learning_rate': 9.977477533988418e-05, 'epoch': 0.04}
{'loss': 3.7544, 'grad_norm': 0.629123775588322, 'learning_rate': 9.978673759982365e-05, 'epoch': 0.04}
{'loss': 3.7424, 'grad_norm': 0.6091511761619539, 'learning_rate': 9.979868768443282e-05, 'epoch': 0.04}
{'loss': 3.7406, 'grad_norm': 0.5670553734064498, 'learning_rate': 9.981062561847088e-05, 'epoch': 0.04}
{'loss': 3.6542, 'grad_norm': 0.6214070033557132, 'learning_rate': 9.982255142662162e-05, 'epoch': 0.04}
{'loss': 3.7628, 'grad_norm': 0.6419709202116827, 'learning_rate': 9.983446513349362e-05, 'epoch': 0.04}
{'loss': 3.58, 'grad_norm': 0.605696714985995, 'learning_rate': 9.984636676362062e-05, 'epoch': 0.04}
{'loss': 3.671, 'grad_norm': 1.4363807079857944, 'learning_rate': 9.985825634146192e-05, 'epoch': 0.04}
{'loss': 3.6381, 'grad_norm': 0.5652345398883415, 'learning_rate': 9.987013389140249e-05, 'epoch': 0.04}
{'loss': 3.7739, 'grad_norm': 0.597344074164035, 'learning_rate': 9.988199943775337e-05, 'epoch': 0.04}
{'loss': 3.674, 'grad_norm': 0.701291752728535, 'learning_rate': 9.989385300475205e-05, 'epoch': 0.04}
{'loss': 3.4526, 'grad_norm': 0.5652347114131925, 'learning_rate': 9.990569461656266e-05, 'epoch': 0.04}
{'loss': 3.799, 'grad_norm': 0.5986241955535448, 'learning_rate': 9.99175242972762e-05, 'epoch': 0.04}
{'loss': 3.7332, 'grad_norm': 0.5650769388593303, 'learning_rate': 9.992934207091101e-05, 'epoch': 0.04}
{'loss': 3.5615, 'grad_norm': 0.5993170759601404, 'learning_rate': 9.994114796141296e-05, 'epoch': 0.04}
{'loss': 3.4606, 'grad_norm': 0.5736604027895736, 'learning_rate': 9.995294199265576e-05, 'epoch': 0.04}
{'loss': 3.6532, 'grad_norm': 0.5803112525310231, 'learning_rate': 9.996472418844122e-05, 'epoch': 0.04}
{'loss': 3.6704, 'grad_norm': 0.5817913828481098, 'learning_rate': 9.997649457249965e-05, 'epoch': 0.04}
{'loss': 3.4137, 'grad_norm': 0.678987251018485, 'learning_rate': 9.998825316848992e-05, 'epoch': 0.04}
{'loss': 3.4977, 'grad_norm': 0.5840427976274754, 'learning_rate': 0.0001, 'epoch': 0.04}
{'loss': 3.7376, 'grad_norm': 0.5646988370349281, 'learning_rate': 9.999881506058004e-05, 'epoch': 0.04}
{'loss': 3.659, 'grad_norm': 0.5731266682558428, 'learning_rate': 9.999733388630506e-05, 'epoch': 0.04}
{'loss': 3.5249, 'grad_norm': 0.5564480224412661, 'learning_rate': 9.99958527120301e-05, 'epoch': 0.04}
{'loss': 3.3491, 'grad_norm': 6.682070438938963, 'learning_rate': 9.999437153775514e-05, 'epoch': 0.04}
{'loss': 3.7279, 'grad_norm': 0.5593034747871368, 'learning_rate': 9.999289036348017e-05, 'epoch': 0.04}
{'loss': 3.7248, 'grad_norm': 0.5738447724324119, 'learning_rate': 9.999140918920521e-05, 'epoch': 0.04}
{'loss': 3.7573, 'grad_norm': 0.5711854796384364, 'learning_rate': 9.998992801493025e-05, 'epoch': 0.04}
{'loss': 3.6175, 'grad_norm': 0.5705834375037007, 'learning_rate': 9.998844684065528e-05, 'epoch': 0.04}
{'loss': 3.5714, 'grad_norm': 0.58683634950507, 'learning_rate': 9.99869656663803e-05, 'epoch': 0.04}
{'loss': 3.6961, 'grad_norm': 0.5806300902489906, 'learning_rate': 9.998548449210534e-05, 'epoch': 0.04}
{'loss': 3.4834, 'grad_norm': 0.5711322709658994, 'learning_rate': 9.998400331783038e-05, 'epoch': 0.04}
{'loss': 3.6152, 'grad_norm': 0.60992984303728, 'learning_rate': 9.998252214355541e-05, 'epoch': 0.04}
{'loss': 3.6209, 'grad_norm': 0.558557607135394, 'learning_rate': 9.998104096928045e-05, 'epoch': 0.04}
{'loss': 3.7324, 'grad_norm': 0.5811468960233742, 'learning_rate': 9.997955979500549e-05, 'epoch': 0.04}
{'loss': 3.4605, 'grad_norm': 0.576028555004308, 'learning_rate': 9.997807862073052e-05, 'epoch': 0.04}
{'loss': 3.7362, 'grad_norm': 0.5604563091826499, 'learning_rate': 9.997659744645555e-05, 'epoch': 0.04}
{'loss': 3.7524, 'grad_norm': 0.572295335298592, 'learning_rate': 9.99751162721806e-05, 'epoch': 0.04}
{'loss': 3.7573, 'grad_norm': 0.5896184863805841, 'learning_rate': 9.997363509790562e-05, 'epoch': 0.04}
{'loss': 3.6985, 'grad_norm': 0.5508189202800228, 'learning_rate': 9.997215392363066e-05, 'epoch': 0.04}
{'loss': 3.4826, 'grad_norm': 0.5501616013340292, 'learning_rate': 9.99706727493557e-05, 'epoch': 0.04}
{'loss': 3.4047, 'grad_norm': 0.5636117890228984, 'learning_rate': 9.996919157508073e-05, 'epoch': 0.04}
{'loss': 3.7683, 'grad_norm': 0.5484940027309024, 'learning_rate': 9.996771040080575e-05, 'epoch': 0.04}
{'loss': 3.6733, 'grad_norm': 0.6012337854050095, 'learning_rate': 9.99662292265308e-05, 'epoch': 0.04}
{'loss': 3.6763, 'grad_norm': 0.5519541888112894, 'learning_rate': 9.996474805225583e-05, 'epoch': 0.04}
{'loss': 3.6244, 'grad_norm': 0.6402984867637789, 'learning_rate': 9.996326687798086e-05, 'epoch': 0.04}
{'loss': 3.7194, 'grad_norm': 0.7073538917109669, 'learning_rate': 9.99617857037059e-05, 'epoch': 0.04}
{'loss': 3.6593, 'grad_norm': 0.5622670956207426, 'learning_rate': 9.996030452943094e-05, 'epoch': 0.04}
{'loss': 3.6533, 'grad_norm': 0.5622231549371745, 'learning_rate': 9.995882335515597e-05, 'epoch': 0.05}
{'loss': 3.6635, 'grad_norm': 0.6195476560636923, 'learning_rate': 9.995734218088101e-05, 'epoch': 0.05}
{'loss': 3.4722, 'grad_norm': 0.5564211217317414, 'learning_rate': 9.995586100660605e-05, 'epoch': 0.05}
{'loss': 3.7325, 'grad_norm': 0.5551001899312136, 'learning_rate': 9.995437983233107e-05, 'epoch': 0.05}
{'loss': 3.5823, 'grad_norm': 0.5693039285476565, 'learning_rate': 9.995289865805611e-05, 'epoch': 0.05}
{'loss': 3.6306, 'grad_norm': 0.5744051596644969, 'learning_rate': 9.995141748378115e-05, 'epoch': 0.05}
{'loss': 3.655, 'grad_norm': 0.626673654947758, 'learning_rate': 9.994993630950618e-05, 'epoch': 0.05}
{'loss': 3.4681, 'grad_norm': 0.6046086726441398, 'learning_rate': 9.99484551352312e-05, 'epoch': 0.05}
{'loss': 3.6835, 'grad_norm': 0.56199586754407, 'learning_rate': 9.994697396095625e-05, 'epoch': 0.05}
{'loss': 3.8515, 'grad_norm': 0.6066743639921729, 'learning_rate': 9.994549278668129e-05, 'epoch': 0.05}
{'loss': 3.7293, 'grad_norm': 0.5666371057078963, 'learning_rate': 9.994401161240631e-05, 'epoch': 0.05}
{'loss': 3.5099, 'grad_norm': 0.609473442127928, 'learning_rate': 9.994253043813135e-05, 'epoch': 0.05}
{'loss': 2.5093, 'grad_norm': 0.5988666754786477, 'learning_rate': 9.994104926385639e-05, 'epoch': 0.05}
{'loss': 3.6476, 'grad_norm': 0.5782477961517994, 'learning_rate': 9.993956808958143e-05, 'epoch': 0.05}
{'loss': 3.773, 'grad_norm': 0.5686931529965874, 'learning_rate': 9.993808691530646e-05, 'epoch': 0.05}
{'loss': 3.6922, 'grad_norm': 0.5999248483026225, 'learning_rate': 9.99366057410315e-05, 'epoch': 0.05}
{'loss': 3.672, 'grad_norm': 0.5615925811970991, 'learning_rate': 9.993512456675654e-05, 'epoch': 0.05}
{'loss': 3.698, 'grad_norm': 0.5918185993823949, 'learning_rate': 9.993364339248156e-05, 'epoch': 0.05}
{'loss': 3.7513, 'grad_norm': 0.5740476250447211, 'learning_rate': 9.99321622182066e-05, 'epoch': 0.05}
{'loss': 3.6855, 'grad_norm': 0.5524650206954287, 'learning_rate': 9.993068104393163e-05, 'epoch': 0.05}
{'loss': 3.8467, 'grad_norm': 0.555677444387977, 'learning_rate': 9.992919986965666e-05, 'epoch': 0.05}
{'loss': 3.692, 'grad_norm': 0.5493277556710875, 'learning_rate': 9.99277186953817e-05, 'epoch': 0.05}
{'loss': 3.5609, 'grad_norm': 0.604703133712512, 'learning_rate': 9.992623752110674e-05, 'epoch': 0.05}
{'loss': 3.6957, 'grad_norm': 0.7391765027652727, 'learning_rate': 9.992475634683178e-05, 'epoch': 0.05}
{'loss': 3.877, 'grad_norm': 0.6034057811785243, 'learning_rate': 9.99232751725568e-05, 'epoch': 0.05}
{'loss': 3.2265, 'grad_norm': 0.6643089666799333, 'learning_rate': 9.992179399828184e-05, 'epoch': 0.05}
{'loss': 3.4843, 'grad_norm': 0.5968563732405059, 'learning_rate': 9.992031282400688e-05, 'epoch': 0.05}
{'loss': 3.5065, 'grad_norm': 0.5685560477629373, 'learning_rate': 9.991883164973191e-05, 'epoch': 0.05}
{'loss': 3.6723, 'grad_norm': 0.5840550081389124, 'learning_rate': 9.991735047545695e-05, 'epoch': 0.05}
{'loss': 3.6552, 'grad_norm': 0.554788232004651, 'learning_rate': 9.991586930118199e-05, 'epoch': 0.05}
{'loss': 3.6647, 'grad_norm': 0.5725922898321362, 'learning_rate': 9.991438812690702e-05, 'epoch': 0.05}
{'loss': 3.6752, 'grad_norm': 0.5952579823181441, 'learning_rate': 9.991290695263204e-05, 'epoch': 0.05}
{'loss': 3.7074, 'grad_norm': 0.572820520845863, 'learning_rate': 9.991142577835708e-05, 'epoch': 0.05}
{'loss': 3.6257, 'grad_norm': 0.5710309740655539, 'learning_rate': 9.990994460408212e-05, 'epoch': 0.05}
{'loss': 3.5138, 'grad_norm': 0.5792401593356659, 'learning_rate': 9.990846342980715e-05, 'epoch': 0.05}
{'loss': 3.6302, 'grad_norm': 0.5592566864797864, 'learning_rate': 9.990698225553219e-05, 'epoch': 0.05}
{'loss': 3.4801, 'grad_norm': 0.5621961209210532, 'learning_rate': 9.990550108125723e-05, 'epoch': 0.05}
{'loss': 3.6386, 'grad_norm': 0.5704468642301781, 'learning_rate': 9.990401990698226e-05, 'epoch': 0.05}
{'loss': 3.6365, 'grad_norm': 0.5457585155364275, 'learning_rate': 9.99025387327073e-05, 'epoch': 0.05}
{'loss': 3.7386, 'grad_norm': 0.5885289389858936, 'learning_rate': 9.990105755843234e-05, 'epoch': 0.05}
{'loss': 3.4457, 'grad_norm': 0.5794927089370591, 'learning_rate': 9.989957638415736e-05, 'epoch': 0.05}
{'loss': 3.4778, 'grad_norm': 0.5760807775728805, 'learning_rate': 9.98980952098824e-05, 'epoch': 0.05}
{'loss': 3.5499, 'grad_norm': 0.6218034092023643, 'learning_rate': 9.989661403560744e-05, 'epoch': 0.05}
{'loss': 2.8073, 'grad_norm': 0.572532614225657, 'learning_rate': 9.989513286133247e-05, 'epoch': 0.05}
{'loss': 3.7226, 'grad_norm': 0.5867265217604286, 'learning_rate': 9.98936516870575e-05, 'epoch': 0.05}
{'loss': 3.7132, 'grad_norm': 0.5658734500524395, 'learning_rate': 9.989217051278254e-05, 'epoch': 0.05}
{'loss': 3.8005, 'grad_norm': 0.5987860462464677, 'learning_rate': 9.989068933850758e-05, 'epoch': 0.05}
{'loss': 3.4841, 'grad_norm': 0.5706786625873987, 'learning_rate': 9.98892081642326e-05, 'epoch': 0.05}
{'loss': 3.3951, 'grad_norm': 0.5946448549945467, 'learning_rate': 9.988772698995764e-05, 'epoch': 0.05}
{'loss': 3.7768, 'grad_norm': 0.5686365926593794, 'learning_rate': 9.988624581568268e-05, 'epoch': 0.05}
{'loss': 3.6198, 'grad_norm': 0.538403209514807, 'learning_rate': 9.988476464140771e-05, 'epoch': 0.05}
{'loss': 3.6314, 'grad_norm': 0.5343841918021611, 'learning_rate': 9.988328346713275e-05, 'epoch': 0.05}
{'loss': 3.5631, 'grad_norm': 0.6229165370662708, 'learning_rate': 9.988180229285779e-05, 'epoch': 0.05}
{'loss': 3.5582, 'grad_norm': 0.5851799154717978, 'learning_rate': 9.988032111858281e-05, 'epoch': 0.05}
{'loss': 3.4871, 'grad_norm': 0.5700212981780067, 'learning_rate': 9.987883994430785e-05, 'epoch': 0.05}
{'loss': 3.4615, 'grad_norm': 0.5563275819077724, 'learning_rate': 9.98773587700329e-05, 'epoch': 0.05}
{'loss': 3.3142, 'grad_norm': 0.6204779427537911, 'learning_rate': 9.987587759575792e-05, 'epoch': 0.05}
{'loss': 3.7111, 'grad_norm': 0.5716814684431015, 'learning_rate': 9.987439642148295e-05, 'epoch': 0.05}
{'loss': 3.6866, 'grad_norm': 0.5610993207594943, 'learning_rate': 9.987291524720799e-05, 'epoch': 0.05}
{'loss': 3.4493, 'grad_norm': 0.6158870658924757, 'learning_rate': 9.987143407293303e-05, 'epoch': 0.05}
{'loss': 3.6469, 'grad_norm': 0.7229725653940812, 'learning_rate': 9.986995289865805e-05, 'epoch': 0.05}
{'loss': 3.5869, 'grad_norm': 0.5534750076716045, 'learning_rate': 9.986847172438309e-05, 'epoch': 0.05}
{'loss': 3.7239, 'grad_norm': 0.5951406837011353, 'learning_rate': 9.986699055010813e-05, 'epoch': 0.05}
{'loss': 3.6173, 'grad_norm': 0.5584024479299666, 'learning_rate': 9.986550937583316e-05, 'epoch': 0.05}
{'loss': 3.6174, 'grad_norm': 0.5967956568380698, 'learning_rate': 9.98640282015582e-05, 'epoch': 0.05}
{'loss': 3.7781, 'grad_norm': 0.5544126595935597, 'learning_rate': 9.986254702728324e-05, 'epoch': 0.05}
{'loss': 3.6786, 'grad_norm': 0.5585728390920836, 'learning_rate': 9.986106585300828e-05, 'epoch': 0.05}
{'loss': 3.5713, 'grad_norm': 0.6059323883125254, 'learning_rate': 9.98595846787333e-05, 'epoch': 0.05}
{'loss': 3.7504, 'grad_norm': 0.708841281901364, 'learning_rate': 9.985810350445835e-05, 'epoch': 0.05}
{'loss': 3.8489, 'grad_norm': 0.6693888266722842, 'learning_rate': 9.985662233018337e-05, 'epoch': 0.05}
{'loss': 3.7399, 'grad_norm': 0.5660875742809981, 'learning_rate': 9.98551411559084e-05, 'epoch': 0.05}
{'loss': 3.7425, 'grad_norm': 0.5578889388049146, 'learning_rate': 9.985365998163344e-05, 'epoch': 0.05}
{'loss': 3.5643, 'grad_norm': 0.5712930853668428, 'learning_rate': 9.985217880735848e-05, 'epoch': 0.05}
{'loss': 3.6526, 'grad_norm': 0.5750251269893039, 'learning_rate': 9.985069763308352e-05, 'epoch': 0.05}
{'loss': 3.5836, 'grad_norm': 0.5897957055286561, 'learning_rate': 9.984921645880855e-05, 'epoch': 0.05}
{'loss': 3.6024, 'grad_norm': 0.5582517370207238, 'learning_rate': 9.984773528453359e-05, 'epoch': 0.05}
{'loss': 3.6496, 'grad_norm': 0.5813449056568999, 'learning_rate': 9.984625411025862e-05, 'epoch': 0.05}
{'loss': 3.639, 'grad_norm': 0.5289899017615229, 'learning_rate': 9.984477293598365e-05, 'epoch': 0.05}
{'loss': 3.6629, 'grad_norm': 0.5788693113851073, 'learning_rate': 9.984329176170869e-05, 'epoch': 0.05}
{'loss': 3.3041, 'grad_norm': 1.0061101678146973, 'learning_rate': 9.984181058743373e-05, 'epoch': 0.05}
{'loss': 3.2073, 'grad_norm': 0.5856467487567539, 'learning_rate': 9.984032941315876e-05, 'epoch': 0.05}
{'loss': 3.5677, 'grad_norm': 0.5714265730334286, 'learning_rate': 9.983884823888378e-05, 'epoch': 0.05}
{'loss': 3.4298, 'grad_norm': 0.5571860864823581, 'learning_rate': 9.983736706460882e-05, 'epoch': 0.05}
{'loss': 3.6974, 'grad_norm': 0.5947836171412979, 'learning_rate': 9.983588589033386e-05, 'epoch': 0.05}
{'loss': 3.6885, 'grad_norm': 0.5709111648033091, 'learning_rate': 9.983440471605889e-05, 'epoch': 0.05}
{'loss': 3.6007, 'grad_norm': 0.580888528823138, 'learning_rate': 9.983292354178393e-05, 'epoch': 0.05}
{'loss': 3.8045, 'grad_norm': 0.5876404746956502, 'learning_rate': 9.983144236750897e-05, 'epoch': 0.05}
{'loss': 3.6138, 'grad_norm': 0.6432353261818422, 'learning_rate': 9.9829961193234e-05, 'epoch': 0.05}
{'loss': 3.7366, 'grad_norm': 0.5841881733497601, 'learning_rate': 9.982848001895904e-05, 'epoch': 0.05}
{'loss': 3.6659, 'grad_norm': 0.574286131234652, 'learning_rate': 9.982699884468408e-05, 'epoch': 0.05}
{'loss': 3.5813, 'grad_norm': 0.5651248593942804, 'learning_rate': 9.98255176704091e-05, 'epoch': 0.05}
{'loss': 3.6118, 'grad_norm': 0.5736213470803454, 'learning_rate': 9.982403649613414e-05, 'epoch': 0.05}
{'loss': 3.6889, 'grad_norm': 0.576786338266124, 'learning_rate': 9.982255532185918e-05, 'epoch': 0.05}
{'loss': 3.4394, 'grad_norm': 0.6249845813880776, 'learning_rate': 9.982107414758421e-05, 'epoch': 0.05}
{'loss': 3.5467, 'grad_norm': 0.6130017819435285, 'learning_rate': 9.981959297330924e-05, 'epoch': 0.05}
{'loss': 3.5645, 'grad_norm': 0.6147244416674037, 'learning_rate': 9.981811179903428e-05, 'epoch': 0.05}
{'loss': 3.5894, 'grad_norm': 0.6120837638743534, 'learning_rate': 9.981663062475932e-05, 'epoch': 0.05}
{'loss': 3.6916, 'grad_norm': 0.5896115176446485, 'learning_rate': 9.981514945048434e-05, 'epoch': 0.05}
{'loss': 3.481, 'grad_norm': 0.582286340279841, 'learning_rate': 9.981366827620938e-05, 'epoch': 0.05}
{'loss': 3.6861, 'grad_norm': 0.5808496152120239, 'learning_rate': 9.981218710193442e-05, 'epoch': 0.05}
{'loss': 3.7566, 'grad_norm': 0.562331259194138, 'learning_rate': 9.981070592765945e-05, 'epoch': 0.05}
{'loss': 3.6415, 'grad_norm': 0.5917179415137613, 'learning_rate': 9.980922475338449e-05, 'epoch': 0.05}
{'loss': 3.4726, 'grad_norm': 0.6180974709147308, 'learning_rate': 9.980774357910953e-05, 'epoch': 0.05}
{'loss': 3.6893, 'grad_norm': 0.584761696895325, 'learning_rate': 9.980626240483456e-05, 'epoch': 0.05}
{'loss': 3.6684, 'grad_norm': 0.5628158685889633, 'learning_rate': 9.98047812305596e-05, 'epoch': 0.05}
{'loss': 3.4468, 'grad_norm': 0.583185525307986, 'learning_rate': 9.980330005628464e-05, 'epoch': 0.05}
{'loss': 3.5489, 'grad_norm': 0.5830834299112598, 'learning_rate': 9.980181888200966e-05, 'epoch': 0.05}
{'loss': 3.4771, 'grad_norm': 0.6145516380358268, 'learning_rate': 9.980033770773469e-05, 'epoch': 0.05}
{'loss': 3.4635, 'grad_norm': 0.574314144153033, 'learning_rate': 9.979885653345973e-05, 'epoch': 0.05}
{'loss': 3.6869, 'grad_norm': 0.5441523073319054, 'learning_rate': 9.979737535918477e-05, 'epoch': 0.05}
{'loss': 3.7725, 'grad_norm': 0.5508041312184947, 'learning_rate': 9.97958941849098e-05, 'epoch': 0.05}
{'loss': 3.5745, 'grad_norm': 0.5932470840098293, 'learning_rate': 9.979441301063483e-05, 'epoch': 0.05}
{'loss': 3.6831, 'grad_norm': 0.6105911587491454, 'learning_rate': 9.979293183635987e-05, 'epoch': 0.05}
{'loss': 3.6448, 'grad_norm': 0.6060691734817608, 'learning_rate': 9.97914506620849e-05, 'epoch': 0.05}
{'loss': 3.6418, 'grad_norm': 0.5696745475225138, 'learning_rate': 9.978996948780994e-05, 'epoch': 0.05}
{'loss': 3.571, 'grad_norm': 0.5723295712331196, 'learning_rate': 9.978848831353498e-05, 'epoch': 0.05}
{'loss': 3.4902, 'grad_norm': 0.5720697609610123, 'learning_rate': 9.978700713926001e-05, 'epoch': 0.05}
{'loss': 3.4175, 'grad_norm': 0.6138994628975506, 'learning_rate': 9.978552596498505e-05, 'epoch': 0.05}
{'loss': 3.6253, 'grad_norm': 0.5654800910942576, 'learning_rate': 9.978404479071009e-05, 'epoch': 0.05}
{'loss': 3.4584, 'grad_norm': 0.5686311178589226, 'learning_rate': 9.978256361643511e-05, 'epoch': 0.05}
{'loss': 3.4962, 'grad_norm': 0.6041353163505508, 'learning_rate': 9.978108244216014e-05, 'epoch': 0.05}
{'loss': 3.4081, 'grad_norm': 0.6093908320503272, 'learning_rate': 9.977960126788518e-05, 'epoch': 0.05}
{'loss': 3.6457, 'grad_norm': 0.5589744030589089, 'learning_rate': 9.977812009361022e-05, 'epoch': 0.05}
{'loss': 3.7281, 'grad_norm': 0.5576400079791136, 'learning_rate': 9.977663891933525e-05, 'epoch': 0.05}
{'loss': 3.6939, 'grad_norm': 0.5656952307564709, 'learning_rate': 9.977515774506029e-05, 'epoch': 0.05}
{'loss': 3.0041, 'grad_norm': 0.5230388207922229, 'learning_rate': 9.977367657078533e-05, 'epoch': 0.05}
{'loss': 3.6655, 'grad_norm': 0.5606291741371786, 'learning_rate': 9.977219539651037e-05, 'epoch': 0.05}
{'loss': 3.4895, 'grad_norm': 0.6271724202832603, 'learning_rate': 9.977071422223539e-05, 'epoch': 0.05}
{'loss': 3.5661, 'grad_norm': 0.5908732201305051, 'learning_rate': 9.976923304796043e-05, 'epoch': 0.05}
{'loss': 3.6195, 'grad_norm': 0.5740547163194705, 'learning_rate': 9.976775187368547e-05, 'epoch': 0.05}
{'loss': 3.6324, 'grad_norm': 0.5871532500179136, 'learning_rate': 9.97662706994105e-05, 'epoch': 0.05}
{'loss': 3.5903, 'grad_norm': 0.5688835612657769, 'learning_rate': 9.976478952513553e-05, 'epoch': 0.05}
{'loss': 3.7551, 'grad_norm': 0.5956830416405711, 'learning_rate': 9.976330835086057e-05, 'epoch': 0.05}
{'loss': 3.612, 'grad_norm': 0.6258738907869866, 'learning_rate': 9.97618271765856e-05, 'epoch': 0.05}
{'loss': 3.4988, 'grad_norm': 0.5483121231339401, 'learning_rate': 9.976034600231063e-05, 'epoch': 0.05}
{'loss': 3.5818, 'grad_norm': 0.5675973783667947, 'learning_rate': 9.975886482803567e-05, 'epoch': 0.05}
{'loss': 3.5957, 'grad_norm': 0.5644450675552591, 'learning_rate': 9.975738365376071e-05, 'epoch': 0.05}
{'loss': 3.5647, 'grad_norm': 1.1175510246120726, 'learning_rate': 9.975590247948574e-05, 'epoch': 0.05}
{'loss': 3.7058, 'grad_norm': 0.5431302495931809, 'learning_rate': 9.975442130521078e-05, 'epoch': 0.05}
{'loss': 3.8092, 'grad_norm': 0.5692607898759395, 'learning_rate': 9.975294013093582e-05, 'epoch': 0.05}
{'loss': 3.5072, 'grad_norm': 0.5914978540128607, 'learning_rate': 9.975145895666084e-05, 'epoch': 0.05}
{'loss': 3.641, 'grad_norm': 0.5706714452906182, 'learning_rate': 9.974997778238588e-05, 'epoch': 0.05}
{'loss': 3.6916, 'grad_norm': 0.6267161567534305, 'learning_rate': 9.974849660811092e-05, 'epoch': 0.05}
{'loss': 3.495, 'grad_norm': 0.5552577389107933, 'learning_rate': 9.974701543383595e-05, 'epoch': 0.05}
{'loss': 3.7085, 'grad_norm': 0.5552411028197618, 'learning_rate': 9.974553425956098e-05, 'epoch': 0.05}
{'loss': 3.6459, 'grad_norm': 0.5742224966857199, 'learning_rate': 9.974405308528602e-05, 'epoch': 0.05}
{'loss': 3.7028, 'grad_norm': 0.5778438369955922, 'learning_rate': 9.974257191101106e-05, 'epoch': 0.05}
{'loss': 3.4734, 'grad_norm': 0.5612118453404279, 'learning_rate': 9.974109073673608e-05, 'epoch': 0.05}
{'loss': 3.3025, 'grad_norm': 0.7728580501834347, 'learning_rate': 9.973960956246112e-05, 'epoch': 0.05}
{'loss': 3.3327, 'grad_norm': 0.5755232254833136, 'learning_rate': 9.973812838818616e-05, 'epoch': 0.05}
{'loss': 3.8046, 'grad_norm': 0.5725047638449675, 'learning_rate': 9.973664721391119e-05, 'epoch': 0.05}
{'loss': 3.4343, 'grad_norm': 0.8129753431409772, 'learning_rate': 9.973516603963623e-05, 'epoch': 0.05}
{'loss': 3.7373, 'grad_norm': 0.5688662888801248, 'learning_rate': 9.973368486536127e-05, 'epoch': 0.05}
{'loss': 3.7171, 'grad_norm': 0.5538237854150088, 'learning_rate': 9.97322036910863e-05, 'epoch': 0.05}
{'loss': 3.3586, 'grad_norm': 1.0749069718773787, 'learning_rate': 9.973072251681134e-05, 'epoch': 0.05}
{'loss': 3.566, 'grad_norm': 0.5914649886869346, 'learning_rate': 9.972924134253638e-05, 'epoch': 0.05}
{'loss': 3.6393, 'grad_norm': 0.5788455613690664, 'learning_rate': 9.97277601682614e-05, 'epoch': 0.05}
{'loss': 3.4346, 'grad_norm': 0.6169068032576837, 'learning_rate': 9.972627899398643e-05, 'epoch': 0.05}
{'loss': 3.5166, 'grad_norm': 0.5687803609998283, 'learning_rate': 9.972479781971147e-05, 'epoch': 0.05}
{'loss': 3.6107, 'grad_norm': 0.6188150392241958, 'learning_rate': 9.972331664543651e-05, 'epoch': 0.05}
{'loss': 3.6528, 'grad_norm': 0.7668209263571182, 'learning_rate': 9.972183547116154e-05, 'epoch': 0.05}
{'loss': 3.5555, 'grad_norm': 0.9476825913891979, 'learning_rate': 9.972035429688658e-05, 'epoch': 0.05}
{'loss': 3.6624, 'grad_norm': 0.5662430235807445, 'learning_rate': 9.971887312261162e-05, 'epoch': 0.05}
{'loss': 3.6628, 'grad_norm': 0.5863039192188972, 'learning_rate': 9.971739194833664e-05, 'epoch': 0.05}
{'loss': 3.6509, 'grad_norm': 0.601691472356894, 'learning_rate': 9.971591077406168e-05, 'epoch': 0.05}
{'loss': 3.5214, 'grad_norm': 0.5757541912734052, 'learning_rate': 9.971442959978672e-05, 'epoch': 0.05}
{'loss': 3.5087, 'grad_norm': 0.569904820215225, 'learning_rate': 9.971294842551175e-05, 'epoch': 0.05}
{'loss': 3.7041, 'grad_norm': 0.5798587390359834, 'learning_rate': 9.971146725123679e-05, 'epoch': 0.05}
{'loss': 3.5278, 'grad_norm': 0.5858536346587802, 'learning_rate': 9.970998607696183e-05, 'epoch': 0.05}
{'loss': 3.5765, 'grad_norm': 0.5597717784895772, 'learning_rate': 9.970850490268685e-05, 'epoch': 0.05}
{'loss': 3.4613, 'grad_norm': 0.5964581050115102, 'learning_rate': 9.970702372841188e-05, 'epoch': 0.05}
{'loss': 3.3423, 'grad_norm': 0.53214164151438, 'learning_rate': 9.970554255413692e-05, 'epoch': 0.05}
{'loss': 3.5706, 'grad_norm': 0.5709944666147477, 'learning_rate': 9.970406137986196e-05, 'epoch': 0.05}
[2024-04-12 14:17:51,983] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6000 is about to be saved!
[2024-04-12 14:17:51,988] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-6000/global_step6000/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-12 14:17:51,989] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6000/global_step6000/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-12 14:17:51,995] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6000/global_step6000/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-12 14:17:51,996] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-6000/global_step6000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-12 14:17:59,064] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-6000/global_step6000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-12 14:17:59,064] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-6000/global_step6000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-12 14:17:59,065] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
{'loss': 3.5456, 'grad_norm': 0.5526108996376464, 'learning_rate': 9.970258020558699e-05, 'epoch': 0.05}
{'loss': 3.7198, 'grad_norm': 0.5349255485456004, 'learning_rate': 9.970109903131203e-05, 'epoch': 0.05}
{'loss': 3.571, 'grad_norm': 0.5785168583127361, 'learning_rate': 9.969961785703707e-05, 'epoch': 0.05}
{'loss': 3.5911, 'grad_norm': 0.5657465732872798, 'learning_rate': 9.96981366827621e-05, 'epoch': 0.05}
{'loss': 3.7668, 'grad_norm': 0.6303999567077425, 'learning_rate': 9.969665550848713e-05, 'epoch': 0.05}
{'loss': 3.6077, 'grad_norm': 0.5457850379363113, 'learning_rate': 9.969517433421217e-05, 'epoch': 0.05}
{'loss': 3.7417, 'grad_norm': 0.5885251125298379, 'learning_rate': 9.969369315993721e-05, 'epoch': 0.05}
{'loss': 3.4212, 'grad_norm': 0.5856860791902119, 'learning_rate': 9.969221198566224e-05, 'epoch': 0.05}
{'loss': 3.6218, 'grad_norm': 0.6068256153763186, 'learning_rate': 9.969073081138727e-05, 'epoch': 0.05}
{'loss': 3.5673, 'grad_norm': 0.5626725549357977, 'learning_rate': 9.96892496371123e-05, 'epoch': 0.05}
{'loss': 3.2392, 'grad_norm': 1.4606311917883708, 'learning_rate': 9.968776846283733e-05, 'epoch': 0.05}
{'loss': 3.6341, 'grad_norm': 0.5723169275729919, 'learning_rate': 9.968628728856237e-05, 'epoch': 0.05}
{'loss': 3.6588, 'grad_norm': 0.583245741325977, 'learning_rate': 9.968480611428741e-05, 'epoch': 0.05}
{'loss': 3.636, 'grad_norm': 0.5618449975238307, 'learning_rate': 9.968332494001245e-05, 'epoch': 0.05}
{'loss': 3.5433, 'grad_norm': 0.5745434279304672, 'learning_rate': 9.968184376573748e-05, 'epoch': 0.05}
{'loss': 3.6821, 'grad_norm': 0.6393466482927399, 'learning_rate': 9.968036259146252e-05, 'epoch': 0.05}
{'loss': 3.5155, 'grad_norm': 0.7650013215230375, 'learning_rate': 9.967888141718756e-05, 'epoch': 0.05}
{'loss': 3.7533, 'grad_norm': 0.5846224681925495, 'learning_rate': 9.967740024291259e-05, 'epoch': 0.05}
{'loss': 3.4334, 'grad_norm': 1.0984100184397287, 'learning_rate': 9.967591906863763e-05, 'epoch': 0.05}
{'loss': 3.7526, 'grad_norm': 0.5881201070487188, 'learning_rate': 9.967443789436267e-05, 'epoch': 0.05}
{'loss': 3.6588, 'grad_norm': 0.5716004056098162, 'learning_rate': 9.967295672008769e-05, 'epoch': 0.05}
{'loss': 3.6303, 'grad_norm': 0.5646577441929027, 'learning_rate': 9.967147554581272e-05, 'epoch': 0.05}
{'loss': 3.5082, 'grad_norm': 0.6144528876154404, 'learning_rate': 9.966999437153776e-05, 'epoch': 0.05}
{'loss': 3.5967, 'grad_norm': 0.558952911171602, 'learning_rate': 9.96685131972628e-05, 'epoch': 0.05}
{'loss': 3.58, 'grad_norm': 0.6216240123993186, 'learning_rate': 9.966703202298782e-05, 'epoch': 0.05}
{'loss': 3.7265, 'grad_norm': 0.6721088196733026, 'learning_rate': 9.966555084871286e-05, 'epoch': 0.05}
{'loss': 3.7748, 'grad_norm': 0.6090585931227831, 'learning_rate': 9.96640696744379e-05, 'epoch': 0.05}
{'loss': 3.7497, 'grad_norm': 0.5962045730042255, 'learning_rate': 9.966258850016293e-05, 'epoch': 0.05}
{'loss': 3.5649, 'grad_norm': 0.5637857380025129, 'learning_rate': 9.966110732588797e-05, 'epoch': 0.05}
{'loss': 3.3801, 'grad_norm': 0.5536335352361592, 'learning_rate': 9.965962615161301e-05, 'epoch': 0.05}
{'loss': 3.6391, 'grad_norm': 0.5740652670486122, 'learning_rate': 9.965814497733804e-05, 'epoch': 0.05}
{'loss': 3.5795, 'grad_norm': 0.5529385950310506, 'learning_rate': 9.965666380306308e-05, 'epoch': 0.05}
{'loss': 3.4008, 'grad_norm': 0.5317413445052384, 'learning_rate': 9.965518262878812e-05, 'epoch': 0.05}
{'loss': 3.6854, 'grad_norm': 0.5602867355230097, 'learning_rate': 9.965370145451314e-05, 'epoch': 0.05}
{'loss': 3.7179, 'grad_norm': 0.6440183459843424, 'learning_rate': 9.965222028023817e-05, 'epoch': 0.05}
{'loss': 3.5887, 'grad_norm': 0.6392074694160447, 'learning_rate': 9.965073910596321e-05, 'epoch': 0.05}
{'loss': 3.4768, 'grad_norm': 0.5836886391514988, 'learning_rate': 9.964925793168825e-05, 'epoch': 0.05}
{'loss': 3.5041, 'grad_norm': 0.5623523495400458, 'learning_rate': 9.964777675741328e-05, 'epoch': 0.05}
{'loss': 3.4222, 'grad_norm': 0.6048726209311729, 'learning_rate': 9.964629558313832e-05, 'epoch': 0.05}
{'loss': 3.6182, 'grad_norm': 0.5551559794885997, 'learning_rate': 9.964481440886336e-05, 'epoch': 0.05}
{'loss': 2.9715, 'grad_norm': 0.7829220698271773, 'learning_rate': 9.964333323458838e-05, 'epoch': 0.05}
{'loss': 3.6832, 'grad_norm': 0.5685573414987404, 'learning_rate': 9.964185206031342e-05, 'epoch': 0.05}
{'loss': 3.5164, 'grad_norm': 0.5705191940507263, 'learning_rate': 9.964037088603846e-05, 'epoch': 0.05}
{'loss': 3.5996, 'grad_norm': 0.5728229886818759, 'learning_rate': 9.963888971176349e-05, 'epoch': 0.05}
{'loss': 3.6566, 'grad_norm': 0.6381442435920269, 'learning_rate': 9.963740853748853e-05, 'epoch': 0.05}
{'loss': 3.4247, 'grad_norm': 0.57689348841902, 'learning_rate': 9.963592736321357e-05, 'epoch': 0.05}
{'loss': 3.6139, 'grad_norm': 0.5711349167618109, 'learning_rate': 9.96344461889386e-05, 'epoch': 0.05}
{'loss': 3.3672, 'grad_norm': 0.6044172053817095, 'learning_rate': 9.963296501466362e-05, 'epoch': 0.05}
{'loss': 3.5607, 'grad_norm': 0.5898820875691827, 'learning_rate': 9.963148384038866e-05, 'epoch': 0.05}
{'loss': 3.5772, 'grad_norm': 0.5557625098776382, 'learning_rate': 9.96300026661137e-05, 'epoch': 0.05}
{'loss': 3.6861, 'grad_norm': 0.5739597371006524, 'learning_rate': 9.962852149183873e-05, 'epoch': 0.05}
{'loss': 3.7677, 'grad_norm': 0.7799442523151248, 'learning_rate': 9.962704031756377e-05, 'epoch': 0.05}
{'loss': 3.719, 'grad_norm': 0.5652439015538249, 'learning_rate': 9.962555914328881e-05, 'epoch': 0.05}
{'loss': 3.6613, 'grad_norm': 0.6247255759016161, 'learning_rate': 9.962407796901383e-05, 'epoch': 0.05}
{'loss': 3.536, 'grad_norm': 0.6918455929828622, 'learning_rate': 9.962259679473887e-05, 'epoch': 0.05}
{'loss': 3.3034, 'grad_norm': 0.5438772057793979, 'learning_rate': 9.962111562046391e-05, 'epoch': 0.05}
{'loss': 3.5845, 'grad_norm': 0.5812822775255764, 'learning_rate': 9.961963444618895e-05, 'epoch': 0.06}
{'loss': 3.2886, 'grad_norm': 0.600994093914015, 'learning_rate': 9.961815327191398e-05, 'epoch': 0.06}
{'loss': 3.5749, 'grad_norm': 0.747078565555064, 'learning_rate': 9.961667209763901e-05, 'epoch': 0.06}
{'loss': 3.4076, 'grad_norm': 0.5858380845592382, 'learning_rate': 9.961519092336405e-05, 'epoch': 0.06}
{'loss': 3.459, 'grad_norm': 0.5827629132195539, 'learning_rate': 9.961370974908907e-05, 'epoch': 0.06}
{'loss': 3.598, 'grad_norm': 0.5809179977886036, 'learning_rate': 9.961222857481411e-05, 'epoch': 0.06}
{'loss': 3.6694, 'grad_norm': 0.5899997789959813, 'learning_rate': 9.961074740053915e-05, 'epoch': 0.06}
{'loss': 3.3974, 'grad_norm': 0.5590460104515973, 'learning_rate': 9.960926622626418e-05, 'epoch': 0.06}
{'loss': 3.6626, 'grad_norm': 0.6105566400706258, 'learning_rate': 9.960778505198922e-05, 'epoch': 0.06}
{'loss': 3.5337, 'grad_norm': 0.5801611136433221, 'learning_rate': 9.960630387771426e-05, 'epoch': 0.06}
{'loss': 3.7624, 'grad_norm': 0.5625257102719702, 'learning_rate': 9.96048227034393e-05, 'epoch': 0.06}
{'loss': 3.6781, 'grad_norm': 0.5536063075568339, 'learning_rate': 9.960334152916433e-05, 'epoch': 0.06}
{'loss': 3.5325, 'grad_norm': 0.5780487410433746, 'learning_rate': 9.960186035488937e-05, 'epoch': 0.06}
{'loss': 3.6068, 'grad_norm': 0.7183280148572557, 'learning_rate': 9.96003791806144e-05, 'epoch': 0.06}
{'loss': 3.4914, 'grad_norm': 0.5768325024214073, 'learning_rate': 9.959889800633943e-05, 'epoch': 0.06}
{'loss': 3.372, 'grad_norm': 0.573597847307114, 'learning_rate': 9.959741683206446e-05, 'epoch': 0.06}
{'loss': 3.6484, 'grad_norm': 0.5786506583796603, 'learning_rate': 9.95959356577895e-05, 'epoch': 0.06}
{'loss': 3.5101, 'grad_norm': 0.5611928685086307, 'learning_rate': 9.959445448351454e-05, 'epoch': 0.06}
{'loss': 3.638, 'grad_norm': 0.5643994663066184, 'learning_rate': 9.959297330923957e-05, 'epoch': 0.06}
{'loss': 3.5572, 'grad_norm': 0.5796065673507163, 'learning_rate': 9.95914921349646e-05, 'epoch': 0.06}
{'loss': 3.7328, 'grad_norm': 0.5846613260881618, 'learning_rate': 9.959001096068965e-05, 'epoch': 0.06}
{'loss': 3.5396, 'grad_norm': 0.604918969513177, 'learning_rate': 9.958852978641467e-05, 'epoch': 0.06}
{'loss': 3.4749, 'grad_norm': 0.5756964592643368, 'learning_rate': 9.958704861213971e-05, 'epoch': 0.06}
{'loss': 3.7108, 'grad_norm': 0.6263538017677168, 'learning_rate': 9.958556743786475e-05, 'epoch': 0.06}
{'loss': 3.5869, 'grad_norm': 0.574771563306476, 'learning_rate': 9.958408626358978e-05, 'epoch': 0.06}
{'loss': 3.6377, 'grad_norm': 0.5620143630165814, 'learning_rate': 9.958260508931482e-05, 'epoch': 0.06}
{'loss': 3.6787, 'grad_norm': 0.5786721078643441, 'learning_rate': 9.958112391503986e-05, 'epoch': 0.06}
{'loss': 3.685, 'grad_norm': 0.5768100781474498, 'learning_rate': 9.957964274076488e-05, 'epoch': 0.06}
{'loss': 3.5135, 'grad_norm': 0.5833935055884002, 'learning_rate': 9.957816156648991e-05, 'epoch': 0.06}
{'loss': 3.4603, 'grad_norm': 0.5749770912087456, 'learning_rate': 9.957668039221495e-05, 'epoch': 0.06}
{'loss': 3.469, 'grad_norm': 0.5722684186407145, 'learning_rate': 9.957519921793999e-05, 'epoch': 0.06}
{'loss': 3.6839, 'grad_norm': 0.6096866710932215, 'learning_rate': 9.957371804366502e-05, 'epoch': 0.06}
{'loss': 3.3783, 'grad_norm': 0.5769404837565797, 'learning_rate': 9.957223686939006e-05, 'epoch': 0.06}
{'loss': 3.6515, 'grad_norm': 0.5624539332145942, 'learning_rate': 9.95707556951151e-05, 'epoch': 0.06}
{'loss': 3.6877, 'grad_norm': 0.5592456210773569, 'learning_rate': 9.956927452084012e-05, 'epoch': 0.06}
{'loss': 3.6157, 'grad_norm': 0.5601809966459339, 'learning_rate': 9.956779334656516e-05, 'epoch': 0.06}
{'loss': 3.2979, 'grad_norm': 0.5997904011920664, 'learning_rate': 9.95663121722902e-05, 'epoch': 0.06}
{'loss': 3.4912, 'grad_norm': 0.6938545410679564, 'learning_rate': 9.956483099801523e-05, 'epoch': 0.06}
{'loss': 3.5099, 'grad_norm': 0.5618478004683619, 'learning_rate': 9.956334982374027e-05, 'epoch': 0.06}
{'loss': 3.6083, 'grad_norm': 0.5967352788012635, 'learning_rate': 9.956186864946531e-05, 'epoch': 0.06}
{'loss': 3.5546, 'grad_norm': 0.5973209224142185, 'learning_rate': 9.956038747519034e-05, 'epoch': 0.06}
{'loss': 3.631, 'grad_norm': 0.5699490695758563, 'learning_rate': 9.955890630091536e-05, 'epoch': 0.06}
{'loss': 3.5542, 'grad_norm': 0.5790921199830554, 'learning_rate': 9.95574251266404e-05, 'epoch': 0.06}
{'loss': 3.3794, 'grad_norm': 0.8847035551660405, 'learning_rate': 9.955594395236544e-05, 'epoch': 0.06}
{'loss': 3.0265, 'grad_norm': 0.5889507609791873, 'learning_rate': 9.955446277809047e-05, 'epoch': 0.06}
{'loss': 3.6123, 'grad_norm': 0.6243362757099962, 'learning_rate': 9.955298160381551e-05, 'epoch': 0.06}
{'loss': 3.4197, 'grad_norm': 0.585860420745773, 'learning_rate': 9.955150042954055e-05, 'epoch': 0.06}
{'loss': 3.4991, 'grad_norm': 0.64498310911751, 'learning_rate': 9.955001925526558e-05, 'epoch': 0.06}
{'loss': 3.5561, 'grad_norm': 0.5422347692749692, 'learning_rate': 9.954853808099062e-05, 'epoch': 0.06}
{'loss': 3.4965, 'grad_norm': 0.6893459946985531, 'learning_rate': 9.954705690671566e-05, 'epoch': 0.06}
{'loss': 3.5709, 'grad_norm': 0.5579498600614556, 'learning_rate': 9.954557573244068e-05, 'epoch': 0.06}
{'loss': 3.6839, 'grad_norm': 0.587623864877351, 'learning_rate': 9.954409455816572e-05, 'epoch': 0.06}
{'loss': 3.5858, 'grad_norm': 0.5715639672048668, 'learning_rate': 9.954261338389075e-05, 'epoch': 0.06}
{'loss': 3.6478, 'grad_norm': 0.5586456926393416, 'learning_rate': 9.954113220961579e-05, 'epoch': 0.06}
{'loss': 3.5474, 'grad_norm': 0.5728576481804404, 'learning_rate': 9.953965103534081e-05, 'epoch': 0.06}
{'loss': 3.5083, 'grad_norm': 0.5550903210690312, 'learning_rate': 9.953816986106585e-05, 'epoch': 0.06}
{'loss': 3.4166, 'grad_norm': 0.7407454982289876, 'learning_rate': 9.95366886867909e-05, 'epoch': 0.06}
{'loss': 3.5402, 'grad_norm': 0.5954816715912977, 'learning_rate': 9.953520751251592e-05, 'epoch': 0.06}
{'loss': 3.5749, 'grad_norm': 0.5549114029987254, 'learning_rate': 9.953372633824096e-05, 'epoch': 0.06}
{'loss': 3.6301, 'grad_norm': 0.5567132354632456, 'learning_rate': 9.9532245163966e-05, 'epoch': 0.06}
{'loss': 3.5747, 'grad_norm': 0.6394554944365302, 'learning_rate': 9.953076398969103e-05, 'epoch': 0.06}
{'loss': 3.5873, 'grad_norm': 0.5720244642747706, 'learning_rate': 9.952928281541607e-05, 'epoch': 0.06}
{'loss': 3.6549, 'grad_norm': 0.553018228821819, 'learning_rate': 9.952780164114111e-05, 'epoch': 0.06}
{'loss': 3.4913, 'grad_norm': 0.5630412135422513, 'learning_rate': 9.952632046686615e-05, 'epoch': 0.06}
{'loss': 3.632, 'grad_norm': 0.575380601408662, 'learning_rate': 9.952483929259117e-05, 'epoch': 0.06}
{'loss': 3.5515, 'grad_norm': 0.7681302301236974, 'learning_rate': 9.95233581183162e-05, 'epoch': 0.06}
{'loss': 3.3883, 'grad_norm': 0.6185485389645581, 'learning_rate': 9.952187694404124e-05, 'epoch': 0.06}
{'loss': 3.6497, 'grad_norm': 0.5627530742920005, 'learning_rate': 9.952039576976627e-05, 'epoch': 0.06}
{'loss': 3.4096, 'grad_norm': 0.6858260321982198, 'learning_rate': 9.95189145954913e-05, 'epoch': 0.06}
{'loss': 3.4755, 'grad_norm': 0.6217552669179363, 'learning_rate': 9.951743342121635e-05, 'epoch': 0.06}
{'loss': 3.6078, 'grad_norm': 0.574574293745046, 'learning_rate': 9.951595224694139e-05, 'epoch': 0.06}
{'loss': 3.4848, 'grad_norm': 0.5842074246026595, 'learning_rate': 9.951447107266641e-05, 'epoch': 0.06}
{'loss': 3.7062, 'grad_norm': 0.5783253013650043, 'learning_rate': 9.951298989839145e-05, 'epoch': 0.06}
{'loss': 3.4588, 'grad_norm': 0.5757642249649845, 'learning_rate': 9.951150872411649e-05, 'epoch': 0.06}
{'loss': 3.4421, 'grad_norm': 0.5896278725203398, 'learning_rate': 9.951002754984152e-05, 'epoch': 0.06}
{'loss': 3.4786, 'grad_norm': 0.7143133360599184, 'learning_rate': 9.950854637556656e-05, 'epoch': 0.06}
{'loss': 3.6186, 'grad_norm': 0.5603418305408214, 'learning_rate': 9.95070652012916e-05, 'epoch': 0.06}
{'loss': 3.5349, 'grad_norm': 0.5537421191948686, 'learning_rate': 9.950558402701663e-05, 'epoch': 0.06}
{'loss': 3.5726, 'grad_norm': 0.5569927254645103, 'learning_rate': 9.950410285274165e-05, 'epoch': 0.06}
{'loss': 3.1497, 'grad_norm': 0.5600404546267231, 'learning_rate': 9.950262167846669e-05, 'epoch': 0.06}
{'loss': 3.4699, 'grad_norm': 0.5448123311818084, 'learning_rate': 9.950114050419173e-05, 'epoch': 0.06}
{'loss': 3.5675, 'grad_norm': 0.5756887839578664, 'learning_rate': 9.949965932991676e-05, 'epoch': 0.06}
{'loss': 3.6404, 'grad_norm': 0.6850613739375255, 'learning_rate': 9.94981781556418e-05, 'epoch': 0.06}
{'loss': 3.7139, 'grad_norm': 0.5748592556389933, 'learning_rate': 9.949669698136684e-05, 'epoch': 0.06}
{'loss': 3.5588, 'grad_norm': 0.5653270208710642, 'learning_rate': 9.949521580709186e-05, 'epoch': 0.06}
{'loss': 3.1973, 'grad_norm': 0.6074170625040519, 'learning_rate': 9.94937346328169e-05, 'epoch': 0.06}
{'loss': 3.4688, 'grad_norm': 0.5744886858005502, 'learning_rate': 9.949225345854194e-05, 'epoch': 0.06}
{'loss': 3.5882, 'grad_norm': 0.5658915029002186, 'learning_rate': 9.949077228426697e-05, 'epoch': 0.06}
{'loss': 3.6532, 'grad_norm': 0.5458870066666848, 'learning_rate': 9.948929110999201e-05, 'epoch': 0.06}
{'loss': 3.6174, 'grad_norm': 0.6294516183677941, 'learning_rate': 9.948780993571705e-05, 'epoch': 0.06}
{'loss': 3.3048, 'grad_norm': 0.6538093560802664, 'learning_rate': 9.948632876144208e-05, 'epoch': 0.06}
{'loss': 3.6334, 'grad_norm': 0.588949263194228, 'learning_rate': 9.94848475871671e-05, 'epoch': 0.06}
{'loss': 3.4385, 'grad_norm': 0.5675362167290423, 'learning_rate': 9.948336641289214e-05, 'epoch': 0.06}
{'loss': 3.5221, 'grad_norm': 0.5933707823569353, 'learning_rate': 9.948188523861718e-05, 'epoch': 0.06}
{'loss': 3.7223, 'grad_norm': 0.5690018170197484, 'learning_rate': 9.948040406434221e-05, 'epoch': 0.06}
{'loss': 3.5349, 'grad_norm': 0.5734991373259308, 'learning_rate': 9.947892289006725e-05, 'epoch': 0.06}
{'loss': 3.4742, 'grad_norm': 0.5584775528719687, 'learning_rate': 9.947744171579229e-05, 'epoch': 0.06}
{'loss': 3.6117, 'grad_norm': 0.557952819981149, 'learning_rate': 9.947596054151732e-05, 'epoch': 0.06}
{'loss': 3.5514, 'grad_norm': 0.6118504809764482, 'learning_rate': 9.947447936724236e-05, 'epoch': 0.06}
{'loss': 3.6147, 'grad_norm': 0.5848817503010065, 'learning_rate': 9.94729981929674e-05, 'epoch': 0.06}
{'loss': 3.7341, 'grad_norm': 0.5781561183864059, 'learning_rate': 9.947151701869242e-05, 'epoch': 0.06}
{'loss': 3.4944, 'grad_norm': 0.566549223850579, 'learning_rate': 9.947003584441746e-05, 'epoch': 0.06}
{'loss': 3.6854, 'grad_norm': 0.568288350007183, 'learning_rate': 9.946855467014249e-05, 'epoch': 0.06}
{'loss': 3.7132, 'grad_norm': 0.5570724700545633, 'learning_rate': 9.946707349586753e-05, 'epoch': 0.06}
{'loss': 3.6961, 'grad_norm': 0.576597025865842, 'learning_rate': 9.946559232159256e-05, 'epoch': 0.06}
{'loss': 3.3692, 'grad_norm': 0.670601495080283, 'learning_rate': 9.94641111473176e-05, 'epoch': 0.06}
{'loss': 3.5958, 'grad_norm': 0.6955189791150082, 'learning_rate': 9.946262997304264e-05, 'epoch': 0.06}
{'loss': 3.7477, 'grad_norm': 0.5985831979846964, 'learning_rate': 9.946114879876766e-05, 'epoch': 0.06}
{'loss': 3.3509, 'grad_norm': 0.6058044591547589, 'learning_rate': 9.94596676244927e-05, 'epoch': 0.06}
{'loss': 3.6577, 'grad_norm': 0.6290545188587181, 'learning_rate': 9.945818645021774e-05, 'epoch': 0.06}
{'loss': 3.2854, 'grad_norm': 0.6035577535426445, 'learning_rate': 9.945670527594277e-05, 'epoch': 0.06}
{'loss': 3.4741, 'grad_norm': 0.5824199962502948, 'learning_rate': 9.945522410166781e-05, 'epoch': 0.06}
{'loss': 3.2948, 'grad_norm': 0.548134465206202, 'learning_rate': 9.945374292739285e-05, 'epoch': 0.06}
{'loss': 3.5995, 'grad_norm': 0.5712104822450891, 'learning_rate': 9.945226175311789e-05, 'epoch': 0.06}
{'loss': 3.3658, 'grad_norm': 0.5731699724635192, 'learning_rate': 9.94507805788429e-05, 'epoch': 0.06}
{'loss': 3.6675, 'grad_norm': 0.5467971492121336, 'learning_rate': 9.944929940456794e-05, 'epoch': 0.06}
{'loss': 3.3352, 'grad_norm': 0.6011705536930141, 'learning_rate': 9.944781823029298e-05, 'epoch': 0.06}
{'loss': 3.5531, 'grad_norm': 0.5613651177585169, 'learning_rate': 9.944633705601801e-05, 'epoch': 0.06}
{'loss': 3.4823, 'grad_norm': 0.5570298549293274, 'learning_rate': 9.944485588174305e-05, 'epoch': 0.06}
{'loss': 3.6263, 'grad_norm': 0.7045762130895062, 'learning_rate': 9.944337470746809e-05, 'epoch': 0.06}
{'loss': 3.5365, 'grad_norm': 0.5631712607167944, 'learning_rate': 9.944189353319311e-05, 'epoch': 0.06}
{'loss': 3.5494, 'grad_norm': 0.5885517182423636, 'learning_rate': 9.944041235891815e-05, 'epoch': 0.06}
{'loss': 3.4842, 'grad_norm': 0.6453634643295802, 'learning_rate': 9.94389311846432e-05, 'epoch': 0.06}
{'loss': 3.6267, 'grad_norm': 0.5546633534351999, 'learning_rate': 9.943745001036823e-05, 'epoch': 0.06}
{'loss': 3.6032, 'grad_norm': 0.5675931718133846, 'learning_rate': 9.943596883609326e-05, 'epoch': 0.06}
{'loss': 3.4859, 'grad_norm': 0.5895160493474113, 'learning_rate': 9.94344876618183e-05, 'epoch': 0.06}
{'loss': 3.3007, 'grad_norm': 0.5920125230758441, 'learning_rate': 9.943300648754334e-05, 'epoch': 0.06}
{'loss': 3.2097, 'grad_norm': 1.459158030830772, 'learning_rate': 9.943152531326835e-05, 'epoch': 0.06}
{'loss': 3.4658, 'grad_norm': 0.5836874538991098, 'learning_rate': 9.943004413899339e-05, 'epoch': 0.06}
{'loss': 3.4208, 'grad_norm': 0.5813792986351604, 'learning_rate': 9.942856296471843e-05, 'epoch': 0.06}
{'loss': 2.4376, 'grad_norm': 0.6386134349007658, 'learning_rate': 9.942708179044347e-05, 'epoch': 0.06}
{'loss': 3.7054, 'grad_norm': 0.5929227359132289, 'learning_rate': 9.94256006161685e-05, 'epoch': 0.06}
{'loss': 3.5781, 'grad_norm': 0.6278643861073459, 'learning_rate': 9.942411944189354e-05, 'epoch': 0.06}
{'loss': 3.6261, 'grad_norm': 0.5729036526535858, 'learning_rate': 9.942263826761858e-05, 'epoch': 0.06}
{'loss': 3.3105, 'grad_norm': 0.6489777479071168, 'learning_rate': 9.94211570933436e-05, 'epoch': 0.06}
{'loss': 3.3806, 'grad_norm': 0.586780569447305, 'learning_rate': 9.941967591906865e-05, 'epoch': 0.06}
{'loss': 3.4899, 'grad_norm': 0.566799650551848, 'learning_rate': 9.941819474479369e-05, 'epoch': 0.06}
{'loss': 3.4503, 'grad_norm': 0.6330565035075857, 'learning_rate': 9.941671357051871e-05, 'epoch': 0.06}
{'loss': 3.5576, 'grad_norm': 0.6563569280813889, 'learning_rate': 9.941523239624375e-05, 'epoch': 0.06}
{'loss': 3.1849, 'grad_norm': 0.5772965126955584, 'learning_rate': 9.941375122196878e-05, 'epoch': 0.06}
{'loss': 3.4662, 'grad_norm': 0.5834747728830417, 'learning_rate': 9.941227004769382e-05, 'epoch': 0.06}
{'loss': 3.6475, 'grad_norm': 0.5815609334125567, 'learning_rate': 9.941078887341884e-05, 'epoch': 0.06}
{'loss': 3.6787, 'grad_norm': 0.5672831208620666, 'learning_rate': 9.940930769914388e-05, 'epoch': 0.06}
{'loss': 3.5679, 'grad_norm': 0.5627124051052221, 'learning_rate': 9.940782652486892e-05, 'epoch': 0.06}
{'loss': 3.4334, 'grad_norm': 0.5854903969910049, 'learning_rate': 9.940634535059395e-05, 'epoch': 0.06}
{'loss': 3.5451, 'grad_norm': 0.5634538306471343, 'learning_rate': 9.940486417631899e-05, 'epoch': 0.06}
{'loss': 3.6925, 'grad_norm': 0.5903384463034018, 'learning_rate': 9.940338300204403e-05, 'epoch': 0.06}
{'loss': 3.4622, 'grad_norm': 0.581864045861593, 'learning_rate': 9.940190182776906e-05, 'epoch': 0.06}
{'loss': 3.5086, 'grad_norm': 0.6147364974688743, 'learning_rate': 9.94004206534941e-05, 'epoch': 0.06}
{'loss': 3.4246, 'grad_norm': 0.5954175572795019, 'learning_rate': 9.939893947921914e-05, 'epoch': 0.06}
{'loss': 3.011, 'grad_norm': 0.5956913760711865, 'learning_rate': 9.939745830494416e-05, 'epoch': 0.06}
{'loss': 3.4359, 'grad_norm': 0.5564064882106525, 'learning_rate': 9.93959771306692e-05, 'epoch': 0.06}
{'loss': 3.6952, 'grad_norm': 0.5925662149267462, 'learning_rate': 9.939449595639423e-05, 'epoch': 0.06}
{'loss': 3.5536, 'grad_norm': 0.562915753962629, 'learning_rate': 9.939301478211927e-05, 'epoch': 0.06}
{'loss': 3.4288, 'grad_norm': 0.5681172304615482, 'learning_rate': 9.93915336078443e-05, 'epoch': 0.06}
{'loss': 3.3617, 'grad_norm': 0.5482140067460113, 'learning_rate': 9.939005243356934e-05, 'epoch': 0.06}
{'loss': 3.4747, 'grad_norm': 0.5708126915063746, 'learning_rate': 9.938857125929438e-05, 'epoch': 0.06}
{'loss': 3.6586, 'grad_norm': 0.6094626335783051, 'learning_rate': 9.93870900850194e-05, 'epoch': 0.06}
{'loss': 3.5388, 'grad_norm': 0.5897568683366423, 'learning_rate': 9.938560891074444e-05, 'epoch': 0.06}
{'loss': 3.6914, 'grad_norm': 0.6521304754257238, 'learning_rate': 9.938412773646948e-05, 'epoch': 0.06}
{'loss': 3.6189, 'grad_norm': 0.667125772664361, 'learning_rate': 9.938264656219451e-05, 'epoch': 0.06}
{'loss': 3.4215, 'grad_norm': 0.5538066824192777, 'learning_rate': 9.938116538791955e-05, 'epoch': 0.06}
{'loss': 3.4036, 'grad_norm': 0.6057983663260124, 'learning_rate': 9.937968421364459e-05, 'epoch': 0.06}
{'loss': 3.3981, 'grad_norm': 0.5684981718967909, 'learning_rate': 9.937820303936962e-05, 'epoch': 0.06}
{'loss': 3.4762, 'grad_norm': 0.6030566420400306, 'learning_rate': 9.937672186509464e-05, 'epoch': 0.06}
{'loss': 3.3902, 'grad_norm': 0.6065922895415785, 'learning_rate': 9.937524069081968e-05, 'epoch': 0.06}
{'loss': 3.6234, 'grad_norm': 0.5531680993453431, 'learning_rate': 9.937375951654472e-05, 'epoch': 0.06}
{'loss': 3.4812, 'grad_norm': 0.5812907865764805, 'learning_rate': 9.937227834226975e-05, 'epoch': 0.06}
{'loss': 3.5829, 'grad_norm': 0.582934136176802, 'learning_rate': 9.937079716799479e-05, 'epoch': 0.06}
{'loss': 3.5978, 'grad_norm': 0.557731029876619, 'learning_rate': 9.936931599371983e-05, 'epoch': 0.06}
{'loss': 3.6386, 'grad_norm': 0.5453306114822407, 'learning_rate': 9.936783481944485e-05, 'epoch': 0.06}
{'loss': 3.4787, 'grad_norm': 0.5638886564728727, 'learning_rate': 9.93663536451699e-05, 'epoch': 0.06}
{'loss': 3.5635, 'grad_norm': 0.5632280518745797, 'learning_rate': 9.936487247089493e-05, 'epoch': 0.06}
{'loss': 3.6499, 'grad_norm': 0.5541997896429112, 'learning_rate': 9.936339129661997e-05, 'epoch': 0.06}
{'loss': 3.5536, 'grad_norm': 0.5930495838368449, 'learning_rate': 9.9361910122345e-05, 'epoch': 0.06}
{'loss': 3.7664, 'grad_norm': 0.572609037318727, 'learning_rate': 9.936042894807004e-05, 'epoch': 0.06}
{'loss': 3.3954, 'grad_norm': 0.5353302467306147, 'learning_rate': 9.935894777379508e-05, 'epoch': 0.06}
{'loss': 3.5797, 'grad_norm': 0.567392252108924, 'learning_rate': 9.93574665995201e-05, 'epoch': 0.06}
{'loss': 3.634, 'grad_norm': 0.5553158464438657, 'learning_rate': 9.935598542524513e-05, 'epoch': 0.06}
{'loss': 3.6259, 'grad_norm': 0.6037887169459079, 'learning_rate': 9.935450425097017e-05, 'epoch': 0.06}
{'loss': 3.5894, 'grad_norm': 0.5672776050146833, 'learning_rate': 9.93530230766952e-05, 'epoch': 0.06}
{'loss': 3.3497, 'grad_norm': 0.6366503717690342, 'learning_rate': 9.935154190242024e-05, 'epoch': 0.06}
{'loss': 3.6268, 'grad_norm': 0.5832647274850086, 'learning_rate': 9.935006072814528e-05, 'epoch': 0.06}
{'loss': 3.497, 'grad_norm': 0.5829416759952994, 'learning_rate': 9.934857955387032e-05, 'epoch': 0.06}
{'loss': 3.5857, 'grad_norm': 0.5744783944338748, 'learning_rate': 9.934709837959535e-05, 'epoch': 0.06}
{'loss': 3.6178, 'grad_norm': 0.6212611067805979, 'learning_rate': 9.934561720532039e-05, 'epoch': 0.06}
{'loss': 3.5575, 'grad_norm': 0.5539902112611916, 'learning_rate': 9.934413603104543e-05, 'epoch': 0.06}
{'loss': 3.5826, 'grad_norm': 0.5553550898616686, 'learning_rate': 9.934265485677045e-05, 'epoch': 0.06}
{'loss': 3.3354, 'grad_norm': 0.592812849673133, 'learning_rate': 9.934117368249549e-05, 'epoch': 0.06}
{'loss': 3.4923, 'grad_norm': 0.5581458566823714, 'learning_rate': 9.933969250822052e-05, 'epoch': 0.06}
{'loss': 3.5519, 'grad_norm': 0.579615244883853, 'learning_rate': 9.933821133394556e-05, 'epoch': 0.06}
{'loss': 3.5494, 'grad_norm': 0.5905773700550304, 'learning_rate': 9.933673015967059e-05, 'epoch': 0.06}
{'loss': 3.4799, 'grad_norm': 0.9080922483972707, 'learning_rate': 9.933524898539563e-05, 'epoch': 0.06}
{'loss': 3.3578, 'grad_norm': 0.6031617116409598, 'learning_rate': 9.933376781112067e-05, 'epoch': 0.06}
{'loss': 3.5871, 'grad_norm': 0.5801128905107293, 'learning_rate': 9.933228663684569e-05, 'epoch': 0.06}
{'loss': 3.5621, 'grad_norm': 0.5697836011711359, 'learning_rate': 9.933080546257073e-05, 'epoch': 0.06}
{'loss': 3.6503, 'grad_norm': 0.6325759113118122, 'learning_rate': 9.932932428829577e-05, 'epoch': 0.06}
{'loss': 3.5363, 'grad_norm': 0.5357566927938098, 'learning_rate': 9.93278431140208e-05, 'epoch': 0.06}
{'loss': 3.4441, 'grad_norm': 0.5477607664528416, 'learning_rate': 9.932636193974584e-05, 'epoch': 0.06}
{'loss': 3.5275, 'grad_norm': 0.5789075855845562, 'learning_rate': 9.932488076547088e-05, 'epoch': 0.06}
{'loss': 3.4833, 'grad_norm': 0.56829327510503, 'learning_rate': 9.93233995911959e-05, 'epoch': 0.06}
{'loss': 3.4398, 'grad_norm': 0.6396032287594537, 'learning_rate': 9.932191841692094e-05, 'epoch': 0.06}
{'loss': 3.7095, 'grad_norm': 0.5715527669073662, 'learning_rate': 9.932043724264597e-05, 'epoch': 0.06}
{'loss': 3.4364, 'grad_norm': 0.5965459441139164, 'learning_rate': 9.931895606837101e-05, 'epoch': 0.06}
{'loss': 3.6173, 'grad_norm': 0.6142757110248003, 'learning_rate': 9.931747489409604e-05, 'epoch': 0.06}
{'loss': 3.6917, 'grad_norm': 0.5880078450497553, 'learning_rate': 9.931599371982108e-05, 'epoch': 0.06}
{'loss': 3.5318, 'grad_norm': 0.6371180063387235, 'learning_rate': 9.931451254554612e-05, 'epoch': 0.06}
{'loss': 3.6605, 'grad_norm': 0.5908768975160306, 'learning_rate': 9.931303137127114e-05, 'epoch': 0.06}
{'loss': 3.4827, 'grad_norm': 0.5668322279181546, 'learning_rate': 9.931155019699618e-05, 'epoch': 0.06}
{'loss': 3.5289, 'grad_norm': 0.5723342472302975, 'learning_rate': 9.931006902272122e-05, 'epoch': 0.06}
{'loss': 3.5195, 'grad_norm': 0.5440872482406792, 'learning_rate': 9.930858784844625e-05, 'epoch': 0.06}
{'loss': 3.5874, 'grad_norm': 0.6460506566025414, 'learning_rate': 9.930710667417129e-05, 'epoch': 0.06}
{'loss': 3.4656, 'grad_norm': 0.5985832028867766, 'learning_rate': 9.930562549989633e-05, 'epoch': 0.06}
{'loss': 3.5782, 'grad_norm': 0.5866958268834198, 'learning_rate': 9.930414432562136e-05, 'epoch': 0.06}
{'loss': 3.3981, 'grad_norm': 0.5564771429553027, 'learning_rate': 9.930266315134638e-05, 'epoch': 0.06}
{'loss': 3.6538, 'grad_norm': 0.5727750403980668, 'learning_rate': 9.930118197707142e-05, 'epoch': 0.06}
{'loss': 3.2895, 'grad_norm': 0.5605755916864915, 'learning_rate': 9.929970080279646e-05, 'epoch': 0.06}
{'loss': 3.4938, 'grad_norm': 0.5807044129631295, 'learning_rate': 9.929821962852149e-05, 'epoch': 0.06}
{'loss': 3.4633, 'grad_norm': 0.5740778733794606, 'learning_rate': 9.929673845424653e-05, 'epoch': 0.06}
{'loss': 3.0225, 'grad_norm': 0.8928994994674787, 'learning_rate': 9.929525727997157e-05, 'epoch': 0.06}
{'loss': 3.4223, 'grad_norm': 0.571373966439402, 'learning_rate': 9.92937761056966e-05, 'epoch': 0.06}
{'loss': 3.7134, 'grad_norm': 0.581758524081962, 'learning_rate': 9.929229493142164e-05, 'epoch': 0.06}
{'loss': 3.6408, 'grad_norm': 0.6365313790032846, 'learning_rate': 9.929081375714668e-05, 'epoch': 0.06}
{'loss': 3.3112, 'grad_norm': 0.5219710728383781, 'learning_rate': 9.92893325828717e-05, 'epoch': 0.06}
{'loss': 3.7853, 'grad_norm': 0.6736678889900598, 'learning_rate': 9.928785140859674e-05, 'epoch': 0.06}
{'loss': 3.3153, 'grad_norm': 0.6269994079980531, 'learning_rate': 9.928637023432178e-05, 'epoch': 0.06}
{'loss': 3.4832, 'grad_norm': 0.5649367661656163, 'learning_rate': 9.928488906004682e-05, 'epoch': 0.06}
{'loss': 3.4837, 'grad_norm': 0.5788902696677644, 'learning_rate': 9.928340788577183e-05, 'epoch': 0.06}
{'loss': 3.6584, 'grad_norm': 0.5978565977694844, 'learning_rate': 9.928192671149687e-05, 'epoch': 0.07}
{'loss': 3.7993, 'grad_norm': 0.5942725754922016, 'learning_rate': 9.928044553722191e-05, 'epoch': 0.07}
{'loss': 3.5124, 'grad_norm': 0.5759579542569578, 'learning_rate': 9.927896436294694e-05, 'epoch': 0.07}
{'loss': 3.5553, 'grad_norm': 0.5808541336713, 'learning_rate': 9.927748318867198e-05, 'epoch': 0.07}
{'loss': 3.5111, 'grad_norm': 0.5939180251859737, 'learning_rate': 9.927600201439702e-05, 'epoch': 0.07}
{'loss': 3.6572, 'grad_norm': 0.577864627947853, 'learning_rate': 9.927452084012205e-05, 'epoch': 0.07}
{'loss': 3.6624, 'grad_norm': 0.562011397797863, 'learning_rate': 9.927303966584709e-05, 'epoch': 0.07}
{'loss': 3.0486, 'grad_norm': 0.6653410392176896, 'learning_rate': 9.927155849157213e-05, 'epoch': 0.07}
{'loss': 3.3426, 'grad_norm': 0.5898523394793603, 'learning_rate': 9.927007731729717e-05, 'epoch': 0.07}
{'loss': 3.1704, 'grad_norm': 0.5775532681282048, 'learning_rate': 9.92685961430222e-05, 'epoch': 0.07}
{'loss': 3.58, 'grad_norm': 0.6077777387110408, 'learning_rate': 9.926711496874723e-05, 'epoch': 0.07}
{'loss': 3.4533, 'grad_norm': 0.5535523591172216, 'learning_rate': 9.926563379447226e-05, 'epoch': 0.07}
{'loss': 3.399, 'grad_norm': 0.5782885244258157, 'learning_rate': 9.926415262019729e-05, 'epoch': 0.07}
{'loss': 3.5259, 'grad_norm': 0.5737775745943025, 'learning_rate': 9.926267144592233e-05, 'epoch': 0.07}
{'loss': 3.5674, 'grad_norm': 0.5820576009730516, 'learning_rate': 9.926119027164737e-05, 'epoch': 0.07}
{'loss': 3.5066, 'grad_norm': 0.5743222160365923, 'learning_rate': 9.92597090973724e-05, 'epoch': 0.07}
{'loss': 3.5598, 'grad_norm': 0.5785730472439191, 'learning_rate': 9.925822792309743e-05, 'epoch': 0.07}
{'loss': 3.4924, 'grad_norm': 0.56407602922318, 'learning_rate': 9.925674674882247e-05, 'epoch': 0.07}
{'loss': 3.3666, 'grad_norm': 0.5325092546552059, 'learning_rate': 9.925526557454751e-05, 'epoch': 0.07}
{'loss': 3.6627, 'grad_norm': 0.5460853749584826, 'learning_rate': 9.925378440027254e-05, 'epoch': 0.07}
{'loss': 3.4799, 'grad_norm': 0.6055151911301864, 'learning_rate': 9.925230322599758e-05, 'epoch': 0.07}
{'loss': 3.5356, 'grad_norm': 0.5522700480588272, 'learning_rate': 9.925082205172262e-05, 'epoch': 0.07}
{'loss': 3.589, 'grad_norm': 0.5616880192380996, 'learning_rate': 9.924934087744765e-05, 'epoch': 0.07}
{'loss': 3.5454, 'grad_norm': 0.5555733194129303, 'learning_rate': 9.924785970317269e-05, 'epoch': 0.07}
{'loss': 3.5929, 'grad_norm': 0.5586724774463891, 'learning_rate': 9.924637852889771e-05, 'epoch': 0.07}
{'loss': 3.4098, 'grad_norm': 0.6159118936120002, 'learning_rate': 9.924489735462275e-05, 'epoch': 0.07}
{'loss': 3.6244, 'grad_norm': 0.5792639506941752, 'learning_rate': 9.924341618034778e-05, 'epoch': 0.07}
{'loss': 3.5878, 'grad_norm': 0.5603240903784599, 'learning_rate': 9.924193500607282e-05, 'epoch': 0.07}
{'loss': 3.3114, 'grad_norm': 0.6489123676271034, 'learning_rate': 9.924045383179786e-05, 'epoch': 0.07}
{'loss': 3.4275, 'grad_norm': 0.5463859626477998, 'learning_rate': 9.923897265752288e-05, 'epoch': 0.07}
{'loss': 3.4857, 'grad_norm': 0.6133378657269076, 'learning_rate': 9.923749148324792e-05, 'epoch': 0.07}
{'loss': 3.7366, 'grad_norm': 0.5674106071073022, 'learning_rate': 9.923601030897296e-05, 'epoch': 0.07}
{'loss': 3.4666, 'grad_norm': 0.587535889162989, 'learning_rate': 9.923452913469799e-05, 'epoch': 0.07}
{'loss': 3.5999, 'grad_norm': 0.5690620600238956, 'learning_rate': 9.923304796042303e-05, 'epoch': 0.07}
{'loss': 3.5767, 'grad_norm': 0.5754336544419395, 'learning_rate': 9.923156678614807e-05, 'epoch': 0.07}
{'loss': 3.5909, 'grad_norm': 0.5509060227520657, 'learning_rate': 9.92300856118731e-05, 'epoch': 0.07}
{'loss': 3.5886, 'grad_norm': 0.5961341633289488, 'learning_rate': 9.922860443759812e-05, 'epoch': 0.07}
{'loss': 3.469, 'grad_norm': 0.6060409730590207, 'learning_rate': 9.922712326332316e-05, 'epoch': 0.07}
{'loss': 3.5224, 'grad_norm': 0.5567437022164397, 'learning_rate': 9.92256420890482e-05, 'epoch': 0.07}
{'loss': 3.5781, 'grad_norm': 0.5902966780076069, 'learning_rate': 9.922416091477323e-05, 'epoch': 0.07}
{'loss': 3.3854, 'grad_norm': 0.5820044552804059, 'learning_rate': 9.922267974049827e-05, 'epoch': 0.07}
{'loss': 3.6231, 'grad_norm': 0.6040711573158132, 'learning_rate': 9.922119856622331e-05, 'epoch': 0.07}
{'loss': 3.4066, 'grad_norm': 0.7069086830417965, 'learning_rate': 9.921971739194834e-05, 'epoch': 0.07}
{'loss': 3.4192, 'grad_norm': 0.5633575177217149, 'learning_rate': 9.921823621767338e-05, 'epoch': 0.07}
{'loss': 3.5081, 'grad_norm': 0.5606083011022295, 'learning_rate': 9.921675504339842e-05, 'epoch': 0.07}
{'loss': 3.3603, 'grad_norm': 0.5903817165649042, 'learning_rate': 9.921527386912344e-05, 'epoch': 0.07}
{'loss': 3.5946, 'grad_norm': 0.5828487600704446, 'learning_rate': 9.921379269484848e-05, 'epoch': 0.07}
{'loss': 1.4812, 'grad_norm': 0.8771274304587554, 'learning_rate': 9.921231152057352e-05, 'epoch': 0.07}
{'loss': 3.4534, 'grad_norm': 0.5985708650880868, 'learning_rate': 9.921083034629855e-05, 'epoch': 0.07}
{'loss': 3.4834, 'grad_norm': 0.5726328026436807, 'learning_rate': 9.920934917202358e-05, 'epoch': 0.07}
{'loss': 3.6088, 'grad_norm': 0.5896599006991, 'learning_rate': 9.920786799774862e-05, 'epoch': 0.07}
{'loss': 3.5623, 'grad_norm': 0.5920684438062758, 'learning_rate': 9.920638682347366e-05, 'epoch': 0.07}
{'loss': 3.5262, 'grad_norm': 0.6234104376789069, 'learning_rate': 9.920490564919868e-05, 'epoch': 0.07}
{'loss': 3.5507, 'grad_norm': 0.5776947528474223, 'learning_rate': 9.920342447492372e-05, 'epoch': 0.07}
                                                                                                         